title,authors,keywords,abstract,date,cite_num,doi
Deep Multi-view Stereo for Dense 3D Reconstruction from Monocular Endoscopic Video.,"[{'@pid': '275/6835', 'text': 'Gwangbin Bae'}, {'@pid': '08/8939', 'text': 'Ignas Budvytis'}, {'@pid': '216/5300', 'text': 'Chung-Kwong Yeung'}, {'@pid': 'c/RobertoCipolla', 'text': 'Roberto Cipolla'}]","['Multi-viewstereo', '3Dreconstruction', 'Endoscopy']","3D reconstruction from monocular endoscopic images is a challenging task. State-of-the-art multi-view stereo (MVS) algorithms based on image patch similarity often fail to obtain a dense reconstruction from weakly-textured endoscopic images. In this paper, we present a novel deep-learning-based MVS algorithm that can produce a dense and accurate 3D reconstruction from a monocular endoscopic image sequence. Our method consists of three key steps. Firstly, a number of depth candidates are sampled around the depth prediction made by a pre-trained CNN. Secondly, each candidate is projected to the other images in the sequence, and the matching score is measured using a patch embedding network that maps each image patch into a compact embedding. Finally, the candidate with the highest score is selected for each pixel. Experiments on colonoscopy videos demonstrate that our patch embedding network outperforms zero-normalized cross-correlation and a state-of-the-art stereo matching network in terms of matching accuracy and that our MVS algorithm produces several degrees of magnitude denser reconstruction than the competing methods when same accuracy filtering is applied.",2020/9/29,1,10.1007/978-3-030-59716-0_74
Deep Placental Vessel Segmentation for Fetoscopic Mosaicking.,"[{'@pid': '159/3825', 'text': 'Sophia Bano'}, {'@pid': '80/9881', 'text': 'Francisco Vasconcelos'}, {'@pid': '270/0724', 'text': 'Luke M. Shepherd'}, {'@pid': '61/5832', 'text': 'Emmanuel B. Vander Poorten'}, {'@pid': '99/4387', 'text': 'Tom Vercauteren'}, {'@pid': '40/2838', 'text': 'S¨¦bastien Ourselin'}, {'@pid': '168/5581', 'text': 'Anna L. David'}, {'@pid': '168/5453', 'text': 'Jan Deprest'}, {'@pid': '53/3543', 'text': 'Danail Stoyanov'}]","['Fetoscopy', 'Deeplearning', 'Vesselsegmentation', 'Vesselregistration', 'Mosaicking', 'Twin-to-twintransfusionsyndrome']","During fetoscopic laser photocoagulation, a treatment for twin-to-twin transfusion syndrome (TTTS), the clinician first identifies abnormal placental vascular connections and laser ablates them to regulate blood flow in both fetuses. The procedure is challenging due to the mobility of the environment, poor visibility in amniotic fluid, occasional bleeding, and limitations in the fetoscopic field-of-view and image quality. Ideally, anastomotic placental vessels would be automatically identified, segmented and registered to create expanded vessel maps to guide laser ablation, however, such methods have yet to be clinically adopted. We propose a solution utilising the U-Net architecture for performing placental vessel segmentation in fetoscopic videos. The obtained vessel probability maps provide sufficient cues for mosaicking alignment by registering consecutive vessel maps using the direct intensity-based technique. Experiments on 6 different in vivo fetoscopic videos demonstrate that the vessel intensity-based registration outperformed image intensity-based registration approaches showing better robustness in qualitative and quantitative comparison. We additionally reduce drift accumulation to negligible even for sequences with up?to 400 frames and we incorporate a scheme for quantifying drift error in the absence of the ground-truth. Our paper provides a benchmark for fetoscopy placental vessel segmentation and registration by contributing the first in vivo vessel segmentation and fetoscopic videos dataset.",2020/9/29,1,10.1007/978-3-030-59716-0_73
Source-Relaxed Domain Adaptation for Image Segmentation.,"[{'@pid': '246/4942', 'text': 'Mathilde Bateson'}, {'@pid': '150/6611', 'text': 'Hoel Kervadec'}, {'@pid': '165/8035', 'text': 'Jose Dolz'}, {'@pid': '92/1233', 'text': 'Herv¨¦ Lombaert'}, {'@pid': '68/4478', 'text': 'Ismail Ben Ayed'}]","['Imagesegmentation', 'Domainadaptation', 'Entropyminimization']","Domain adaptation (DA) has drawn high interests for its capacity to adapt a model trained on labeled source data to perform well on unlabeled or weakly labeled target data from a different domain. Most common DA techniques require the concurrent access to the input images of both the source and target domains. However, in practice, it is common that the source images are not available in the adaptation phase. This is a very frequent DA scenario in medical imaging, for instance, when the source and target images come from different clinical sites. We propose a novel formulation for adapting segmentation networks, which relaxes such a constraint. Our formulation is based on minimizing a label-free entropy loss defined over target-domain data, which we further guide with a domain-invariant prior on the segmentation regions. Many priors can be used, derived from anatomical information. Here, a class-ratio prior is learned via an auxiliary network and integrated in the form of a Kullback¨CLeibler (KL) divergence in our overall loss function. We show the effectiveness of our prior-aware entropy minimization in adapting spine segmentation across different MRI modalities. Our method yields comparable results to several state-of-the-art adaptation techniques, even though is has access to less information, the source images being absent in the adaptation phase. Our straight-forward adaptation strategy only uses one network, contrary to popular adversarial techniques, which cannot perform without the presence of the source images. Our framework can be readily used with various priors and segmentation problems.",2020/9/29,1,10.1007/978-3-030-59710-8_48
Scale-Space Autoencoders for Unsupervised Anomaly Segmentation in Brain MRI.,"[{'@pid': '73/5647', 'text': 'Christoph Baur'}, {'@pid': '165/7294', 'text': 'Benedikt Wiestler'}, {'@pid': '165/7751', 'text': 'Shadi Albarqouni'}, {'@pid': 'n/NassirNavab', 'text': 'Nassir Navab'}]","['Anomalysegmentation', 'Anomalydetection', 'Unsupervised', 'Laplacianpyramid', 'Scalespace', 'Autoencoders', 'BrainMRI']","Brain pathologies can vary greatly in size and shape, ranging from few pixels (i.e. MS lesions) to large, space-occupying tumors. Recently proposed Autoencoder-based methods for unsupervised anomaly segmentation in brain MRI have shown promising performance, but face difficulties in modeling distributions with high fidelity, which is crucial for accurate delineation of particularly small lesions. Here, similar to these previous works, we model the distribution of healthy brain MRI to localize pathologies from erroneous reconstructions. However, to achieve improved reconstruction fidelity at higher resolutions, we learn to compress and reconstruct different frequency bands of healthy brain MRI using the laplacian pyramid. In a range of experiments comparing our method to different State-of-the-Art approaches on three different brain MR datasets with MS lesions and tumors, we show improved anomaly segmentation performance and the general capability to obtain much more crisp reconstructions of input data at native resolution. The modeling of the laplacian pyramid further enables the delineation and aggregation of lesions at multiple scales, which allows to effectively cope with different pathologies and lesion sizes using a single model.",2020/9/29,1,10.1007/978-3-030-59719-1_54
Inferring the 3D Standing Spine Posture from 2D Radiographs.,"[{'@pid': '257/3280', 'text': 'Amirhossein Bayat'}, {'@pid': '198/0458', 'text': 'Anjany Sekuboyina'}, {'@pid': '239/8775', 'text': 'Johannes C. Paetzold'}, {'@pid': '167/9663', 'text': 'Christian Payer'}, {'@pid': '02/8488', 'text': 'Darko Stern'}, {'@pid': '68/309', 'text': 'Martin Urschler'}, {'@pid': '181/6853', 'text': 'Jan S. Kirschke'}, {'@pid': '74/6505', 'text': 'Bjoern H. Menze'}]","['3Dreconstruction', 'Fullyconvolutionalneworks', 'Spineposture', 'Digitallyreconstructedradiographs']","The treatment of degenerative spinal disorders requires an understanding of the individual spinal anatomy and curvature in 3D. An upright spinal pose (i.e. standing) under natural weight bearing is crucial for such bio-mechanical analysis. 3D volumetric imaging modalities (e.g. CT and MRI) are performed in patients lying down. On the other hand, radiographs are captured in an upright pose, but result in 2D projections. This work aims to integrate the two realms, i.e. it combines the upright spinal curvature from radiographs with the 3D vertebral shape from CT imaging for synthesizing an upright 3D model of spine, loaded naturally. Specifically, we propose a novel neural network architecture working vertebra-wise, termed TransVert, which takes orthogonal 2D radiographs and infers the spine¡¯s 3D posture. We validate our architecture on digitally reconstructed radiographs, achieving a 3D reconstruction Dice of \(95.52\%\), indicating an almost perfect 2D-to-3D domain translation. Deploying our model on clinical radiographs, we successfully synthesise full-3D, upright, patient-specific spine models for the first time
.",2020/9/29,1,10.1007/978-3-030-59725-2_75
Spectral-spatial Recurrent-Convolutional Networks for In-Vivo Hyperspectral Tumor Type Classification.,"[{'@pid': '241/6119', 'text': 'Marcel Bengs'}, {'@pid': '217/1525', 'text': 'Nils Gessert'}, {'@pid': '187/1717', 'text': 'Wiebke Laffers'}, {'@pid': '263/7583', 'text': 'Dennis Eggert'}, {'@pid': '187/2123', 'text': 'Stephan Westermann'}, {'@pid': '187/1815', 'text': 'Nina A. M¨¹ller'}, {'@pid': '187/1714', 'text': 'Andreas O. H. Gerstner'}, {'@pid': '80/218', 'text': 'Christian Betz'}, {'@pid': '08/5407', 'text': 'Alexander Schlaefer'}]","['Hyperspectralimaging', 'Headandneckcancer', 'Spatio-spectraldeeplearning']","Early detection of cancerous tissue is crucial for long-term patient survival. In the head and neck region, a typical diagnostic procedure is an endoscopic intervention where a medical expert manually assesses tissue using RGB camera images. While healthy and tumor regions are generally easier to distinguish, differentiating benign and malignant tumors is very challenging. This requires an invasive biopsy, followed by histological evaluation for diagnosis. Also, during tumor resection, tumor margins need to be verified by histological analysis. To avoid unnecessary tissue resection, a non-invasive, image-based diagnostic tool would be very valuable. Recently, hyperspectral imaging paired with deep learning has been proposed for this task, demonstrating promising results on ex-vivo specimens. In this work, we demonstrate the feasibility of in-vivo tumor type classification using hyperspectral imaging and deep learning. We analyze the value of using multiple hyperspectral bands compared to conventional RGB images and we study several machine learning models¡¯ ability to make use of the additional spectral information. Based on our insights, we address spectral and spatial processing using recurrent-convolutional models for effective spectral aggregating and spatial feature learning. Our best model achieves an AUC of \({76.3\,\mathrm{\%}}\), significantly outperforming previous conventional and deep learning methods.",2020/9/29,1,10.1007/978-3-030-59716-0_66
Topology-Aware Generative Adversarial Network for Joint Prediction of Multiple Brain Graphs from a Single Brain Graph.,"[{'@pid': '226/4917', 'text': 'Alaa Bessadok'}, {'@pid': '44/9613', 'text': 'Mohamed Ali Mahjoub'}, {'@pid': '124/9354', 'text': 'Islem Rekik'}]","['Adversarialbrainmultigraphprediction', 'Geometricdeeplearning', 'MultigraphGAN']","Multimodal medical datasets with incomplete observations present a barrier to large-scale neuroscience studies. Several works based on Generative Adversarial Networks (GAN) have been recently proposed to predict a set of medical images from a single modality (e.g., FLAIR MRI from T1 MRI). However, such frameworks are primarily designed to operate on images, limiting their generalizability to non-Euclidean geometric data such as brain graphs. While a growing number of connectomic studies has demonstrated the promise of including brain graphs for diagnosing neurological disorders, no geometric deep learning work was designed for multiple target brain graphs prediction from a source brain graph. Despite the momentum the field of graph generation has gained in the last two years, existing works have two critical drawbacks. First, the bulk of such works aims to learn one model for each target domain to generate from a source domain. Thus, they have a limited scalability in jointly predicting multiple target domains. Second, they merely consider the global topological scale of a graph (i.e., graph connectivity structure) and overlook the local topology at the node scale of a graph (e.g., how central a node is in the graph). To meet these challenges, we introduce MultiGraphGAN architecture, which not only predicts multiple brain graphs from a single brain graph but also preserves the topological structure of each target graph to predict. Its three core contributions lie in: (i) designing a graph adversarial auto-encoder for jointly predicting brain graphs from a single one, (ii) handling the mode collapse problem of GAN by clustering the encoded source graphs and proposing a cluster-specific decoder, (iii) introducing a topological loss to force the reconstruction of topologically sound target brain graphs. Our MultiGraphGAN significantly outperformed its variants thereby showing its great potential in multi-view brain graph generation from a single graph. Our code is available at https://github.com/basiralab/MultiGraphGAN.",2020/9/29,1,10.1007/978-3-030-59728-3_54
CorrSigNet - Learning CORRelated Prostate Cancer SIGnatures from Radiology and Pathology Images for Improved Computer Aided Diagnosis.,"[{'@pid': '172/7716', 'text': 'Indrani Bhattacharya'}, {'@pid': '271/7842', 'text': 'Arun Seetharaman'}, {'@pid': '24/803', 'text': 'Wei Shao'}, {'@pid': '234/1644', 'text': 'Rewa Sood'}, {'@pid': '239/2003', 'text': 'Christian A. Kunder'}, {'@pid': '141/7829', 'text': 'Richard E. Fan'}, {'@pid': '271/7826', 'text': 'Simon John Christoph Soerensen'}, {'@pid': '271/8527', 'text': 'Jeffrey B. Wang'}, {'@pid': '239/2040', 'text': 'Pejman Ghanouni'}, {'@pid': '271/7985', 'text': 'Nikola C. Teslovich'}, {'@pid': '75/7457', 'text': 'James D. Brooks'}, {'@pid': '192/1630', 'text': 'Geoffrey A. Sonn'}, {'@pid': '50/3569', 'text': 'Mirabela Rusu'}]","['Computeraideddiagnosis', 'CommonrepresentationLearning', 'MRI', 'Histopathologyimages', 'Prostatecancer']","Magnetic Resonance Imaging (MRI) is widely used for screening and staging prostate cancer. However, many prostate cancers have subtle features which are not easily identifiable on MRI, resulting in missed diagnoses and alarming variability in radiologist interpretation. Machine learning models have been developed in an effort to improve cancer identification, but current models localize cancer using MRI-derived features, while failing to consider the disease pathology characteristics observed on resected tissue. In this paper, we propose CorrSigNet, an automated two-step model that localizes prostate cancer on MRI by capturing the pathology features of cancer. First, the model learns MRI signatures of cancer that are correlated with corresponding histopathology features using Common Representation Learning. Second, the model uses the learned correlated MRI features to train a Convolutional Neural Network to localize prostate cancer. The histopathology images are used only in the first step to learn the correlated features. Once learned, these correlated features can be extracted from MRI of new patients (without histopathology or surgery) to localize cancer. We trained and validated our framework on a unique dataset of 75 patients with 806 slices who underwent MRI followed by prostatectomy surgery. We tested our method on an independent test set of 20 prostatectomy patients (139 slices, 24 cancerous lesions, 1.12M pixels) and achieved a per-pixel sensitivity of 0.81, specificity of 0.71, AUC of 0.86 and a per-lesion AUC of \(0.96 \pm 0.07\), outperforming the current state-of-the-art accuracy in predicting prostate cancer using MRI.",2020/9/29,1,10.1007/978-3-030-59713-9_31
BUNET - Blind Medical Image Segmentation Based on Secure UNET.,"[{'@pid': '179/7914', 'text': 'Song Bian 0001'}, {'@pid': '181/2733-4', 'text': 'Xiaowei Xu 0004'}, {'@pid': '151/4529', 'text': 'Weiwen Jiang'}, {'@pid': '94/5536', 'text': 'Yiyu Shi'}, {'@pid': '48/4595', 'text': 'Takashi Sato'}]",[],"The strict security requirements placed on medical records by various privacy regulations become major obstacles in the age of big data. To ensure efficient machine learning as a service schemes while protecting data confidentiality, in this work, we propose blind UNET (BUNET), a secure protocol that implements privacy-preserving medical image segmentation based on the UNET architecture. In BUNET, we efficiently utilize cryptographic primitives such as homomorphic encryption and garbled circuits (GC) to design a complete secure protocol for the UNET neural architecture. In addition, we perform extensive architectural search in reducing the computational bottleneck of GC-based secure activation protocols with high-dimensional input data. In the experiment, we thoroughly examine the parameter space of our protocol, and show that we can achieve up?to 14x inference time reduction compared to the-state-of-the-art secure inference technique on a baseline architecture with negligible accuracy degradation.",2020/9/29,1,10.1007/978-3-030-59713-9_59
Partial Volume Segmentation of Brain MRI Scans of Any Resolution and Contrast.,"[{'@pid': '243/9034', 'text': 'Benjamin Billot'}, {'@pid': '263/7161', 'text': 'Eleanor D. Robinson'}, {'@pid': '27/4108', 'text': 'Adrian V. Dalca'}, {'@pid': '06/5946', 'text': 'Juan Eugenio Iglesias'}]","['Partialvolumesegmentation', 'BrainMRI']","Partial voluming (PV) is arguably the last crucial unsolved problem in Bayesian segmentation of brain MRI with probabilistic atlases. PV occurs when voxels contain multiple tissue classes, giving rise to image intensities that may not be representative of any one of the underlying classes. PV is particularly problematic for segmentation when there is a large resolution gap between the atlas and the test scan, e.g., when segmenting clinical scans with thick slices, or when using a high-resolution atlas. Forward models of PV are realistic and simple, as they amount to blurring and subsampling a high resolution (HR) volume into a lower resolution (LR) scan. Unfortunately, segmentation as Bayesian inference quickly becomes intractable when ¡°inverting¡± this forward PV model, as it requires marginalizing over all possible anatomical configurations of the HR volume. In this work, we present PV-SynthSeg, a convolutional neural network (CNN) that tackles this problem by directly learning a mapping between (possibly multi-modal) LR scans and underlying HR segmentations. PV-SynthSeg simulates LR images from HR label maps with a generative model of PV, and can be trained to segment scans of any desired target contrast and resolution, even for previously unseen modalities where neither images nor segmentations are available at training. PV-SynthSeg does not require any preprocessing, and runs in seconds. We demonstrate the accuracy and flexibility of our method with extensive experiments on three datasets and 2,680 scans. The code is available at https://github.com/BBillot/SynthSeg.
",2020/9/29,1,10.1007/978-3-030-59728-3_18
Flow-Based Deformation Guidance for Unpaired Multi-contrast MRI Image-to-Image Translation.,"[{'@pid': '143/9351', 'text': 'Toan Duc Bui'}, {'@pid': '242/6386', 'text': 'Manh Nguyen'}, {'@pid': '37/245', 'text': 'Ngan Le'}, {'@pid': '43/8092', 'text': 'Khoa Luu'}]","['Flow-basedgenerator', 'Image-to-imagetranslation', 'cycleGAN']","Image synthesis from corrupted contrasts increases the diversity of diagnostic information available for many neurological diseases. Recently the image-to-image translation has experienced significant levels of interest within medical research, beginning with the successful use of the Generative Adversarial Network (GAN) to the introduction of cyclic constraint extended to multiple domains. However, in current approaches, there is no guarantee that the mapping between the two image domains would be unique or one-to-one. In this paper, we introduce a novel approach to unpaired image-to-image translation based on the invertible architecture. The invertible property of the flow-based architecture assures a cycle-consistency of image-to-image translation without additional loss functions. We utilize the temporal information between consecutive slices to provide more constraints to the optimization for transforming one domain to another in unpaired volumetric medical images. To capture temporal structures in the medical images, we explore the displacement between the consecutive slices using a deformation field. In our approach, the deformation field is used as a guidance to keep the translated slides realistic and consistent across the translation. The experimental results have shown that the synthesized images using our proposed approach are able to archive a competitive performance in terms of mean squared error, peak signal-to-noise ratio, and structural similarity index when compared with the existing deep learning-based methods on three standard datasets, i.e. HCP, MRBrainS13 and Brats2019.",2020/9/29,1,10.1007/978-3-030-59713-9_70
Multi-task Learning for Detection and Classification of Cancer in Screening Mammography.,"[{'@pid': '165/8066', 'text': 'Maria V. Sainz de Cea'}, {'@pid': '275/6912', 'text': 'Karl Diedrich'}, {'@pid': '201/7700', 'text': 'Ran Bakalo'}, {'@pid': '250/5785', 'text': 'Lior Ness'}, {'@pid': '194/2839', 'text': 'David Richmond'}]","['Decisionsupport', 'Deeplearning', 'RetinaNet', 'ResNet']","Breast screening is an effective method to identify breast cancer in asymptomatic women; however, not all exams are read by radiologists specialized in breast imaging, and missed cancers are a reality. Deep learning provides a valuable tool to support this critical decision point. Algorithmically, accurate assessment of breast mammography requires both detection of abnormal findings (object detection) and a correct decision whether to recall a patient for additional imaging (image classification). In this paper, we present a multi-task learning approach, that we argue is ideally suited to this problem. We train a network for both object detection and image classification, based on state-of-the-art models, and demonstrate significant improvement in the recall vs no recall decision on a multi-site, multi-vendor data set, measured by concordance with biopsy proven malignancy. We also observe improved detection of microcalcifications, and detection of cancer cases that were missed by radiologists, demonstrating that this approach could provide meaningful support for radiologists in breast screening (especially non-specialists). Moreover, we argue that this multi-task framework is broadly applicable to a wide range of medical imaging problems that require a patient-level recommendation, based on specific imaging findings.",2020/9/29,1,10.1007/978-3-030-59725-2_24
Boundary-Assisted Region Proposal Networks for Nucleus Segmentation.,"[{'@pid': '233/9768', 'text': 'Shengcong Chen'}, {'@pid': '140/4707', 'text': 'Changxing Ding'}, {'@pid': '46/3391', 'text': 'Dacheng Tao'}]","['Nucleussegmentation', 'Multi-tasklearning', 'Instancesegmentation']","Nucleus segmentation is an important task in medical image analysis. However, machine learning models cannot perform well because there are large amount of clusters of crowded nuclei. To handle this problem, existing approaches typically resort to sophisticated hand-crafted post-processing strategies; therefore, they are vulnerable to the variation of post-processing hyper-parameters. Accordingly, in this paper, we devise a Boundary-assisted Region Proposal Network (BRP-Net) that achieves robust instance-level nucleus segmentation. First, we propose a novel Task-aware Feature Encoding (TAFE) network that efficiently extracts respective high-quality features for semantic segmentation and instance boundary detection tasks. This is achieved by carefully considering the correlation and differences between the two tasks. Second, coarse nucleus proposals are generated based on the predictions of the above two tasks. Third, these proposals are fed into instance segmentation networks for more accurate prediction. Experimental results demonstrate that the performance of BRP-Net is robust to the variation of post-processing hyper-parameters. Furthermore, BRP-Net achieves state-of-the-art performances on both the Kumar and CPM17 datasets. The code of BRP-Net will be released at https://github.com/csccsccsccsc/brpnet.",2020/9/29,1,10.1007/978-3-030-59722-1_27
Joint Spatial-Wavelet Dual-Stream Network for Super-Resolution.,"[{'@pid': '11/1266-13', 'text': 'Zhen Chen 0013'}, {'@pid': '25/8118', 'text': 'Xiaoqing Guo'}, {'@pid': '01/2478', 'text': 'Chen Yang'}, {'@pid': '119/9151', 'text': 'Bulat Ibragimov'}, {'@pid': '36/9220', 'text': 'Yixuan Yuan'}]","['Super-resolution', 'Waveletdomain', 'Convolutionalneuralnetworks']","Super-Resolution (SR) techniques can compensate for the missing information of low-resolution images and further promote experts and algorithms to make accurate diagnosis decisions. Although the existing pixel-loss based SR works produce high-resolution images with impressive objective metrics, the over-smoothed contents that lose high-frequency information would disturb the visual experience and the subsequent diagnosis. To address this issue, we propose a joint Spatial-Wavelet super-resolution Network (SWD-Net) with collaborative Dual-stream. In the spatial stage, a Refined Context Fusion (RCF) is proposed to iteratively rectify the features by a counterpart stream with compensative receptive fields. After that, the wavelet stage enhances the reconstructed images, especially the structural boundaries. Specifically, we design the tailor-made Wavelet Features Adaptation (WFA) to adjust the wavelet coefficients for better compatibility with networks and Wavelet-Aware Convolutional blocks (WAC) to exploit features in the wavelet domain efficiently. We further introduce the wavelet coefficients supervision together with the traditional spatial loss to jointly optimize the network and obtain the high-frequency enhanced SR images. To evaluate the SR for medical images, we build a benchmark dataset with histopathology images and evaluate the proposed SWD-Net under different settings. The comprehensive experiments demonstrate our SWD-Net outperforms state-of-the-art methods. Furthermore, SWD-Net is proven to promote medical image diagnosis with a large margin. The source code and dataset are available at https://github.com/franciszchen/SWD-Net.",2020/9/29,1,10.1007/978-3-030-59722-1_18
A Deep Spatial Context Guided Framework for Infant Brain Subcortical Segmentation.,"[{'@pid': '178/9781', 'text': 'Liangjun Chen'}, {'@pid': '35/8016', 'text': 'Zhengwang Wu'}, {'@pid': '22/967', 'text': 'Dan Hu'}, {'@pid': '33/1842', 'text': 'Ya Wang'}, {'@pid': '261/2442', 'text': 'Zhanhao Mo'}, {'@pid': '58/6810-26', 'text': 'Li Wang 0026'}, {'@pid': '98/3393', 'text': 'Weili Lin'}, {'@pid': '14/4383', 'text': 'Dinggang Shen'}, {'@pid': '62/2655-1', 'text': 'Gang Li 0001'}]","['Subcorticalsegmentation', 'Spatialcontextinformation', 'Coarse-to-fineframework', 'Infantbrain']","Accurate subcortical segmentation of infant brain magnetic resonance (MR) images is crucial for studying early subcortical structural growth patterns and related diseases diagnosis. However, dynamic intensity changes, low tissue contrast, and small subcortical size of infant brain MR images make subcortical segmentation a challenging task. In this paper, we propose a spatial context guided, coarse-to-fine deep convolutional neural network (CNN) based framework for accurate infant subcortical segmentation. At the coarse stage, we propose a signed distance map (SDM) learning UNet (SDM-UNet) to predict SDMs from the original multi-modal images, including T1w, T2w, and T1w/T2w images. By doing this, the spatial context information, including the relative position information across different structures and the shape information of the segmented structures contained in the ground-truth SDMs, is used for supervising the SDM-UNet to remedy the bad influence from the low tissue contrast in infant brain MR images and generate high-quality SDMs. To improve the robustness to outliers, a Correntropy based loss is introduced in SDM-UNet to penalize the difference between the ground-truth SDMs and predicted SDMs in training. At the fine stage, the predicted SDMs, which contains spatial context information of subcortical structures, are combined with the multi-modal images, and then fed into a multi-source and multi-path UNet (M2-UNet) for delivering refined segmentation. We validate our method on an infant brain MR image dataset with 24 scans by evaluating the Dice ratio between our segmentation and the manual delineation. Compared to four state-of-the-art methods, our method consistently achieves better performances in both qualitative and quantitative evaluations.",2020/9/29,1,10.1007/978-3-030-59728-3_63
Learning Directional Feature Maps for Cardiac MRI Segmentation.,"[{'@pid': '12/3274', 'text': 'Feng Cheng'}, {'@pid': '10/217', 'text': 'Cheng Chen'}, {'@pid': '230/8552', 'text': 'Yukang Wang'}, {'@pid': '271/0256', 'text': 'Heshui Shi'}, {'@pid': '96/5464', 'text': 'Yukun Cao'}, {'@pid': '88/8375', 'text': 'Dandan Tu'}, {'@pid': '85/7804', 'text': 'Changzheng Zhang'}, {'@pid': '86/11269', 'text': 'Yongchao Xu'}]","['Cardiacsegmentation', 'Deeplearning', 'Directionfield']","Cardiac MRI segmentation plays a crucial role in clinical diagnosis for evaluating personalized cardiac performance parameters. Due to the indistinct boundaries and heterogeneous intensity distributions in the cardiac MRI, most existing methods still suffer from two aspects of challenges: inter-class indistinction and intra-class inconsistency. To tackle these two problems, we propose a novel method to exploit the directional feature maps, which can simultaneously strengthen the differences between classes and the similarities within classes. Specifically, we perform cardiac segmentation and learn a direction field pointing away from the nearest cardiac tissue boundary to each pixel via a direction field (DF) module. Based on the learned direction field, we then propose a feature rectification and fusion (FRF) module to improve the original segmentation features, and obtain the final segmentation. The proposed modules are simple yet effective and can be flexibly added to any existing segmentation network without excessively increasing time and space complexity. We evaluate the proposed method on the 2017 MICCAI Automated Cardiac Diagnosis Challenge (ACDC) dataset and a large-scale self-collected dataset, showing good segmentation performance and robust generalization ability of the proposed method. The code is publicly available at https://github.com/c-feng/DirectionalFeature.",2020/9/29,1,10.1007/978-3-030-59719-1_11
Multiple Instance Learning with Center Embeddings for Histopathology Classification.,"[{'@pid': '229/8107', 'text': 'Philip Chikontwe'}, {'@pid': '275/6820', 'text': 'Meejeong Kim'}, {'@pid': '275/7064', 'text': 'Soo Jeong Nam'}, {'@pid': '275/6775', 'text': 'Heounjeong Go'}, {'@pid': '88/1351', 'text': 'Sang Hyun Park'}]",[],"Histopathology image analysis plays an important role in the treatment and diagnosis of cancer. However, analysis of whole slide images (WSI) with deep learning is challenging given that the duration of pixel-level annotations is laborious and time consuming. To address this, recent methods have considered WSI classification as a Multiple Instance Learning (MIL) problem often with a multi-stage process for learning instance and slide level features. Currently, most methods focus on either instance-selection or instance prediction-aggregation that often fails to generalize and ignores instance relations. In this work, we propose a MIL-based method to jointly learn both instance- and bag-level embeddings in a single framework. In addition, we propose a center loss that maps embeddings of instances from the same bag to a single centroid and reduces intra-class variations. Consequently, our model can accurately predict instance labels and leverages robust hierarchical pooling of features to obtain bag-level features without sacrificing accuracy. Experimental results on curated colon datasets show the effectiveness of the proposed methods against recent state-of-the-art methods.
",2020/9/29,1,10.1007/978-3-030-59722-1_50
Harnessing Uncertainty in Domain Adaptation for MRI Prostate Lesion Segmentation.,"[{'@pid': '194/9137', 'text': 'Eleni Chiou'}, {'@pid': '205/3550', 'text': 'Francesco Giganti'}, {'@pid': '85/10169', 'text': 'Shonit Punwani'}, {'@pid': '21/4922', 'text': 'Iasonas Kokkinos'}, {'@pid': '29/7426', 'text': 'Eleftheria Panagiotaki'}]","['Domainadaptation', 'Imagesynthesis', 'GANs', 'Segmentation', 'MRI']","The need for training data can impede the adoption of novel imaging modalities for learning-based medical image analysis. Domain adaptation methods partially mitigate this problem by translating training data from a related source domain to a novel target domain, but typically assume that a one-to-one translation is possible. Our work addresses the challenge of adapting to a more informative target domain where multiple target samples can emerge from a single source sample. In particular we consider translating from mp-MRI to VERDICT, a richer MRI modality involving an optimized acquisition protocol for cancer characterization. We explicitly account for the inherent uncertainty of this mapping and exploit it to generate multiple outputs conditioned on a single input. Our results show that this allows us to extract systematically better image representations for the target domain, when used in tandem with both simple, CycleGAN-based baselines, as well as more powerful approaches that integrate discriminative segmentation losses and/or residual adapters. When compared to its deterministic counterparts, our approach yields substantial improvements across a broad range of dataset sizes, increasingly strong baselines, and evaluation measures.",2020/9/29,1,10.1007/978-3-030-59710-8_50
An Unsupervised Approach to Ultrasound Elastography with End-to-end Strain Regularisation.,"[{'@pid': '274/3295', 'text': 'R¨¦mi Delaunay'}, {'@pid': '45/5086', 'text': 'Yipeng Hu'}, {'@pid': '99/4387', 'text': 'Tom Vercauteren'}]","['Ultrasoundelastography', 'Timedelayestimation', 'Convolutionalneuralnetwork']","Quasi-static ultrasound elastography (USE) is an imaging modality that consists of determining a measure of deformation (i.e. strain) of soft tissue in response to an applied mechanical force. The strain is generally determined by estimating the displacement between successive ultrasound frames acquired before and after applying manual compression. The computational efficiency and accuracy of the displacement prediction, also known as time-delay estimation, are key challenges for real-time USE applications. In this paper, we present a novel deep-learning method for efficient time-delay estimation between ultrasound radio-frequency (RF) data. The proposed method consists of a convolutional neural network (CNN) that predicts a displacement field between a pair of pre- and post-compression ultrasound RF frames. The network is trained in an unsupervised way, by optimizing a similarity metric between the reference and compressed image. We also introduce a new regularization term that preserves displacement continuity by directly optimizing the strain smoothness. We validated the performance of our method by using both ultrasound simulation and in vivo data on healthy volunteers. We also compared the performance of our method with a state-of-the-art method called OVERWIND?
[17]. Average contrast-to-noise ratio (CNR) and signal-to-noise ratio (SNR) of our method in 30 simulation and 3 in vivo image pairs are 7.70 and 6.95, 7 and 0.31, respectively. Our results suggest that our approach can effectively predict accurate strain images. The unsupervised aspect of our approach represents a great potential for the use of deep learning application for the analysis of clinical ultrasound data.",2020/9/29,1,10.1007/978-3-030-59716-0_55
Isotropic Reconstruction of 3D EM Images with Unsupervised Degradation Learning.,"[{'@pid': '215/7734', 'text': 'Shiyu Deng'}, {'@pid': '136/9389', 'text': 'Xueyang Fu'}, {'@pid': '54/6827', 'text': 'Zhiwei Xiong'}, {'@pid': '16/4406', 'text': 'Chang Chen'}, {'@pid': '98/1737-2', 'text': 'Dong Liu 0002'}, {'@pid': '17/4378', 'text': 'Xuejin Chen'}, {'@pid': '52/3617', 'text': 'Qing Ling'}, {'@pid': '25/3972-1', 'text': 'Feng Wu 0001'}]","['Isotropicreconstruction', 'Unsupervisedlearning', 'EMimage']","The isotropic reconstruction of 3D electron microscopy (EM) images with low axial resolution is of great importance for biological analysis. Existing deep learning-based methods rely on handcrafted down-scaled training data, which does not model the real degradation accurately and thus leads to unsatisfying performance in practice. To address this problem, we propose a universal and unsupervised framework to simultaneously learn the real axial degradation and the isotropic reconstruction of 3D EM images. First, we train a degradation network using unpaired low-resolution (LR) and high-resolution (HR) slices, both of which are from real data, in an adversarial manner. Then, the degradation network is further used to generate realistic LR data from HR labels to form paired training data. In this way, the generated degraded data is consistent with the real axial degradation process, which guarantees the generalization ability of subsequent reconstruction networks to the real data. Our framework has the flexibility to work with different existing reconstruction methods. Experiments on both simulated and real anisotropic EM images validate the superiority of our framework.",2020/9/29,1,10.1007/978-3-030-59722-1_16
Automatic CAD-RADS Scoring Using Deep Learning.,"[{'@pid': '250/4005', 'text': 'Felix Denzinger'}, {'@pid': '25/6931', 'text': 'Michael Wels'}, {'@pid': '146/6667', 'text': 'Katharina Breininger'}, {'@pid': '57/947', 'text': 'Mehmet A. G¨¹ls¨¹n'}, {'@pid': '15/4913', 'text': 'Max Sch?binger'}, {'@pid': '275/6839', 'text': 'Florian Andr¨¦'}, {'@pid': '267/5335', 'text': 'Sebastian Bu?'}, {'@pid': '275/6773', 'text': 'Johannes G?rich'}, {'@pid': '52/6256', 'text': 'Michael S¨¹hling'}, {'@pid': '131/7133', 'text': 'Andreas K. Maier'}]","['CoronaryArteryDisease', 'CoronaryCTAngiography', 'Deeplearning', 'Datarepresentation', 'CAD-RADS']",Coronary CT angiography (CCTA) has established its role as a non-invasive modality for the diagnosis of coronary artery disease (CAD). The CAD-Reporting and Data System (CAD-RADS) has been developed to standardize communication and aid in decision making based on CCTA findings. The CAD-RADS score is determined by manual assessment of all coronary vessels and the grading of lesions within the coronary artery tree.,2020/9/29,1,10.1007/978-3-030-59725-2_5
Unlearning Scanner Bias for MRI Harmonisation.,"[{'@pid': '250/3863', 'text': 'Nicola K. Dinsdale'}, {'@pid': '67/24', 'text': 'Mark Jenkinson'}, {'@pid': '141/8805', 'text': 'Ana I. L. Namburete'}]","['Harmonisation', 'Jointdomainadaptation', 'MRI']","Combining datasets is vital for increased statistical power, especially for neurological conditions where limited data is available. However, variance due to differences in acquisition protocol and hardware limits our ability to combine datasets. We propose an iterative training scheme based on domain adaptation techniques, aiming to create scanner-invariant features while simultaneously maintaining overall performance on the main task. We demonstrate this on age prediction, but expect that our proposed training scheme will be applicable to any feedforward network and classification or regression task. We show that not only can we harmonise three MRI datasets from different studies, but can also successfully adapt the training to work with very biased datasets. The training scheme should, therefore, be applicable to most real-world data scenarios, enabling harmonisation for the task of interest.",2020/9/29,1,10.1007/978-3-030-59713-9_36
Towards Neuron Segmentation from Macaque Brain Images - A Weakly Supervised Approach.,"[{'@pid': '58/2821', 'text': 'Meng Dong'}, {'@pid': '98/1737-2', 'text': 'Dong Liu 0002'}, {'@pid': '54/6827', 'text': 'Zhiwei Xiong'}, {'@pid': '17/4378', 'text': 'Xuejin Chen'}, {'@pid': '124/7067', 'text': 'Yueyi Zhang'}, {'@pid': '23/1818', 'text': 'Zheng-Jun Zha'}, {'@pid': '168/4521', 'text': 'Guoqiang Bi'}, {'@pid': '25/3972-1', 'text': 'Feng Wu 0001'}]","['Instancesegmentation', 'Macaquebrain', 'Weaklysupervisedlearning']","The advance of microscopic imaging technology has enabled the observation of primate brain in its entirety and at single-neuron resolution. It is then an urgent need to develop means for automated analyses of these brain images, e.g. neuron segmentation. Deep learning is demonstrated an appealing approach for segmentation of natural images, but the success of deep learning is highly dependent on the large-scale and well-built training data that are costly to collect. In this paper, we take a step towards the goal of neuron segmentation from primate brain images, using a weakly supervised approach. We build ¨C to our best knowledge ¨C the first dual-channel three-dimensional image dataset of macaque brain for neuron segmentation. We propose two kinds of ¡°weak¡± labels, i.e. central points and rough masks, to prepare training data with an affordable cost. Accordingly, we design a weakly supervised learning method for neuron instance segmentation where instances can be easily extracted from the predicted peak-shape probability maps. Experimental results have shown the effectiveness of our approach. We also verify the efficiency of the proposed method on a public nuclei dataset. Our dataset and code have been published at https://braindata.bitahub.com/.",2020/9/29,1,10.1007/978-3-030-59722-1_19
Spatio-Temporal Consistency and Negative Label Transfer for 3D Freehand US Segmentation.,"[{'@pid': '275/6924', 'text': 'Vanessa Gonzalez Duque'}, {'@pid': '200/0326', 'text': 'Dawood Al Chanti'}, {'@pid': '275/6777', 'text': 'Marion Crouzier'}, {'@pid': '193/2527', 'text': 'Antoine Nordez'}, {'@pid': '275/6750', 'text': 'Lilian Lacourpaille'}, {'@pid': '55/6754', 'text': 'Diana Mateus'}]","['3-Dultrasound', 'Weaklysupervisedlearning', 'Guidedback-propagation', 'ConvolutionalLSTM', 'Fullyconvolutionalneuralnetworks.']","The manual segmentation of multiple organs in 3D ultrasound (US) sequences and volumes towards their quantitative analysis is very expensive and time-consuming. Fully supervised segmentation methods still require the collection of large volumes of annotated data while unlabeled images are abundant. In this work, we propose a novel semi-automatic deep learning approach modeled as a weak-label learning problem: given a few 2-D annotations for selected slices, the goal is to propagate the masks to the entire sequence. To this end, we make use of both positive and negative constraints induced by incomplete labels to penalize the segmentation loss function. Our model is composed of one encoder and two decoders to model the segmentation and an auxiliary reconstruction task. Moreover, we consider the spatio-temporal information by deploying a Convolutional Long Short Term Memory module. Our findings suggest that the reconstruction decoder and the Spatio-temporal information lead to a better geometrical estimation of the mask shape. We apply the model to the task of low-limb muscle segmentation in a dataset of 44 patients and 6160 images.",2020/9/29,1,10.1007/978-3-030-59710-8_69
Predicting Obstructive Hydronephrosis Based on Ultrasound Alone.,"[{'@pid': '192/1850', 'text': 'Lauren Erdman'}, {'@pid': '255/5167', 'text': 'Marta Skreta'}, {'@pid': '275/7063', 'text': 'Mandy Rickard'}, {'@pid': '200/8113', 'text': 'Carson McLean'}, {'@pid': '275/7067', 'text': 'Aziz Mezlini'}, {'@pid': '275/6958', 'text': 'Daniel T. Keefe'}, {'@pid': '275/6984', 'text': 'Anne-Sophie Blais'}, {'@pid': '10/6530', 'text': 'Michael Brudno'}, {'@pid': '275/6817', 'text': 'Armando Lorenzo'}, {'@pid': '06/3543', 'text': 'Anna Goldenberg'}]","['Deeplearning', 'Ultrasound', 'Pediatricurology', 'Hydronephrosis']","Prenatal hydronephrosis (HN) makes up nearly 30% of pediatric Urology Department visits, yet remains challenging to prognosticate without repeated ultrasounds and invasive clinical tests. We build a deep learning model, which uses still images from kidney ultrasound as input and predicts whether HN is due to an obstruction that will receive surgical intervention. We compare our custom convolutional neural network performance against other existing state-of-the-art models. Our best model predicts obstruction with an AUC of 0.93 and an AUPRC of 0.75 in a prospective test set of 89 patients (286 repeated kidney ultrasounds). We show that while maintaining a 5% false negative rate, our classifier identifies 58% of those who will have surgery due to obstruction yet received a functional renogram, indicating that this model could feasibly reduce the amount of testing done in more than half of non-surgical cases. This work demonstrates the ability of deep learning to predict obstructive HN with clinically relevant accuracy based on kidney ultrasound alone, without requiring other clinical variables as input. This algorithm has the potential to change clinical practice by stratifying HN patient risk, reducing repeated follow ups and invasive testing for less severe cases, and bringing more consistency to clinical management.",2020/9/29,1,10.1007/978-3-030-59716-0_47
Deep Learning Based Registration Using Spatial Gradients and Noisy Segmentation Labels.,"[{'@pid': '235/3498', 'text': 'Th¨¦o Estienne'}, {'@pid': '169/9108', 'text': 'Maria Vakalopoulou'}, {'@pid': '216/8496', 'text': 'Enzo Battistella'}, {'@pid': '250/3858', 'text': 'Alexandre Carr¨¦'}, {'@pid': '262/7676', 'text': 'Th¨¦ophraste Henry'}, {'@pid': '239/9835', 'text': 'Marvin Lerousseau'}, {'@pid': '235/3486', 'text': 'Charlotte Robert'}, {'@pid': 'p/NikosParagios', 'text': 'Nikos Paragios'}, {'@pid': '235/3472', 'text': 'Eric Deutsch'}]",[],"Image registration is one of the most challenging problems in medical image analysis. In the recent years, deep learning based approaches became quite popular, providing fast and performing registration strategies. In this short paper, we summarise our work presented on Learn2Reg challenge 2020. The main contributions of our work rely on (i) a symmetric formulation, predicting the transformations from source to target and from target to source simultaneously, enforcing the trained representations to be similar and (ii) integration of variety of publicly available datasets used both for pretraining and for augmenting segmentation labels. Our method reports a mean dice of 0.64 for task 3 and 0.85 for task 4 on the test sets, taking third place on the challenge. Our code and models are publicly available at https://github.com/TheoEst/abdominal_registration and https://github.com/TheoEst/hippocampus_registration.",2021/3/13,1,10.1007/978-3-030-71827-5_11
DMNet - Difference Minimization Network for Semi-supervised Segmentation in Medical Images.,"[{'@pid': '275/7008', 'text': 'Kang Fang'}, {'@pid': '26/988', 'text': 'Wu-Jun Li'}]","['Semanticsegmentation', 'Semi-supervisedlearning']","Semantic segmentation is an important task in medical image analysis. In general, training models with high performance needs a large amount of labeled data. However, collecting labeled data is typically difficult, especially for medical images. Several semi-supervised methods have been proposed to use unlabeled data to facilitate learning. Most of these methods use a self-training framework, in which the model cannot be well trained if the pseudo masks predicted by the model itself are of low quality. Co-training is another widely used semi-supervised method in medical image segmentation. It uses two models and makes them learn from each other. All these methods are not end-to-end. In this paper, we propose a novel end-to-end approach, called difference minimization network?(DMNet), for semi-supervised semantic segmentation. To use unlabeled data, DMNet adopts two decoder branches and minimizes the difference between soft masks generated by the two decoders. In this manner, each decoder can learn under the supervision of the other decoder, thus they can be improved at the same time. Also, to make the model generalize better, we force the model to generate low-entropy masks on unlabeled data so the decision boundary of model lies in low-density regions. Meanwhile, adversarial training strategy is adopted to learn a discriminator which can encourage the model to generate more accurate masks. Experiments on a kidney tumor dataset and a brain tumor dataset show that our method can outperform the baselines, including both supervised and semi-supervised ones, to achieve the best performance.",2020/9/29,1,10.1007/978-3-030-59710-8_52
Extreme Consistency - Overcoming Annotation Scarcity and Domain Shifts.,"[{'@pid': '194/1253', 'text': 'Gaurav Fotedar'}, {'@pid': '58/1008', 'text': 'Nima Tajbakhsh'}, {'@pid': '226/6985', 'text': 'Shilpa P. Ananth'}, {'@pid': '83/1323', 'text': 'Xiaowei Ding'}]","['Limitedannotation', 'Domainshift', 'Semi-supervisedlearning']","Supervised learning has proved effective for medical image analysis. However, it can utilize only the small labeled portion of data; it fails to leverage the large amounts of unlabeled data that is often available in medical image datasets. Supervised models are further handicapped by domain shifts, when the labeled dataset fails to cover different protocols or ethnicities. In this paper, we introduce extreme consistency, which overcomes the above limitations, by maximally leveraging unlabeled data from the same or a different domain in a teacher-student semi-supervised paradigm. Extreme consistency is the process of sending an extreme transformation of a given image to the student network and then constraining its prediction to be consistent with the teacher network¡¯s prediction for the original image. The extreme nature of our consistency loss distinguishes our method from related works that yield suboptimal performance by exercising only mild prediction consistency. Our method is 1) auto-didactic, as it requires no extra expert annotations; 2) versatile, as it handles both domain shift and limited annotation problems; 3) generic, as it is readily applicable to classification, segmentation, and detection tasks; and 4) simple to implement, as it requires no adversarial training. We evaluate our method for the tasks of lesion and retinal vessel segmentation in skin and fundus images. Our experiments demonstrate a significant performance gain over both modern supervised networks and recent semi-supervised models. This performance is attributed to the strong regularization enforced by extreme consistency, which enables the student network to learn how to handle extreme variants of both labeled and unlabeled images. This enhances the network¡¯s ability to tackle the inevitable same- and cross-domain data variability during inference.",2020/9/29,1,10.1007/978-3-030-59710-8_68
Joint Reconstruction and Bias Field Correction for Undersampled MR Imaging.,"[{'@pid': '271/1067', 'text': 'M¨¦lanie Gaillochet'}, {'@pid': '210/0842', 'text': 'Kerem Can Tezcan'}, {'@pid': '45/7041', 'text': 'Ender Konukoglu'}]","['MRIreconstruction', 'Deeplearning', 'Biasfield.']","Undersampling the k-space in MRI allows saving precious acquisition time, yet results in an ill-posed inversion problem. Recently, many deep learning techniques have been developed, addressing this issue of recovering the fully sampled MR image from the undersampled data. However, these learning based schemes are susceptible to differences between the training data and the image to be reconstructed at test time. One such difference can be attributed to the bias field present in MR images, caused by field inhomogeneities and coil sensitivities. In this work, we address the sensitivity of the reconstruction problem to the bias field and propose to model it explicitly in the reconstruction, in order to decrease this sensitivity. To this end, we use an unsupervised learning based reconstruction algorithm as our basis and combine it with a N4-based bias field estimation method, in a joint optimization scheme. We use the HCP dataset as well as in-house measured images for the evaluations. We show that the proposed method improves the reconstruction quality, both visually and in terms of RMSE.",2020/9/29,1,10.1007/978-3-030-59713-9_5
Generalizing Spatial Transformers to Projective Geometry with Applications to 2D/3D Registration.,"[{'@pid': '132/6237-3', 'text': 'Cong Gao 0003'}, {'@pid': '04/1006', 'text': 'Xingtong Liu'}, {'@pid': '221/1031', 'text': 'Wenhao Gu'}, {'@pid': '249/5517', 'text': 'Benjamin Killeen'}, {'@pid': '26/5909', 'text': 'Mehran Armand'}, {'@pid': '71/2446', 'text': 'Russell H. Taylor'}, {'@pid': '165/8137', 'text': 'Mathias Unberath'}]",[],"Differentiable rendering is a technique to connect 3D scenes with corresponding 2D images. Since it is differentiable, processes during image formation can be learned. Previous approaches to differentiable rendering focus on mesh-based representations of 3D scenes, which is inappropriate for medical applications where volumetric, voxelized models are used to represent anatomy. We propose a novel Projective Spatial Transformer module that generalizes spatial transformers to projective geometry, thus enabling differentiable volume rendering. We demonstrate the usefulness of this architecture on the example of 2D/3D registration between radiographs and CT scans. Specifically, we show that our transformer enables end-to-end learning of an image processing and projection model that approximates an image similarity function that is convex with respect to the pose parameters, and can thus be optimized effectively using conventional gradient descent. To the best of our knowledge, we are the first to describe the spatial transformers in the context of projective transmission imaging, including rendering and pose estimation. We hope that our developments will benefit related 3D research applications. The source code is available at https://github.com/gaocong13/Projective-Spatial-Transformers.",2020/9/29,1,10.1007/978-3-030-59716-0_32
Renal Cell Carcinoma Detection and Subtyping with Minimal Point-Based Annotation in Whole-Slide Images.,"[{'@pid': '189/3729', 'text': 'Zeyu Gao'}, {'@pid': '234/3414', 'text': 'Pargorn Puttapirat'}, {'@pid': '250/2175', 'text': 'Jiangbo Shi'}, {'@pid': 'l/ChenLi32', 'text': 'Chen Li 0032'}]","['Detection', 'Subtyping', 'Min-Pointannotation']","Cancerous region detection and subtyping in whole-slide images (WSIs) are fundamental for renal cell carcinoma (RCC) diagnosis. The main challenge in the development of automated RCC diagnostic systems is the lack of large-scale datasets with precise annotations. In this paper, we propose a framework that employs a semi-supervised learning (SSL) method to accurately detect cancerous regions with a novel annotation method called Minimal Point-Based (Min-Point) annotation. The predicted results are efficiently utilized by a hybrid loss training strategy in a classification model for subtyping. The annotator only needs to mark a few cancerous and non-cancerous points in each WSI. The experiments on three significant subtypes of RCC proved that the performance of the cancerous region detector trained with the Min-Point annotated dataset is comparable to the classifiers trained on the dataset with full cancerous region delineation. In subtyping, the proposed model outperforms the model trained with only whole-slide diagnostic labels by 12% in terms of the testing f1-score. We believe that our ¡°detect then classify¡± schema combined with the Min-Point annotation would set a standard for developing intelligent systems with similar challenges.",2020/9/29,1,10.1007/978-3-030-59722-1_42
A Distance-Based Loss for Smooth and Continuous Skin Layer Segmentation in Optoacoustic Images.,"[{'@pid': '270/3730', 'text': 'Stefan Gerl'}, {'@pid': '239/8775', 'text': 'Johannes C. Paetzold'}, {'@pid': '54/10269', 'text': 'Hailong He'}, {'@pid': '244/2240', 'text': 'Ivan Ezhov'}, {'@pid': '154/8612', 'text': 'Suprosanna Shit'}, {'@pid': '239/8604', 'text': 'Florian Kofler'}, {'@pid': '257/3280', 'text': 'Amirhossein Bayat'}, {'@pid': '199/1967', 'text': 'Giles Tetteh'}, {'@pid': '26/6344', 'text': 'Vasilis Ntziachristos'}, {'@pid': '74/6505', 'text': 'Bjoern H. Menze'}]",[],"Raster-scan optoacoustic mesoscopy (RSOM) is a powerful, non-invasive optical imaging technique for functional, anatomical, and molecular skin and tissue analysis. However, both the manual and the automated analysis of such images are challenging, because the RSOM images have very low contrast, poor signal to noise ratio, and systematic overlaps between the absorption spectra of melanin and hemoglobin. Nonetheless, the segmentation of the epidermis layer is a crucial step for many downstream medical and diagnostic tasks, such as vessel segmentation or monitoring of cancer progression. We propose a novel, shape-specific loss function that overcomes discontinuous segmentations and achieves smooth segmentation surfaces while preserving the same volumetric Dice and IoU. Further, we validate our epidermis segmentation through the sensitivity of vessel segmentation. We found a 20\(\%\) improvement in Dice for vessel segmentation tasks when the epidermis mask is provided as additional information to the vessel segmentation network.",2020/9/29,1,10.1007/978-3-030-59725-2_30
Weakly Supervised Organ Localization with Attention Maps Regularized by Local Area Reconstruction.,"[{'@pid': '210/6752', 'text': 'Heng Guo'}, {'@pid': '182/8040', 'text': 'Minfeng Xu'}, {'@pid': '26/16', 'text': 'Ying Chi'}, {'@pid': '64/5666-6', 'text': 'Lei Zhang 0006'}, {'@pid': '56/5807-1', 'text': 'Xian-Sheng Hua 0001'}]","['Organlocalization', 'Localareareconstruction', 'Attentionmap.']","Fully supervised methods with numerous dense-labeled training data have achieved accurate localization results for anatomical structures. However, obtaining such a dedicated dataset usually requires clinical expertise and time-consuming annotation process. In this work, we tackle the organ localization problem under the setting of image-level annotations. Previous Class Activation Map (CAM) and its derivatives have proved that discriminative regions of images can be located with basic classification networks. To improve the representative capacity of attention maps generated by CAMs, a novel learning-based Local Area Reconstruction (LAR) method is proposed. Our weakly supervised organ localization network, namely OLNet, can generate high-resolution attention maps that preserve fine-detailed target anatomical structures. Online generated pseudo ground-truth is utilized to impose geometric constraints on attention maps. Extensive experiments on In-house Chest CT Dataset and Kidney Tumor Segmentation Benchmark (KiTS19) show that our approach can provide promising localization results both in saliency map and semantic segmentation perspectives.",2020/9/29,1,10.1007/978-3-030-59710-8_24
Sensorless Freehand 3D Ultrasound Reconstruction via Deep Contextual Learning.,"[{'@pid': '228/8243', 'text': 'Hengtao Guo'}, {'@pid': '10/1887-1', 'text': 'Sheng Xu 0001'}, {'@pid': '66/3377', 'text': 'Bradford J. Wood'}, {'@pid': 'y/PingkunYan', 'text': 'Pingkun Yan'}]","['Ultrasoundvolumereconstruction', 'Deeplearning', 'Imageguidedintervention']","Transrectal ultrasound (US) is the most commonly used imaging modality to guide prostate biopsy and its 3D volume provides even richer context information. Current methods for 3D volume reconstruction from freehand US scans require external tracking devices to provide spatial position for every frame. In this paper, we propose a deep contextual learning network (DCL-Net), which can efficiently exploit the image feature relationship between US frames and reconstruct 3D US volumes without any tracking device. The proposed DCL-Net utilizes 3D convolutions over a US video segment for feature extraction. An embedded self-attention module makes the network focus on the speckle-rich areas for better spatial movement prediction. We also propose a novel case-wise correlation loss to stabilize the training process for improved accuracy. Highly promising results have been obtained by using the developed method. The experiments with ablation studies demonstrate superior performance of the proposed method by comparing against other state-of-the-art methods. Source code of this work is publicly available at https://github.com/DIAL-RPI/FreehandUSRecon.",2020/9/29,1,10.1007/978-3-030-59716-0_44
Deep Selection - A Fully Supervised Camera Selection Network for Surgery Recordings.,"[{'@pid': '200/0177', 'text': 'Ryo Hachiuma'}, {'@pid': '41/10012', 'text': 'Tomohiro Shimizu'}, {'@pid': '12/6217', 'text': 'Hideo Saito'}, {'@pid': '257/7010', 'text': 'Hiroki Kajita'}, {'@pid': '257/6914', 'text': 'Yoshifumi Takatsume'}]","['Surgeryrecording', 'Cameraselection', 'Deepneuralnetwork']","Recording surgery in operating rooms is an essential task for education and evaluation of medical treatment. However, recording the desired targets, such as the surgery field, surgical tools, or doctor¡¯s hands, is difficult because the targets are heavily occluded during surgery. We use a recording system in which multiple cameras are embedded in the surgical lamp, and we assume that at least one camera is recording the target without occlusion at any given time. As the embedded cameras obtain multiple video sequences, we address the task of selecting the camera with the best view of the surgery. Unlike the conventional method, which selects the camera based on the area size of the surgery field, we propose a deep neural network that predicts the camera selection probability from multiple video sequences by learning the supervision of the expert annotation. We created a dataset in which six different types of plastic surgery are recorded, and we provided the annotation of camera switching. Our experiments show that our approach successfully switched between cameras and outperformed three baseline methods.",2020/9/29,1,10.1007/978-3-030-59716-0_40
Nonlinear Regression on Manifolds for Shape Analysis using Intrinsic B¨¦zier Splines.,"[{'@pid': '275/6832', 'text': 'Martin Hanik'}, {'@pid': '49/283', 'text': 'Hans-Christian Hege'}, {'@pid': '90/3388', 'text': 'Anja Hennemuth'}, {'@pid': '45/4801', 'text': 'Christoph von Tycowicz'}]","['Shapetrajectory', 'Manifold-valuedB¨¦ziercurves', 'Splineregression', 'Riemanniangeometry']","Intrinsic and parametric regression models are of high interest for the statistical analysis of manifold-valued data such as images and shapes. The standard linear ansatz has been generalized to geodesic regression on manifolds making it possible to analyze dependencies of random variables that spread along generalized straight lines. Nevertheless, in some scenarios, the evolution of the data cannot be modeled adequately by a geodesic. We present a framework for nonlinear regression on manifolds by considering Riemannian splines, whose segments are B¨¦zier curves, as trajectories. Unlike variational formulations that require time-discretization, we take a constructive approach that provides efficient and exact evaluation by virtue of the generalized de Casteljau algorithm. We validate our method in experiments on the reconstruction of periodic motion of the mitral valve as well as the analysis of femoral shape changes during the course of osteoarthritis, endorsing B¨¦zier spline regression as an effective and flexible tool for manifold-valued regression.",2020/9/29,1,10.1007/978-3-030-59719-1_60
Self Domain Adapted Network.,"[{'@pid': '167/4028', 'text': 'Yufan He'}, {'@pid': '40/2041', 'text': 'Aaron Carass'}, {'@pid': '239/2014', 'text': 'Lianrui Zuo'}, {'@pid': '180/9476', 'text': 'Blake E. Dewey'}, {'@pid': 'p/JerryLPrince', 'text': 'Jerry L. Prince'}]","['Unsuperviseddomainadaptation', 'Selfsupervisedlearning', 'Segmentation', 'Synthesis']","Domain shift is a major problem for deploying deep networks in clinical practice. Network performance drops significantly with (target) images obtained differently than its (source) training data. Due to a lack of target label data, most work has focused on unsupervised domain adaptation?(UDA). Current UDA methods need both source and target data to train models which perform image translation?(harmonization) or learn domain-invariant features. However, training a model for each target domain is time consuming and computationally expensive, even infeasible when target domain data are scarce or source data are unavailable due to data privacy. In this paper, we propose a novel self domain adapted network?(SDA-Net) that can rapidly adapt itself to a single test subject at the testing stage, without using extra data or training a UDA model. The SDA-Net consists of three parts: adaptors, task model, and auto-encoders. The latter two are pre-trained offline on labeled source images. The task model performs tasks like synthesis, segmentation, or classification, which may suffer from the domain shift problem. At the testing stage, the adaptors are trained to transform the input test image and features to reduce the domain shift as measured by the auto-encoders, and thus perform domain adaptation. We validated our method on retinal layer segmentation from different OCT scanners and T1 to T2 synthesis with T1 from different MRI scanners and with different imaging parameters. Results show that our SDA-Net, with a single test subject and a short amount of time for self adaptation at the testing stage, can achieve significant improvements.",2020/9/29,1,10.1007/978-3-030-59710-8_43
Learning Hybrid Representations for Automatic 3D Vessel Centerline Extraction.,"[{'@pid': '275/6831', 'text': 'Jiafa He'}, {'@pid': '190/5330', 'text': 'Chengwei Pan'}, {'@pid': '38/6056-2', 'text': 'Can Yang 0002'}, {'@pid': '73/1844-4', 'text': 'Ming Zhang 0004'}, {'@pid': '181/2842', 'text': 'Yang Wang'}, {'@pid': '30/1273', 'text': 'Xiaowei Zhou'}, {'@pid': '90/6896', 'text': 'Yizhou Yu'}]","['Centerlineextraction', 'Vesselsegmentation', 'Hybridrepresentations']","Automatic blood vessel extraction from 3D medical images is crucial for vascular disease diagnoses. Existing methods based on convolutional neural networks (CNNs) may suffer from discontinuities of extracted vessels when segmenting such thin tubular structures from 3D images. We argue that preserving the continuity of extracted vessels requires to take into account the global geometry. However, 3D convolutions are computationally inefficient, which prohibits the 3D CNNs from sufficiently large receptive fields to capture the global cues in the entire image. In this work, we propose a hybrid representation learning approach to address this challenge. The main idea is to use CNNs to learn local appearances of vessels in image crops while using another point-cloud network to learn the global geometry of vessels in the entire image. In inference, the proposed approach extracts local segments of vessels using CNNs, classifies each segment based on global geometry using the point-cloud network, and finally connects all the segments that belong to the same vessel using the shortest-path algorithm. This combination results in an efficient, fully-automatic and template-free approach to centerline extraction from 3D images. We validate the proposed approach on CTA datasets and demonstrate its superior performance compared to both traditional and CNN-based baselines.",2020/9/29,1,10.1007/978-3-030-59725-2_3
Dynamic Memory to Alleviate Catastrophic Forgetting in Continuous Learning Settings.,"[{'@pid': '169/4789', 'text': 'Johannes Hofmanninger'}, {'@pid': '226/5088', 'text': 'Matthias Perkonigg'}, {'@pid': '23/5262', 'text': 'James A. Brink'}, {'@pid': '269/9593', 'text': 'Oleg Pianykh'}, {'@pid': '58/8585', 'text': 'Christian Herold'}, {'@pid': '15/366', 'text': 'Georg Langs'}]","['Continuouslearning', 'Domainadaptation', 'Dynamicmemory']","In medical imaging, technical progress or changes in diagnostic procedures lead to a continuous change in image appearance. Scanner manufacturer, reconstruction kernel, dose, other protocol specific settings or administering of contrast agents are examples that influence image content independent of the scanned biology. Such domain and task shifts limit the applicability of machine learning algorithms in the clinical routine by rendering models obsolete over time. Here, we address the problem of data shifts in a continuous learning scenario by adapting a model to unseen variations in the source domain while counteracting catastrophic forgetting effects. Our method uses a dynamic memory to facilitate rehearsal of a diverse training data subset to mitigate forgetting. We evaluated our approach on routine clinical CT data obtained with two different scanner protocols and synthetic classification tasks. Experiments show that dynamic memory counters catastrophic forgetting in a setting with multiple data shifts without the necessity for explicit knowledge about when these shifts occur.",2020/9/29,1,10.1007/978-3-030-59713-9_35
Brain MR to PET Synthesis via Bidirectional Generative Adversarial Network.,"[{'@pid': '272/5567', 'text': 'Shengye Hu'}, {'@pid': '74/2905', 'text': 'Yanyan Shen'}, {'@pid': '130/5774', 'text': 'Shuqiang Wang'}, {'@pid': '49/9638', 'text': 'Baiying Lei'}]","['Medicalimagingsynthesis', 'Generativeadversarialnetwork', 'Bidirectionalmappingmechanism']","Fusing multi-modality medical images, such as MR and PET, can provide complementary information to improve the diagnostic performance. But compared to the substantial and available MR data, PET data is always deficient. In this paper, we propose a novel end-to-end network, called Bidirectional GAN, where image contexts and latent vector are effectively used and jointly optimized for brain MR-to-PET synthesis. Specifically, a bidirectional mapping mechanism is designed to embed the diverse brain structural details into the high-dimensional latent space. And then the superior network architecture and the modified loss functions are further utilized to enhance the quality of synthetic images. The most appealing part is that the proposed method can synthesize the plausible PET images while preserving the diverse brain structures of different subjects. The experiments demonstrate that the performance of the proposed method outperforms the state-of-the-art methods in terms of quantitative measures, qualitative evaluation and the improvement of classification accuracy.",2020/9/29,1,10.1007/978-3-030-59713-9_67
Grading Loss - A Fracture Grade-Based Metric Loss for Vertebral Fracture Detection.,"[{'@pid': '258/4149', 'text': 'Malek El Husseini'}, {'@pid': '198/0458', 'text': 'Anjany Sekuboyina'}, {'@pid': '141/6051', 'text': 'Maximilian L?ffler'}, {'@pid': '03/7527', 'text': 'Fernando Navarro'}, {'@pid': '74/6505', 'text': 'Bjoern H. Menze'}, {'@pid': '181/6853', 'text': 'Jan S. Kirschke'}]","['Fracturedetection', 'Metricloss', 'Representationlearning']","Osteoporotic vertebral fractures have a severe impact on patients¡¯ overall well-being but are severely under-diagnosed. These fractures present themselves at various levels of severity measured using the Genant¡¯s grading scale. Insufficient annotated datasets, severe data-imbalance, and minor difference in appearances between fractured and healthy vertebrae make naive classification approaches result in poor discriminatory performance. Addressing this, we propose a representation learning-inspired approach for automated vertebral fracture detection, aimed at learning latent representations efficient for fracture detection. Building on state-of-art metric losses, we present a novel Grading Loss for learning representations that respect Genant¡¯s fracture grading scheme. On a publicly available spine dataset, the proposed loss function achieves a fracture detection F1 score of 81.5%, a 10% increase over a naive classification baseline.
",2020/9/29,1,10.1007/978-3-030-59725-2_71
Characterizing Intra-soma Diffusion with Spherical Mean Spectrum Imaging.,"[{'@pid': '244/1829', 'text': 'Khoi Minh Huynh'}, {'@pid': '50/3141-1', 'text': 'Ye Wu 0001'}, {'@pid': '13/1611', 'text': 'Kim-Han Thung'}, {'@pid': '44/10619', 'text': 'Sahar Ahmad'}, {'@pid': '250/3868', 'text': 'Hoyt Patrick Taylor IV'}, {'@pid': '14/4383', 'text': 'Dinggang Shen'}, {'@pid': '93/5188', 'text': 'Pew-Thian Yap'}]",[],"Most brain microstructure models are dedicated to the quantification of white matter microstructure, using for example sticks, cylinders, and zeppelins to model intra- and extra-axonal environments. Gray matter presents unique micro-architecture with cell bodies (somas) exhibiting diffusion characteristics that differ from axons in white matter. In this paper, we introduce a method to quantify soma microstructure, giving measures such as volume fraction, diffusivity, and kurtosis. Our method captures a spectrum of diffusion patterns and scales and does not rely on restrictive model assumptions. We show that our method yields unique and meaningful contrasts that are in agreement with histological data. We demonstrate its application in the mapping of the distinct spatial patterns of soma density in the cortex.",2020/9/29,1,10.1007/978-3-030-59728-3_35
Learning and Reasoning with the Graph Structure Representation in Robotic Surgery.,"[{'@pid': '48/10644', 'text': 'Mobarakol Islam'}, {'@pid': '256/8256', 'text': 'Seenivasan Lalithkumar'}, {'@pid': '270/0343', 'text': 'Lim Chwee Ming'}, {'@pid': '44/3343', 'text': 'Hongliang Ren 0001'}]",[],"Learning to infer graph representations and performing spatial reasoning in a complex surgical environment can play a vital role in surgical scene understanding in robotic surgery. For this purpose, we develop an approach to generate the scene graph and predict surgical interactions between instruments and surgical region of interest (ROI) during robot-assisted surgery. We design an attention link function and integrate with a graph parsing network to recognize the surgical interactions. To embed each node with corresponding neighbouring node features, we further incorporate SageConv into the network. The scene graph generation and active edge classification mostly depend on the embedding or feature extraction of node and edge features from complex image representation. Here, we empirically demonstrate the feature extraction methods by employing label smoothing weighted loss. Smoothing the hard label can avoid the over-confident prediction of the model and enhances the feature representation learned by the penultimate layer. To obtain the graph scene label, we annotate the bounding box and the instrument-ROI interactions on the robotic scene segmentation challenge 2018 dataset with an experienced clinical expert in robotic surgery and employ it to evaluate our propositions.",2020/9/29,1,10.1007/978-3-030-59716-0_60
INSIDE - Steering Spatial Attention with Non-imaging Information in CNNs.,"[{'@pid': '244/9587', 'text': 'Grzegorz Jacenk¨®w'}, {'@pid': '167/9700', 'text': 'Alison Q. O&apos;Neil'}, {'@pid': '146/3646', 'text': 'Brian Mohr'}, {'@pid': '14/613', 'text': 'Sotirios A. Tsaftaris'}]","['Attention', 'Conditioning', 'Non-imaging', 'Segmentation']","We consider the problem of integrating non-imaging information into segmentation networks to improve performance. Conditioning layers such as FiLM provide the means to selectively amplify or suppress the contribution of different feature maps in a linear fashion. However, spatial dependency is difficult to learn within a convolutional paradigm. In this paper, we propose a mechanism to allow for spatial localisation conditioned on non-imaging information, using a feature-wise attention mechanism comprising a differentiable parametrised function (e.g. Gaussian), prior to applying the feature-wise modulation. We name our method INstance modulation with SpatIal DEpendency (INSIDE). The conditioning information might comprise any factors that relate to spatial or spatio-temporal information such as lesion location, size, and cardiac cycle phase. Our method can be trained end-to-end and does not require additional supervision. We evaluate the method on two datasets: a new CLEVR-Seg dataset where we segment objects based on location, and the ACDC dataset conditioned on cardiac phase and slice location within the volume. Code and the CLEVR-Seg dataset are available at https://github.com/jacenkow/inside.",2020/9/29,1,10.1007/978-3-030-59719-1_38
Complex Cancer Detector - Complex Neural Networks on Non-stationary Time Series for Guiding Systematic Prostate Biopsy.,"[{'@pid': '206/7631', 'text': 'Golara Javadi'}, {'@pid': '221/3321', 'text': 'Minh Nguyen Nhat To'}, {'@pid': '32/705', 'text': 'Samareh Samadi'}, {'@pid': '204/0754', 'text': 'Sharareh Bayat'}, {'@pid': '145/4919', 'text': 'Samira Sojoudi'}, {'@pid': '25/11134', 'text': 'Antonio Hurtado'}, {'@pid': '130/3399', 'text': 'Silvia D. Chang'}, {'@pid': '172/3692', 'text': 'Peter C. Black'}, {'@pid': '95/1924', 'text': 'Parvin Mousavi'}, {'@pid': '30/909', 'text': 'Purang Abolmaesumi'}]","['Complexneuralnetworks', 'Temporalenhancedultrasound', 'Imageguidedinterventions', 'Prostatecancer']","Ultrasound is a common imaging modality used for targeting suspected cancerous tissue in prostate biopsy. Since ultrasound images have very low specificity and sensitivity for visualizing the cancer foci, a significant body of literature have aimed to develop ultrasound tissue characterization solutions to alleviate this issue. Major challenges are the substantial heterogeneity in data, and the noisy, limited number of labeled data available from pathology of biopsy samples. A recently proposed tissue characterization method uses spectral analysis of time series of ultrasound data taken during the biopsy procedure combined with deep networks. However, the real-value transformations in these networks neglect the phase information of the signal. In this paper, we study the importance of phase information and compare different ways of extracting reliable features including complex neural networks. These networks can help with analyzing the phase information to use the full capacity of the data. Our results show that the phase content can stabilize training specially with non-stationary time series. The proposed approach is generic and can be applied to several other scenarios where the phase information is important and noisy labels are present.",2020/9/29,1,10.1007/978-3-030-59716-0_50
UXNet - Searching Multi-level Feature Aggregation for 3D Medical Image Segmentation.,"[{'@pid': '227/4488', 'text': 'Yuanfeng Ji'}, {'@pid': '54/10697', 'text': 'Ruimao Zhang'}, {'@pid': '74/2397-26', 'text': 'Zhen Li 0026'}, {'@pid': '157/9475', 'text': 'Jiamin Ren'}, {'@pid': '53/3894', 'text': 'Shaoting Zhang'}, {'@pid': '54/4989-2', 'text': 'Ping Luo 0002'}]",[],"Aggregating multi-level feature representation plays a critical role in achieving robust volumetric medical image segmentation, which is important for the auxiliary diagnosis and treatment. Unlike the recent neural architecture search (NAS) methods that typically searched the optimal operators in each network layer, but missed a good strategy to search for feature aggregations, this paper proposes a novel NAS method for 3D medical image segmentation, named UXNet, which searches both the scale-wise feature aggregation strategies as well as the block-wise operators in the encoder-decoder network. UXNet has several appealing benefits. (1) It significantly improves flexibility of the classical UNet architecture, which only aggregates feature representations of encoder and decoder in equivalent resolution. (2) A continuous relaxation of UXNet is carefully designed, enabling its searching scheme performed in an efficient differentiable manner. (3) Extensive experiments demonstrate the effectiveness of UXNet compared with recent NAS methods for medical image segmentation. The architecture discovered by UXNet outperforms existing state-of-the-art models in terms of Dice on several public 3D medical image segmentation benchmarks, especially for the boundary locations and tiny tissues. The searching computational complexity of UXNet is cheap, enabling to search a network with best performance less than 1.5 days on two TitanXP GPUs.",2020/9/29,1,10.1007/978-3-030-59710-8_34
Learning High-Resolution and Efficient Non-local Features for Brain Glioma Segmentation in MR Images.,"[{'@pid': '201/7583', 'text': 'Haozhe Jia'}, {'@pid': '50/2433', 'text': 'Yong Xia'}, {'@pid': 'c/WeidongCai', 'text': 'Weidong Cai 0001'}, {'@pid': '03/281', 'text': 'Heng Huang'}]","['Braingliomasegmentation', 'High-resolutionfeaturerepresentation', 'Lightweightnon-localmodule']","Brain glioma segmentation using multi-parametric magnetic resonance (MR) imaging has significant clinical value. Although 3D convolutional neural networks (CNNs) have become increasingly prevalent in delivering this segmentation task, these models still suffer from an insufficient ability to high-resolution feature representation for small and irregular regions, limited local receptive fields, and poor long-range dependencies. In this paper, we propose a 3D High-resolution and Non-local Feature Network (HNF-Net) for brain glioma segmentation using multi-parametric MR imaging. We construct HNF-Net based mainly on the parallel multi-scale fusion (PMF) module, which helps produce strong high-resolution feature representation and aggregate multi-scale contextual information. We also introduce the expectation-maximization attention (EMA) module to HNF-Net, aiming to capture the long-range dependent contextual information and reduce the feature redundancy in a lightweight fashion. We evaluated our HNF-Net on the BraTS 2019 Challenge dataset against eight top-ranking methods listed on the challenge leaderboard. Our results suggest that the proposed HNF-Net achieves improved overall performance over these methods, and our ablation study demonstrates the effectiveness of the PMF module and EMA module.",2020/9/29,1,10.1007/978-3-030-59719-1_47
A Shared Neural Encoding Model for the Prediction of Subject-Specific fMRI Response.,"[{'@pid': '222/2059', 'text': 'Meenakshi Khosla'}, {'@pid': '148/9698', 'text': 'Gia H. Ngo'}, {'@pid': '222/1578', 'text': 'Keith Jamison'}, {'@pid': '77/9339', 'text': 'Amy Kuceyeski'}, {'@pid': '36/4898', 'text': 'Mert R. Sabuncu'}]",[],"The increasing popularity of naturalistic paradigms in fMRI (such as movie watching) demands novel strategies for multi-subject data analysis, such as use of neural encoding models. In the present study, we propose a shared convolutional neural encoding method that accounts for individual-level differences. Our method leverages multi-subject data to improve the prediction of subject-specific responses evoked by visual or auditory stimuli. We showcase our approach on high-resolution 7T fMRI data from the Human Connectome Project movie-watching protocol and demonstrate significant improvement over single-subject encoding models. We further demonstrate the ability of the shared encoding model to successfully capture meaningful individual differences in response to traditional task-based facial and scenes stimuli. Taken together, our findings suggest that inter-subject knowledge transfer can be beneficial to subject-specific predictive models. (Our code is available at https://github.com/mk2299/SharedEncoding_MICCAI.)",2020/9/29,1,10.1007/978-3-030-59728-3_53
Fairness of Classifiers Across Skin Tones in Dermatology.,"[{'@pid': '251/8887', 'text': 'Newton M. Kinyanjui'}, {'@pid': '251/8769', 'text': 'Timothy Odonga'}, {'@pid': '199/3961', 'text': 'Celia Cintas'}, {'@pid': '172/1174', 'text': 'Noel C. F. Codella'}, {'@pid': '126/0986', 'text': 'Rameswar Panda'}, {'@pid': '00/7428', 'text': 'Prasanna Sattigeri'}, {'@pid': '02/3069', 'text': 'Kush R. Varshney'}]","['Algorithmicfairness', 'Dermatologyimageanalysis', 'Medicalimaging']","Recent advances in computer vision have led to breakthroughs in the development of automated skin image analysis. However, no attempt has been made to evaluate the consistency in performance across populations with varying skin tones. In this paper, we present an approach to estimate skin tone in skin disease benchmark datasets and investigate whether model performance is dependent on this measure. Specifically, we use individual typology angle (ITA) to approximate skin tone in dermatology datasets. We look at the distribution of ITA values to better understand skin color representation in two benchmark datasets: 1) the ISIC 2018 Challenge dataset, a collection of dermoscopic images of skin lesions for the detection of skin cancer, and 2) the SD-198 dataset, a collection of clinical images capturing a wide variety of skin diseases. To estimate ITA, we first develop segmentation models to isolate non-diseased areas of skin. We find that the majority of the data in the two datasets have ITA values between 34.5\(^\circ \) and 48\(^\circ \), which are associated with lighter skin, and is consistent with under-representation of darker skinned populations in these datasets. We also find no measurable correlation between accuracy of machine learning models and ITA values, though more comprehensive data is needed for further validation.",2020/9/29,1,10.1007/978-3-030-59725-2_31
Contour-Based Bone Axis Detection for X-Ray Guided Surgery on the Knee.,"[{'@pid': '235/2584', 'text': 'Florian Kordon'}, {'@pid': '131/7133', 'text': 'Andreas K. Maier'}, {'@pid': '197/5047', 'text': 'Benedict Swartman'}, {'@pid': '245/7473', 'text': 'Maxim Privalov'}, {'@pid': '275/6899', 'text': 'Jan Siad El Barbari'}, {'@pid': '23/22', 'text': 'Holger Kunze'}]","['Surgicalplanning', 'Orthopedics', 'Boneaxisdetection', 'X-rayimaging', 'Intra-operativeguidance']","The anatomical axis of long bones is an important reference line for guiding fracture reduction and assisting in the correct placement of guide pins, screws, and implants in orthopedics and trauma surgery. This study investigates an automatic approach for detection of such axes on X-ray images based on the segmentation contour of the bone. For this purpose, we use the medically established two-line method and translate it into a learning-based approach. The proposed method is evaluated on 38 clinical test images of the femoral and tibial bone and achieves a median angulation error of \(0.19^{\circ }\) and \(0.33^{\circ }\) respectively. An inter-rater study with three trauma surgery experts confirms reliability of the method and recommends further clinical application.",2020/9/29,1,10.1007/978-3-030-59725-2_65
Automatic Localization of Landmarks in Craniomaxillofacial CBCT Images Using a Local Attention-Based Graph Convolution Network.,"[{'@pid': '152/3374', 'text': 'Yankun Lang'}, {'@pid': '161/1328', 'text': 'Chunfeng Lian'}, {'@pid': '205/5417', 'text': 'Deqiang Xiao'}, {'@pid': '226/3340', 'text': 'Hannah H. Deng'}, {'@pid': '99/1838-1', 'text': 'Peng Yuan 0001'}, {'@pid': '96/5540', 'text': 'Jaime Gateno'}, {'@pid': '39/5095', 'text': 'Steve Guo-Fang Shen'}, {'@pid': '210/9419', 'text': 'David M. Alfi'}, {'@pid': '93/5188', 'text': 'Pew-Thian Yap'}, {'@pid': '67/7453', 'text': 'James J. Xia'}, {'@pid': '14/4383', 'text': 'Dinggang Shen'}]","['Craniomaxillofacial(CMF)surgery', 'Landmarklocalization', 'GCN', 'Deeplearning']","Landmark localization is an important step in quantifying craniomaxillofacial (CMF) deformities and designing treatment plans of reconstructive surgery. However, due to the severity of deformities and defects (partially missing anatomy), it is difficult to automatically and accurately localize a large set of landmarks simultaneously. In this work, we propose two cascaded networks for digitizing 60 anatomical CMF landmarks in cone-beam computed tomography (CBCT) images. The first network is a U-Net that outputs heatmaps for landmark locations and landmark features extracted with a local attention mechanism. The second network is a graph convolution network that takes the features extracted by the first network as input and determines whether each landmark exists via binary classification. We evaluated our approach on 50 sets of CBCT scans of patients with CMF deformities and compared them with state-of-the-art methods. The results indicate that our approach can achieve an average detection error of 1.47?mm with a false positive rate of 19%, outperforming related methods.",2020/9/29,1,10.1007/978-3-030-59719-1_79
Automated Measurements of Key Morphological Features of Human Embryos for IVF.,"[{'@pid': '275/6789', 'text': 'Brian D. Leahy'}, {'@pid': '124/7034', 'text': 'Won-Dong Jang'}, {'@pid': '275/6944', 'text': 'Helen Y. Yang'}, {'@pid': '275/7017', 'text': 'Robbert Struyven'}, {'@pid': '89/10116', 'text': 'Donglai Wei'}, {'@pid': '43/8664', 'text': 'Zhe Sun'}, {'@pid': '275/6821', 'text': 'Kylie R. Lee'}, {'@pid': '275/6724', 'text': 'Charlotte Royston'}, {'@pid': '275/6783', 'text': 'Liz Cam'}, {'@pid': '275/6845', 'text': 'Yael Kalma'}, {'@pid': '275/6876', 'text': 'Foad Azem'}, {'@pid': '275/6807', 'text': 'Dalit Ben-Yosef'}, {'@pid': 'p/HanspeterPfister', 'text': 'Hanspeter Pfister'}, {'@pid': '275/7036', 'text': 'Daniel Needleman'}]","['Deeplearning', 'Humanembryos', 'In-vitrofertilization']","A major challenge in clinical In-Vitro Fertilization (IVF) is selecting the highest quality embryo to transfer to the patient in the hopes of achieving a pregnancy. Time-lapse microscopy provides clinicians with a wealth of information for selecting embryos. However, the resulting movies of embryos are currently analyzed manually, which is time consuming and subjective. Here, we automate feature extraction of time-lapse microscopy of human embryos with a machine-learning pipeline of five convolutional neural networks (CNNs). Our pipeline consists of (1) semantic segmentation of the regions of the embryo, (2) regression predictions of fragment severity, (3) classification of the developmental stage, and object instance segmentation of (4) cells and (5) pronuclei. Our approach greatly speeds up the measurement of quantitative, biologically relevant features that may aid in embryo selection.",2020/9/29,1,10.1007/978-3-030-59722-1_3
Scribble2Label - Scribble-Supervised Cell Segmentation via Self-generating Pseudo-Labels with Consistency.,"[{'@pid': '268/5955', 'text': 'Hyeonsoo Lee'}, {'@pid': '70/5286', 'text': 'Won-Ki Jeong'}]","['Cellsegmentation', 'Weakly-supervisedlearning', 'Scribbleannotation']","Segmentation is a fundamental process in microscopic cell image analysis. With the advent of recent advances in deep learning, more accurate and high-throughput cell segmentation has become feasible. However, most existing deep learning-based cell segmentation algorithms require fully annotated ground-truth cell labels, which are time-consuming and labor-intensive to generate. In this paper, we introduce Scribble2Label, a novel weakly-supervised cell segmentation framework that exploits only a handful of scribble annotations without full segmentation labels. The core idea is to combine pseudo-labeling and label filtering to generate reliable labels from weak supervision. For this, we leverage the consistency of predictions by iteratively averaging the predictions to improve pseudo labels. We demonstrate the performance of Scribble2Label by comparing it to several state-of-the-art cell segmentation methods with various cell image modalities, including bright-field, fluorescence, and electron microscopy. We also show that our method performs robustly across different levels of scribble details, which confirms that only a few scribble annotations are required in real-use cases.",2020/9/29,1,10.1007/978-3-030-59710-8_2
A Novel Loss Calibration Strategy for Object Detection Networks Training on Sparsely Annotated Pathological Datasets.,"[{'@pid': '245/6165', 'text': 'Hansheng Li'}, {'@pid': '97/4989', 'text': 'Xin Han'}, {'@pid': '258/4899', 'text': 'Yuxin Kang'}, {'@pid': '87/10627', 'text': 'Xiaoshuang Shi'}, {'@pid': '239/7409', 'text': 'Mengdi Yan'}, {'@pid': '275/6941', 'text': 'Zixu Tong'}, {'@pid': '51/9638', 'text': 'Qirong Bu'}, {'@pid': '47/5523-4', 'text': 'Lei Cui 0004'}, {'@pid': '00/4883-3', 'text': 'Jun Feng 0003'}, {'@pid': '20/2970-2', 'text': 'Lin Yang 0002'}]",[],"Recently, object detection frameworks based on Convolutional Neural Networks (CNNs) have become powerful methods for various tasks of medical image analysis; however, they often struggle with most pathological datasets, which are impossible to annotate all the cells. Obviously, sparse annotations may lead to a seriously miscalculated loss in training, which limits the performance of networks. To address this limitation, we investigate the internal training process of object detection networks. Our core observation is that there is a significant density difference between the regression boxes of the positive instances and negative instances. Our novel Boxes Density Energy (BDE) focuses on utilizing the densities of regression boxes to conduct loss-calibration, which is dedicated to reducing the miscalculated loss, meanwhile to penalizing mispredictions with a relatively more significant loss. Thus BDE can guide networks to be trained along the right direction. Extensive experiments have demonstrated that, BDE on the sparsely annotated pathological dataset can significantly boost the performance of networks, and even with 1.0¨C1.5% higher recall than networks trained on the fully annotated dataset.",2020/9/29,1,10.1007/978-3-030-59722-1_31
Bounding Maps for Universal Lesion Detection.,"[{'@pid': '07/1429', 'text': 'Han Li'}, {'@pid': '03/6451-1', 'text': 'Hu Han 0001'}, {'@pid': '57/98', 'text': 'S. Kevin Zhou'}]","['UniversalLesionDetection', 'Boundingbox', 'Boundingmap']","(ULD) in computed tomography plays an essential role in computer-aided diagnosis systems. Many detection approaches achieve excellent results for ULD using possible bounding boxes (or anchors) as proposals. However, empirical evidence shows that using anchor-based proposals leads to a high false-positive (FP) rate. In this paper, we propose a box-to-map method to represent a bounding box with three soft continuous maps with bounds in x-, y- and xy-directions. The bounding maps (BMs) are used in two-stage anchor-based ULD frameworks to reduce the FP rate. In the \(1^{st}\) stage of the region proposal network, we replace the sharp binary ground-truth label of anchors with the corresponding xy-direction BM hence the positive anchors are now graded. In the \(2^{nd}\) stage, we add a branch that takes our continuous BMs in x- and y-directions for extra supervision of detailed locations. Our method, when embedded into three state-of-the-art two-stage anchor-based detection methods, brings a free detection accuracy improvement (e.g., a 1.68% to 3.85% boost of sensitivity at 4 FPs) without extra inference time.",2020/9/29,1,10.1007/978-3-030-59719-1_41
Superpixel-Guided Label Softening for Medical Image Segmentation.,"[{'@pid': '83/5560', 'text': 'Hang Li'}, {'@pid': '34/4292-4', 'text': 'Dong Wei 0004'}, {'@pid': '194/4227-1', 'text': 'Shilei Cao 0001'}, {'@pid': '86/7113-2', 'text': 'Kai Ma 0002'}, {'@pid': '63/6244', 'text': 'Liansheng Wang'}, {'@pid': '44/6510', 'text': 'Yefeng Zheng'}]","['Softlabeling', 'Superpixel', 'Medicalimagesegmentation']","Segmentation of objects of interest is one of the central tasks in medical image analysis, which is indispensable for quantitative analysis. When developing machine-learning based methods for automated segmentation, manual annotations are usually used as the ground truth toward which the models learn to mimic. While the bulky parts of the segmentation targets are relatively easy to label, the peripheral areas are often difficult to handle due to ambiguous boundaries and the partial volume effect, etc., and are likely to be labeled with uncertainty. This uncertainty in labeling may, in turn, result in unsatisfactory performance of the trained models. In this paper, we propose superpixel-based label softening to tackle the above issue. Generated by unsupervised over-segmentation, each superpixel is expected to represent a locally homogeneous area. If a superpixel intersects with the annotation boundary, we consider a high probability of uncertain labeling within this area. Driven by this intuition, we soften labels in this area based on signed distances to the annotation boundary and assign probability values within [0, 1] to them, in comparison with the original ¡°hard¡±, binary labels of either 0 or 1. The softened labels are then used to train the segmentation models together with the hard labels. Experimental results on a brain MRI dataset and an optical coherence tomography dataset demonstrate that this conceptually simple and implementation-wise easy method achieves overall superior segmentation performances to baseline and comparison methods for both 3D and 2D medical images.",2020/9/29,1,10.1007/978-3-030-59719-1_23
Difficulty-Aware Meta-learning for Rare Disease Diagnosis.,"[{'@pid': '02/9850', 'text': 'Xiaomeng Li'}, {'@pid': '165/8092', 'text': 'Lequan Yu'}, {'@pid': '183/6320', 'text': 'Yueming Jin'}, {'@pid': '38/803', 'text': 'Chi-Wing Fu'}, {'@pid': '82/2022', 'text': 'Lei Xing'}, {'@pid': '52/2889', 'text': 'Pheng-Ann Heng'}]",[],"Rare diseases have extremely low-data regimes, unlike common diseases with large amount of available labeled data. Hence, to train a neural network to classify rare diseases with a few per-class data samples is very challenging, and so far, catches very little attention. In this paper, we present a difficulty-aware meta-learning method to address rare disease classifications and demonstrate its capability to classify dermoscopy images. Our key approach is to first train and construct a meta-learning model from data of common diseases, then adapt the model to perform rare disease classification. To achieve this, we develop the difficulty-aware meta-learning method that dynamically monitors the importance of learning tasks during the meta-optimization stage. To evaluate our method, we use the recent ISIC 2018 skin lesion classification dataset, and show that with only five samples per class, our model can quickly adapt to classify unseen classes by a high AUC of 83.3%. Also, we evaluated several rare disease classification results in the public Dermofit Image Library to demonstrate the potential of our method for real clinical practice.",2020/9/29,1,10.1007/978-3-030-59710-8_35
Contrastive Rendering for Ultrasound Image Segmentation.,"[{'@pid': '262/3765', 'text': 'Haoming Li 0008'}, {'@pid': '44/1152-9', 'text': 'Xin Yang 0009'}, {'@pid': '262/3676', 'text': 'Jiamin Liang'}, {'@pid': '250/4162', 'text': 'Wenlong Shi'}, {'@pid': '239/7908', 'text': 'Chaoyu Chen'}, {'@pid': '226/3400', 'text': 'Haoran Dou'}, {'@pid': '96/4282', 'text': 'Rui Li'}, {'@pid': '43/2694', 'text': 'Rui Gao'}, {'@pid': '142/5663', 'text': 'Guangquan Zhou'}, {'@pid': '235/1462', 'text': 'Jinghui Fang'}, {'@pid': '127/3047', 'text': 'Xiaowen Liang'}, {'@pid': '167/9731', 'text': 'Ruobing Huang'}, {'@pid': '16/4982', 'text': 'Alejandro Frangi'}, {'@pid': '98/10178', 'text': 'Zhiyi Chen'}, {'@pid': '02/450-1', 'text': 'Dong Ni 0001'}]","['Ultrasoundimage', 'Segmentation', 'Contrastivelearning']","Ultrasound (US) image segmentation embraced its significant improvement in deep learning era. However, the lack of sharp boundaries in US images still remains an inherent challenge for segmentation. Previous methods often resort to global context, multi-scale cues or auxiliary guidance to estimate the boundaries. It is hard for these methods to approach pixel-level learning for fine-grained boundary generating. In this paper, we propose a novel and effective framework to improve boundary estimation in US images. Our work has three highlights. First, we propose to formulate the boundary estimation as a rendering task, which can recognize ambiguous points (pixels/voxels) and calibrate the boundary prediction via enriched feature representation learning. Second, we introduce point-wise contrastive learning to enhance the similarity of points from the same class and contrastively decrease the similarity of points from different classes. Boundary ambiguities are therefore further addressed. Third, both rendering and contrastive learning tasks contribute to consistent improvement while reducing network parameters. As a proof-of-concept, we performed validation experiments on a challenging dataset of 86 ovarian US volumes. Results show that our proposed method outperforms state-of-the-art methods and has the potential to be used in clinical practice.",2020/9/29,1,10.1007/978-3-030-59716-0_54
Reliable Liver Fibrosis Assessment from Ultrasound Using Global Hetero-Image Fusion and View-Specific Parameterization.,"[{'@pid': '75/10470', 'text': 'Bowen Li'}, {'@pid': '28/7692-6', 'text': 'Ke Yan 0006'}, {'@pid': '272/5445', 'text': 'Dar-In Tai'}, {'@pid': '69/10047', 'text': 'Yuankai Huo'}, {'@pid': '78/6574-1', 'text': 'Le Lu 0001'}, {'@pid': '67/4008-6', 'text': 'Jing Xiao 0006'}, {'@pid': '60/11273', 'text': 'Adam P. Harrison'}]","['Viewfusion', 'Ultrasound', 'Liverfibrosis', 'Computer-aideddiagnosis']","Ultrasound (US) is a critical modality for diagnosing liver fibrosis. Unfortunately, assessment is very subjective, motivating automated approaches. We introduce a principled deep convolutional neural network (CNN) workflow that incorporates several innovations. First, to avoid overfitting on non-relevant image features, we force the network to focus on a clinical region of interest (ROI), encompassing the liver parenchyma and upper border. Second, we introduce global hetero-image fusion (GHIF), which allows the CNN to fuse features from any arbitrary number of images in a study, increasing its versatility and flexibility. Finally, we use ¡°style¡±-based view-specific parameterization (VSP) to tailor the CNN processing for different viewpoints of the liver, while keeping the majority of parameters the same across views. Experiments on a dataset of 610 patient studies (6979 images) demonstrate that our pipeline can contribute roughly 7% and 22% improvements in partial area under the curve and recall at 90% precision, respectively, over conventional classifiers, validating our approach to this crucial problem.",2020/9/29,1,10.1007/978-3-030-59716-0_58
Efficient Shapley Explanation for Features Importance Estimation Under Uncertainty.,"[{'@pid': '71/8042', 'text': 'Xiaoxiao Li'}, {'@pid': '40/7018', 'text': 'Yuan Zhou'}, {'@pid': '00/8526', 'text': 'Nicha C. Dvornek'}, {'@pid': '253/2398', 'text': 'Yufeng Gu'}, {'@pid': '172/7890', 'text': 'Pamela Ventola'}, {'@pid': '96/4489', 'text': 'James S. Duncan'}]",[],"Complex deep learning models have shown their impressive power in analyzing high-dimensional medical image data. To increase the trust of applying deep learning models in medical field, it is essential to understand why a particular prediction was reached. Data feature importance estimation is an important approach to understand both the model and the underlying properties of data. Shapley value explanation (SHAP) is a technique to fairly evaluate input feature importance of a given model. However, the existing SHAP-based explanation works have limitations such as 1) computational complexity, which hinders their applications on high-dimensional medical image data; 2) being sensitive to noise, which can lead to serious errors. Therefore, we propose an uncertainty estimation method for the feature importance results calculated by SHAP. Then we theoretically justify the methods under a Shapley value framework. Finally we evaluate our methods on MNIST and a public neuroimaging dataset. We show the potential of our method to discover disease related biomarkers from neuroimaging data.",2020/9/29,1,10.1007/978-3-030-59710-8_77
Pooling Regularized Graph Neural Network for fMRI Biomarker Analysis.,"[{'@pid': '71/8042', 'text': 'Xiaoxiao Li'}, {'@pid': '40/7018', 'text': 'Yuan Zhou'}, {'@pid': '00/8526', 'text': 'Nicha C. Dvornek'}, {'@pid': '157/5518', 'text': 'Muhan Zhang'}, {'@pid': '220/1417', 'text': 'Juntang Zhuang'}, {'@pid': '172/7890', 'text': 'Pamela Ventola'}, {'@pid': '96/4489', 'text': 'James S. Duncan'}]","['fMRIBiomarker', 'Graphneuralnetwork', 'Autism']","Understanding how certain brain regions relate to a specific neurological disorder has been an important area of neuroimaging research. A promising approach to identify the salient regions is using Graph Neural Networks (GNNs), which can be used to analyze graph structured data, e.g. brain networks constructed by functional magnetic resonance imaging (fMRI). We propose an interpretable GNN framework with a novel salient region selection mechanism to determine neurological brain biomarkers associated with disorders. Specifically, we design novel regularized pooling layers that highlight salient regions of interests (ROIs) so that we can infer which ROIs are important to identify a certain disease based on the node pooling scores calculated by the pooling layers. Our proposed framework, Pooling Regularized-GNN (PR-GNN), encourages reasonable ROI-selection and provides flexibility to preserve either individual- or group-level patterns. We apply the PR-GNN framework on a Biopoint Autism Spectral Disorder (ASD) fMRI dataset. We investigate different choices of the hyperparameters and show that PR-GNN outperforms baseline methods in terms of classification accuracy. The salient ROI detection results show high correspondence with the previous neuroimaging-derived biomarkers for ASD.",2020/9/29,1,10.1007/978-3-030-59728-3_61
Shape-Aware Semi-supervised 3D Semantic Segmentation for Medical Images.,"[{'@pid': '270/8082', 'text': 'Shuailin Li'}, {'@pid': '270/8658', 'text': 'Chuyu Zhang'}, {'@pid': '03/4230', 'text': 'Xuming He 0001'}]","['Geometricconstraints', 'Semanticsegmentation', 'Semi-supevisedlearning']","Semi-supervised learning has attracted much attention in medical image segmentation due to challenges in acquiring pixel-wise image annotations, which is a crucial step for building high-performance deep learning methods. Most existing semi-supervised segmentation approaches either tend to neglect geometric constraint in object segments, leading to incomplete object coverage, or impose strong shape prior that requires extra alignment. In this work, we propose a novel shape-aware semi-supervised segmentation strategy to leverage abundant unlabeled data and to enforce a geometric shape constraint on the segmentation output. To achieve this, we develop a multi-task deep network that jointly predicts semantic segmentation and signed distance map (SDM) of object surfaces. During training, we introduce an adversarial loss between the predicted SDMs of labeled and unlabeled data so that our network is able to capture shape-aware features more effectively. Experiments on the Atrial Segmentation Challenge dataset show that our method outperforms current state-of-the-art approaches with improved shape estimation, which validates its efficacy. Code is available at https://github.com/kleinzcy/SASSnet.",2020/9/29,1,10.1007/978-3-030-59710-8_54
Continual Learning of New Diseases with Dual Distillation and Ensemble Strategy.,"[{'@pid': '222/6955', 'text': 'Zhuoyun Li'}, {'@pid': '273/0850-1', 'text': 'Changhong Zhong 0001'}, {'@pid': '48/6024', 'text': 'Ruixuan Wang'}, {'@pid': '30/8399', 'text': 'Wei-Shi Zheng'}]","['Continuallearning', 'Distillation', 'Ensemble']","Most intelligent diagnosis systems are developed for one or a few specific diseases, while medical specialists can diagnose all diseases of certain organ or tissue. Since it is often difficult to collect data of all diseases, it would be desirable if an intelligent system can initially diagnose a few diseases, and then continually learn to diagnose more and more diseases with coming data of these new classes in the future. However, current intelligent systems are characterised by catastrophic forgetting of old knowledge when learning new classes. In this paper, we propose a new continual learning framework to alleviate this issue by simultaneously distilling both old knowledge and recently learned new knowledge and by ensembling the class-specific knowledge from the previous classifier and the learned new classifier. Experiments showed that the proposed method outperforms state-of-the-art methods on multiple medical and natural image datasets.",2020/9/29,1,10.1007/978-3-030-59710-8_17
Detecting Changes of Functional Connectivity by Dynamic Graph Embedding Learning.,"[{'@pid': '42/5120', 'text': 'Yi Lin'}, {'@pid': '32/6389', 'text': 'Jia Hou'}, {'@pid': '64/5632', 'text': 'Paul J. Laurienti'}, {'@pid': '03/5225-1', 'text': 'Guorong Wu 0001'}]","['Brainstatedecoding', 'Graphlearning', 'Functionaldynamics']","Our current understandings reach the unanimous consensus that the brain functions and cognitive states are dynamically changing even in the resting state rather than remaining at a single constant state. Due to the low signal-to-noise ratio and high vertex-time dependency in BOLD (blood oxygen level dependent) signals, however, it is challenging to detect the dynamic behavior in connectivity without requiring prior knowledge of the experimental design. Like the Fourier bases in signal processing, each brain network can be summarized by a set of harmonic bases (Eigensystem) which are derived from its latent Laplacian matrix. In this regard, we propose to establish a subject-specific spectrum domain, where the learned orthogonal harmonic-Fourier bases allow us to detect the changes of functional connectivity more accurately than using the BOLD signals in an arbitrary sliding window. To do so, we first present a novel dynamic graph learning method to simultaneously estimate the intrinsic BOLD signals and learn the joint harmonic-Fourier bases for the underlying functional connectivity network. Then, we project the BOLD signals to the spectrum domain spanned by learned network harmonic and Fourier bases, forming the new system-level fluctuation patterns, called dynamic graph embeddings. We employ the classic clustering approach to identify the changes of functional connectivity using the novel dynamic graph embedding vectors. Our method has been evaluated on working memory task-based fMRI dataset and comparisons with state-of-the-art methods, where our joint harmonic-Fourier bases achieves higher accuracy in detecting multiple cognitive states.",2020/9/29,1,10.1007/978-3-030-59728-3_48
Shape-Aware Meta-learning for Generalizing Prostate MRI Segmentation to Unseen Domains.,"[{'@pid': '226/6174', 'text': 'Quande Liu'}, {'@pid': '165/7846', 'text': 'Qi Dou'}, {'@pid': '52/2889', 'text': 'Pheng-Ann Heng'}]","['Domaingeneralization', 'Meta-learning', 'ProstateMRIsegmentation']","Model generalization capacity at domain shift (e.g., various imaging protocols and scanners) is crucial for deep learning methods in real-world clinical deployment. This paper tackles the challenging problem of domain generalization,?i.e., learning a model from multi-domain source data such that it can directly generalize to an unseen target domain. We present a novel shape-aware meta-learning scheme to improve the model generalization in prostate MRI segmentation. Our learning scheme roots in the gradient-based meta-learning, by explicitly simulating domain shift with virtual meta-train and meta-test during training. Importantly, considering the deficiencies encountered when applying a segmentation model to unseen domains (i.e., incomplete shape and ambiguous boundary of the prediction masks), we further introduce two complementary loss objectives to enhance the meta-optimization, by particularly encouraging the shape compactness and shape smoothness of the segmentations under simulated domain shift. We evaluate our method on prostate MRI data from six different institutions with distribution shifts acquired from public datasets. Experimental results show that our approach outperforms many state-of-the-art generalization methods consistently across all six settings of unseen domains (Code and dataset are available at https://github.com/liuquande/SAML).",2020/9/29,1,10.1007/978-3-030-59713-9_46
Orchestrating Medical Image Compression and Remote Segmentation Networks.,"[{'@pid': '182/3820', 'text': 'Zihao Liu'}, {'@pid': '129/7659', 'text': 'Sicheng Li'}, {'@pid': '275/7009', 'text': 'Yen-kuang Chen'}, {'@pid': '43/656-23', 'text': 'Tao Liu 0023'}, {'@pid': '95/2446', 'text': 'Qi Liu'}, {'@pid': '181/2733-4', 'text': 'Xiaowei Xu 0004'}, {'@pid': '94/5536', 'text': 'Yiyu Shi'}, {'@pid': '70/11466', 'text': 'Wujie Wen'}]",[],"Deep learning-based medical image segmentation on the cloud offers superb performance by harnessing the recent model innovation and hardware advancement. However, one major factor that limits its overall service speed is the long data transmission latency, which could far exceed the segmentation computation time. Existing image compression techniques are unable to achieve an efficient compression to dramatically reduce the data offloading overhead, while maintaining a high segmentation accuracy. The underlying reason is that they are all developed upon human visual system, whose image perception pattern could be fundamentally different from that of deep learning-based image segmentation. Motivated by this observation, in this paper, we propose a generative segmentation architecture consisting of a compression network, a segmentation network and a discriminator network. Our design orchestrates and coordinates segmentation and compression for simultaneous improvements of segmentation accuracy and compression efficiency, through a dedicated GAN architecture with novel loss functions. Experimental results on 2D and 3D medical images demonstrate that our design can reduce the bandwidth requirement by 2 orders-of-magnitude comparing with that of uncompressed images, and increase the accuracy of remote segmentation remarkably over the state-of-the-art solutions, truly accelerating the cloud-based medical imaging service.",2020/9/29,1,10.1007/978-3-030-59719-1_40
Ultrasound Video Summarization Using Deep Reinforcement Learning.,"[{'@pid': '226/4299', 'text': 'Tianrui Liu'}, {'@pid': '126/0731', 'text': 'Qingjie Meng'}, {'@pid': '186/8028', 'text': 'Athanasios Vlontzos'}, {'@pid': '220/5402', 'text': 'Jeremy Tan'}, {'@pid': '69/2478', 'text': 'Daniel Rueckert'}, {'@pid': '76/5562', 'text': 'Bernhard Kainz'}]","['Videosummarization', 'Reinforcementlearning', 'Ultrasounddiagnostic']","Video is an essential imaging modality for diagnostics, e.g. in ultrasound imaging, for endoscopy, or movement assessment. However, video hasn¡¯t received a lot of attention in the medical image analysis community. In the clinical practice, it is challenging to utilise raw diagnostic video data efficiently as video data takes a long time to process, annotate or audit.  In this paper we introduce a novel, fully automatic video summarization method that is tailored to the needs of medical video data. Our approach is framed as reinforcement learning problem and produces agents focusing on the preservation of important diagnostic information. We evaluate our method on videos from fetal ultrasound screening, where commonly only a small amount of the recorded data is used diagnostically. We show that our method is superior to alternative video summarization methods and that it preserves essential information required by clinical diagnostic standards.",2020/9/29,1,10.1007/978-3-030-59716-0_46
Joint Neuroimage Synthesis and Representation Learning for Conversion Prediction of Subjective Cognitive Decline.,"[{'@pid': '191/5089', 'text': 'Yunbi Liu'}, {'@pid': '67/3966', 'text': 'Yongsheng Pan'}, {'@pid': '03/1094', 'text': 'Wei Yang'}, {'@pid': '240/8090', 'text': 'Zhenyuan Ning'}, {'@pid': '251/0963', 'text': 'Ling Yue'}, {'@pid': '07/8657', 'text': 'Mingxia Liu'}, {'@pid': '14/4383', 'text': 'Dinggang Shen'}]",[],"Predicting the progression of preclinical Alzheimer¡¯s disease (AD) such as subjective cognitive decline (SCD) is fundamental for the effective intervention of pathological cognitive decline. Even though multimodal neuroimaging has been widely used in automated AD diagnosis, there are few studies dedicated to SCD progression prediction, due to challenges of incomplete and limited data. To this end, we propose a Joint neuroimage Synthesis and Representation Learning (JSRL) framework with transfer learning for SCD conversion prediction using incomplete multimodal neuroimaging data. Specifically, JSRL consists of two major components: 1) a generative adversarial network for synthesizing missing neuroimaging data, and 2) a classification network for learning neuroimage representations and predicting the progression of SCD. These two subnetworks share the same feature encoding module, encouraging that the to-be-generated representations are prediction-oriented and also the underlying association among multimodal images can be effectively modeled for accurate prediction. To handle the limited data problem, we further leverage both image synthesis and prediction models learned from a large-scale ADNI database (with MRI and PET acquired from 863 subjects) to a small-scale SCD database (with only MRI acquired from 113 subjects) in a transfer learning manner. Experimental results show that the proposed JSRL can synthesize reasonable PET scans and is superior to several state-of-the-art methods in SCD conversion prediction.",2020/9/29,1,10.1007/978-3-030-59728-3_57
KISEG - A Three-Stage Segmentation Framework for Multi-level Acceleration of Chest CT Scans from COVID-19 Patients.,"[{'@pid': '95/2454', 'text': 'Xiaohong Liu'}, {'@pid': '78/2022', 'text': 'Kai Wang'}, {'@pid': '181/2613', 'text': 'Ke Wang'}, {'@pid': '19/1766-6', 'text': 'Ting Chen 0006'}, {'@pid': '29/177', 'text': 'Kang Zhang'}, {'@pid': '53/35', 'text': 'Guangyu Wang'}]","['ChestCT', 'COVID-19dataset', 'Semanticsegmentation', 'Multi-levelacceleration']","During the ongoing COVID-19 outbreak, it is critical to perform an accurate diagnosis of COVID-19 pneumonia by computed tomography (CT). Although chest lesion segmentation plays a pivotal role in computer-aided diagnosis (CAD), accuracy is hindered by the lack of a publicly available CT dataset with manual annotation. In addition, for clinical deployment, how to balance the accuracy versus efficiency for the semantic segmentation model remains challenging. To address these issues, we construct the first CT dataset of COVID-19 pneumonia with pixel-wise lesion annotations. We propose a three-stage framework, called KISEG (Key and Intermediate frame of Segmentation), to enhance performance on serial CT image segmentation with multi-level acceleration. We first take a policy to divide frames of serial CT into two groups, key frames and intermediate frames. Then KISEG employs a main model (accurate but cumbersome) for key frame segmentation. And third, an auxiliary model was employed for intermediate frame segmentation with incorporating the information of key frames during the fusion module. Moreover, we propose a Gaussian Kernel Dropout for data augmentation. Experiments on our dataset demonstrate that our proposed KISEG achieves comparable accuracy with state-of-the-art methods and fewer GFLOPs, speeding up from 2.88\(\times \) to 9.16\(\times \). This dataset has been made public for further research of COVID-19 for AI community, released on http://ncov-ai.big.ac.cn/download.",2020/9/29,1,10.1007/978-3-030-59719-1_3
White Matter Tract Segmentation with Self-supervised Learning.,"[{'@pid': '41/4012', 'text': 'Qi Lu'}, {'@pid': '186/4871', 'text': 'Yuxing Li'}, {'@pid': '27/10751', 'text': 'Chuyang Ye'}]","['Whitemattertractsegmentation', 'Self-supervisedlearning', 'Deepnetwork']","White matter tract segmentation based on diffusion magnetic resonance imaging?(dMRI) plays an important role in brain analysis. Deep learning based methods of white matter tract segmentation have been proposed to improve the segmentation accuracy. However, manual delineations of white matter tracts for network training are especially difficult to obtain. Therefore, in this paper, we explore how to improve the performance of deep learning based white matter tract segmentation when the number of manual tract delineations is limited. Specifically, we propose to exploit the abundant unannotated data using a self-supervised learning approach, where knowledge about image context can be learned in a well designed pretext task that does not require manual annotations. The knowledge can then be transferred to the white matter tract segmentation task, so that when manual tract delineations for training are scarce, the performance of the network can be improved. To allow the image context knowledge to be relevant to white matter tracts, the pretext task in this work is designed to predict the density map of fiber streamlines, where training data can be obtained using tractography without manual efforts. The model pretrained for the pretext task is then fine-tuned by the small number of tract annotations for the target segmentation task. In addition, we explore the possibility of combining self-supervised learning with a complementary pseudo-labeling strategy of using unannotated data. We validated the proposed approach using dMRI scans from the Human Connectome Project dataset, where the benefit of the proposed method is shown when tract annotations are scarce.",2020/9/29,1,10.1007/978-3-030-59728-3_27
Learning to Segment Anatomical Structures Accurately from One Exemplar.,"[{'@pid': '190/7827', 'text': 'Yuhang Lu'}, {'@pid': '128/5278', 'text': 'Weijian Li'}, {'@pid': '150/4204', 'text': 'Kang Zheng'}, {'@pid': '177/2310-2', 'text': 'Yirui Wang 0002'}, {'@pid': '60/11273', 'text': 'Adam P. Harrison'}, {'@pid': '263/2349', 'text': 'Chihung Lin'}, {'@pid': '62/3151-2', 'text': 'Song Wang 0002'}, {'@pid': '67/4008-6', 'text': 'Jing Xiao 0006'}, {'@pid': '78/6574-1', 'text': 'Le Lu 0001'}, {'@pid': '263/2367', 'text': 'Chang-Fu Kuo'}, {'@pid': '04/8761', 'text': 'Shun Miao'}]","['ContourTransformerNetwork', 'One-shotsegmentation', 'Graphconvolutionalnetwork']","Accurate segmentation of critical anatomical structures is at the core of medical image analysis. The main bottleneck lies in gathering the requisite expert-labeled image annotations in a scalable manner. Methods that permit to produce accurate anatomical structure segmentation without using a large amount of fully annotated training images are highly desirable. In this work, we propose a novel contribution of Contour Transformer Network (CTN), a one-shot anatomy segmentor including a naturally built-in human-in-the-loop mechanism. Segmentation is formulated by learning a contour evolution behavior process based on graph convolutional networks (GCN). Training of our CTN model requires only one labeled image exemplar and leverages additional unlabeled data through newly introduced loss functions that measure the global shape and appearance consistency of contours. We demonstrate that our one-shot learning method significantly outperforms non-learning-based methods and performs competitively to the state-of-the-art fully supervised deep learning approaches. With minimal human-in-the-loop editing feedback, the segmentation performance can be further improved and tailored towards the observer desired outcomes. This can facilitate the clinician designed imaging-based biomarker assessments (to support personalized quantitative clinical diagnosis) and outperforms fully supervised baselines.",2020/9/29,1,10.1007/978-3-030-59710-8_66
MvMM-RegNet - A New Image Registration Framework Based on Multivariate Mixture Model and Neural Network Estimation.,"[{'@pid': '236/6037', 'text': 'Xinzhe Luo'}, {'@pid': '69/1049', 'text': 'Xiahai Zhuang'}]",[],"Current deep-learning-based registration algorithms often exploit intensity-based similarity measures as the loss function, where dense correspondence between a pair of moving and fixed images is optimized through backpropagation during training. However, intensity-based metrics can be misleading when the assumption of intensity class correspondence is violated, especially in cross-modality or contrast-enhanced images. Moreover, existing learning-based registration methods are predominantly applicable to pairwise registration and are rarely extended to groupwise registration or simultaneous registration with multiple images. In this paper, we propose a new image registration framework based on multivariate mixture model (MvMM) and neural network estimation. A generative model consolidating both appearance and anatomical information is established to derive a novel loss function capable of implementing groupwise registration. We highlight the versatility of the proposed framework for various applications on multimodal cardiac images, including single-atlas-based segmentation (SAS) via pairwise registration and multi-atlas segmentation (MAS) unified by groupwise registration. We evaluated performance on two publicly available datasets, i.e. MM-WHS-2017 and MS-CMRSeg-2019. The results show that the proposed framework achieved an average Dice score of \(0.871\pm 0.025\) for whole-heart segmentation on MR images and \(0.783\pm 0.082\) for myocardium segmentation on LGE MR images (Code is available from https://zmiclab.github.io/projects.html).",2020/9/29,1,10.1007/978-3-030-59716-0_15
Cycle Structure and Illumination Constrained GAN for Medical Image Enhancement.,"[{'@pid': '130/8327', 'text': 'Yuhui Ma'}, {'@pid': '03/357', 'text': 'Yonghuai Liu'}, {'@pid': '78/5816', 'text': 'Jun Cheng'}, {'@pid': '15/952', 'text': 'Yalin Zheng'}, {'@pid': '152/6299', 'text': 'Morteza Ghahremani'}, {'@pid': '156/6659', 'text': 'Honghan Chen'}, {'@pid': '23/108-1', 'text': 'Jiang Liu 0001'}, {'@pid': '17/9876', 'text': 'Yitian Zhao'}]","['Illuminationregularization', 'Structuralloss', 'CycleGAN', 'Medicalimageenhancement']","The non-uniform illumination or imbalanced intensity in medical images brings challenges for automated screening, examination and diagnosis of diseases. Previously, CycleGAN was proposed to transform input images into enhanced ones without paired images. However, it did not consider many local details of the structures, which are essential for medical images. In this paper, we propose a Cycle Structure and Illumination constrained GAN (CSI-GAN), for medical image enhancement. Inspired by CycleGAN based on the global constraints of the adversarial loss and cycle consistency, the proposed CSI-GAN treats low and high quality images as those in two domains and computes local structure and illumination constraints for learning both overall characteristics and local details. To evaluate the effectiveness of CSI-GAN, we have conducted experiments over two medical image datasets: corneal confocal microscopy (CCM) and endoscopic images. The experimental results show that our method yields better performance than both conventional methods and other deep learning based methods. As a complementary output, we will release the CCM dataset to the public in the future.",2020/9/29,1,10.1007/978-3-030-59713-9_64
Attention-Guided Deep Graph Neural Network for Longitudinal Alzheimer&apos;s Disease Analysis.,"[{'@pid': '226/3693', 'text': 'Junbo Ma'}, {'@pid': '60/4671-1', 'text': 'Xiaofeng Zhu 0001'}, {'@pid': '168/0440', 'text': 'Defu Yang'}, {'@pid': '47/8066', 'text': 'Jiazhou Chen'}, {'@pid': '03/5225-1', 'text': 'Guorong Wu 0001'}]","['Graphneuralnetwork', 'Attentionmechanism', 'Randomwalk', 'Alzheimer¡¯sdisease', 'Longitudinalanalysis']","Alzheimer¡¯s disease (AD) is the main reason for dementia among aged people. Since AD is less likely reversible and has no cure yet, monitoring its progress is essential for adjusting the therapy plan of the patients to delay its deterioration. The computer-aided longitudinal AD data analysis is helpful to this kind of task, which can be used to evaluate the disease status, identify discriminative brain regions, and reveal the progression of the disease. However, most of the existing methods exist two main issues: i) the graph features are extracted globally from the entire graph, which is very sensitive to the noises; ii) they have difficulties in processing dynamic graphs, whereas the brain networks are highly variable, as they vary from individuals or changes along time or by disease. To address these issues, a novel Attention-Guided Deep Graph Neural (AGDGN) network is proposed in this paper, which utilizes an Attention-Guided Random Walk (AGRW) module to extract the structural graph features from the brain network. Since AGRW only needs the local information around the neighborhood nodes at each step of random walk, it is robust to the graph noise and flexible in dealing with dynamic graphs. Moreover, the global attention mechanism is integrated into the sequence processing module. The two attention mechanisms are jointly trained to reveal the most informative brain regions from both structural and temporal domain for AD analysis. Experimental results and analysis on the Alzheimer¡¯s Disease Neuroimaging Initiative (ADNI) dataset demonstrate the effectiveness and efficiency of the proposed method.",2020/9/29,1,10.1007/978-3-030-59728-3_38
Widening the Focus - Biomedical Image Segmentation Challenges and the Underestimated Role of Patch Sampling and Inference Strategies.,"[{'@pid': '225/4712', 'text': 'Frederic Madesta'}, {'@pid': '225/4511', 'text': 'R¨¹diger Schmitz'}, {'@pid': '17/2895', 'text': 'Thomas R?sch'}, {'@pid': '46/898', 'text': 'Ren¨¦ Werner'}]","['Inference', 'Patchsampling', 'Segmentation', 'Biomedicalimageanalysischallenges']","Image analysis challenges have considerably influenced the recent years in natural and biomedical computer vision. With several important architectures and training strategies having emerged from image analysis challenges, they are often interpreted as contests in model design and training, and much effort is put into optimization of these aspects.",2020/9/29,1,10.1007/978-3-030-59719-1_29
Inertial Measurements for Motion Compensation in Weight-Bearing Cone-Beam CT of the Knee.,"[{'@pid': '201/6654', 'text': 'Jennifer Maier'}, {'@pid': '270/0642', 'text': 'Marlies Nitschke'}, {'@pid': '146/6634', 'text': 'Jang Hwan Choi'}, {'@pid': '197/5042', 'text': 'Garry Gold'}, {'@pid': '79/3714', 'text': 'Rebecca Fahrig'}, {'@pid': '69/5675', 'text': 'Bj?rn M. Eskofier'}, {'@pid': '131/7133', 'text': 'Andreas K. Maier'}]","['Motioncompensation', 'Inertialmeasurements', 'CTreconstruction']","Involuntary motion during weight-bearing cone-beam computed tomography (CT) scans of the knee causes artifacts in the reconstructed volumes making them unusable for clinical diagnosis. Currently, image-based or marker-based methods are applied to correct for this motion, but often require long execution or preparation times. We propose to attach an inertial measurement unit (IMU) containing an accelerometer and a gyroscope to the leg of the subject in order to measure the motion during the scan and correct for it. To validate this approach, we present a simulation study using real motion measured with an optical 3D tracking system. With this motion, an XCAT numerical knee phantom is non-rigidly deformed during a simulated CT scan creating motion corrupted projections. A biomechanical model is animated with the same tracked motion in order to generate measurements of an IMU placed below the knee. In our proposed multi-stage algorithm, these signals are transformed to the global coordinate system of the CT scan and applied for motion compensation during reconstruction. Our proposed approach can effectively reduce motion artifacts in the reconstructed volumes. Compared to the motion corrupted case, the average structural similarity index and root mean squared error with respect to the no-motion case improved by 13¨C21% and 68¨C70%, respectively. These results are qualitatively and quantitatively on par with a state-of-the-art marker-based method we compared our approach to. The presented study shows the feasibility of this novel approach, and yields promising results towards a purely IMU-based motion compensation in C-arm CT.",2020/9/29,1,10.1007/978-3-030-59716-0_2
"Automatic Segmentation, Localization, and Identification of Vertebrae in 3D CT Images Using Cascaded Convolutional Neural Networks.","[{'@pid': '275/3498', 'text': 'Naoto Masuzawa'}, {'@pid': '117/0476', 'text': 'Yoshiro Kitamura'}, {'@pid': '31/8013', 'text': 'Keigo Nakamura'}, {'@pid': '12/9663', 'text': 'Satoshi Iizuka'}, {'@pid': '117/4813', 'text': 'Edgar Simo-Serra'}]","['Vertebrae', 'Segmentation', 'Localization', 'Identification', 'Convolutionalneuralnetworks']","This paper presents a method for automatic segmentation, localization, and identification of vertebrae in arbitrary 3D CT images. Many previous works do not perform the three tasks simultaneously even though requiring a priori knowledge of which part of the anatomy is visible in the 3D CT images. Our method tackles all these tasks in a single multi-stage framework without any assumptions. In the first stage, we train a 3D Fully Convolutional Networks to find the bounding boxes of the cervical, thoracic, and lumbar vertebrae. In the second stage, we train an iterative 3D Fully Convolutional Networks to segment individual vertebrae in the bounding box. The input to the second networks have an auxiliary channel in addition to the 3D CT images. Given the segmented vertebra regions in the auxiliary channel, the networks output the next vertebra. The proposed method is evaluated in terms of segmentation, localization, and identification accuracy with two public datasets of 15 3D CT images from the MICCAI CSI 2014 workshop challenge and 302 3D CT images with various pathologies introduced in 
[1]. Our method achieved a mean Dice score of 96%, a mean localization error of 8.3?mm, and a mean identification rate of 84%. In summary, our method achieved better performance than all existing works in all the three metrics.",2020/9/29,1,10.1007/978-3-030-59725-2_66
Learning Guided Electron Microscopy with Active Acquisition.,"[{'@pid': '185/3258', 'text': 'Lu Mi'}, {'@pid': '181/2812', 'text': 'Hao Wang'}, {'@pid': '128/1028', 'text': 'Yaron Meirovitch'}, {'@pid': '71/8653', 'text': 'Richard Schalek'}, {'@pid': '91/747', 'text': 'Srinivas C. Turaga'}, {'@pid': '70/6445', 'text': 'Jeff W. Lichtman'}, {'@pid': '275/6920', 'text': 'Aravinthan D. T. Samuel'}, {'@pid': 's/NirShavit', 'text': 'Nir Shavit'}]","['Electronmicroscope', 'Activeacquisition', 'Determinantalpointprocess']","Single-beam scanning electron microscopes (SEM) are widely used to acquire massive datasets for biomedical study, material analysis, and fabrication inspection. Datasets are typically acquired with uniform acquisition: applying the electron beam with the same power and duration to all image pixels, even if there is great variety in the pixels¡¯ importance for eventual use. Many SEMs are now able to move the beam to any pixel in the field of view without delay, enabling them, in principle, to invest their time budget more effectively with non-uniform imaging.",2020/9/29,1,10.1007/978-3-030-59722-1_8
Time Matters - Handling Spatio-Temporal Perfusion Information for Automated TICI Scoring.,"[{'@pid': '235/2639', 'text': 'Maximilian Nielsen'}, {'@pid': '235/2499', 'text': 'Moritz Waldmann'}, {'@pid': '225/4532', 'text': 'Thilo Sentker'}, {'@pid': '235/2617', 'text': 'Andreas Fr?lich'}, {'@pid': '87/3973', 'text': 'Jens Fiehler'}, {'@pid': '46/898', 'text': 'Ren¨¦ Werner'}]","['Spatio-temporalimaging', 'Digitalsubtractionangiography(DSA)', 'Gatedrecurrentunit(GRU)networks', 'Ischemicstroke', 'TICI']","X-ray digital subtraction angiography (DSA) imaging is the backbone of diagnosis and therapy response assessment in cerebral ischemic stroke. To evaluate and document the success of endovascular interventions, the spatio-temporal DSA image information and perfusion dynamics are visually assessed by a clinical expert and reperfusion rated using the so-called TICI (treatment in cerebral ischemia) score. Although clinical standard, it is well known that TICI scoring is time-consuming, observer-dependent and not practicable especially in larger clinical studies. Automated TICI scoring has, however, been considered beyond the scope of machine learning capabilities, due to the complexity of the classification task (eg. heterogeneity of clinical DSA data and a complex dependence between TICI score and perfusion dynamics). The present work describes the first study that tackles automated TICI scoring using deep spatio-temporal learning. It thereby defines the first corresponding benchmark. Methodically, we build on gated recurrent unit networks (GRUs) and integrate knowledge about the perfusion and TICI scoring process into loss functions and network training to increase prediction robustness. Differences between GRU-predicted mTICI scores and routine mTICI scores are in the order of literature-reported interrater variability of human expert-based TICI scoring.",2020/9/29,1,10.1007/978-3-030-59725-2_9
Combining Fundus Images and Fluorescein Angiography for Artery/Vein Classification Using the Hierarchical Vessel Graph Network.,"[{'@pid': '187/5841', 'text': 'Kyoung Jin Noh'}, {'@pid': '35/10693', 'text': 'Sang Jun Park'}, {'@pid': '38/4038', 'text': 'Soochahn Lee'}]","['Fundusimages', 'Fluoresceinangiography', 'Artery/veinclassification', 'Convolutionalneuralnetwork', 'Graphneuralnetwork']","We present a new framework for retinal artery/vein classification from fundus images and corresponding fluorescein angiography (FA) images. While FA seem to provide the most relevant information, it is often insufficient depending on the acquisition conditions. As fundus images are often acquired by default, we combine the fundus image and FA within a parallel convolutional neural network to extract the maximum information in the generated features. Furthermore, we use these features as the input to a hierarchical graph neural network to ensure that the connectivity of vessels plays a part in the classification. We provide investigative evidence through ablative and comparative quantitative evaluations to better determine the optimal configuration in combining the fundus image and FA in a deep learning framework and demonstrate the enhancement in performance compared to previous methods.",2020/9/29,1,10.1007/978-3-030-59722-1_57
DistNet - Deep Tracking by Displacement Regression - Application to Bacteria Growing in the Mother Machine.,"[{'@pid': '132/3454', 'text': 'Jean Ollion'}, {'@pid': '28/9882', 'text': 'Charles Ollion'}]","['Multi-objecttracking', 'Deepneuralnetworks', 'Self-attention']","The mother machine is a popular microfluidic device that allows long-term time-lapse imaging of thousands of cells in parallel by microscopy. It has become a valuable tool for single-cell level quantitative analysis and characterization of many cellular processes such as gene expression and regulation, mutagenesis or response to antibiotics. The automated and quantitative analysis of the massive amount of data generated by such experiments is now the limiting step. In particular the segmentation and tracking of bacteria cells imaged in phase-contrast microscopy¡ªwith error rates compatible with high-throughput data¡ªis a challenging problem.",2020/9/29,1,10.1007/978-3-030-59722-1_21
JBFnet - Low Dose CT Denoising by Trainable Joint Bilateral Filtering.,"[{'@pid': '251/5018', 'text': 'Mayank Patwari'}, {'@pid': '251/4932', 'text': 'Ralf Gutjahr'}, {'@pid': '63/3091', 'text': 'Rainer Raupach'}, {'@pid': '131/7133', 'text': 'Andreas Maier 0001'}]","['LowdoseCTdenoising', 'Jointbilateralfiltering', 'Precisionlearning', 'Convolutionalneuralnetworks']","Deep neural networks have shown great success in low dose CT denoising. However, most of these deep neural networks have several hundred thousand trainable parameters. This, combined with the inherent non-linearity of the neural network, makes the deep neural network difficult to understand with low accountability. In this study we introduce JBFnet, a neural network for low dose CT denoising. The architecture of JBFnet implements iterative bilateral filtering. The filter functions of the Joint Bilateral Filter (JBF) are learned via shallow convolutional networks. The guidance image is estimated by a deep neural network. JBFnet is split into four filtering blocks, each of which performs Joint Bilateral Filtering. Each JBF block consists of 112 trainable parameters, making the noise removal process comprehendable. The Noise Map (NM) is added after filtering to preserve high level features. We train JBFnet with the data from the body scans of 10 patients, and test it on the AAPM low dose CT Grand Challenge dataset. We compare JBFnet with state-of-the-art deep learning networks. JBFnet outperforms CPCE3D, GAN and deep GFnet on the test dataset in terms of noise removal while preserving structures. We conduct several ablation studies to test the performance of our network architecture and training method. Our current setup achieves the best performance, while still maintaining behavioural accountability.",2020/9/29,1,10.1007/978-3-030-59713-9_49
Rethinking Anticipation Tasks - Uncertainty-Aware Anticipation of Sparse Surgical Instrument Usage for Context-Aware Assistance.,"[{'@pid': '230/3878', 'text': 'Dominik Rivoir'}, {'@pid': '32/9855', 'text': 'Sebastian Bodenstedt'}, {'@pid': '217/5020', 'text': 'Isabel Funke'}, {'@pid': '245/9066', 'text': 'Felix von Bechtolsheim'}, {'@pid': '250/5275', 'text': 'Marius Distler'}, {'@pid': '217/5026', 'text': 'J¨¹rgen Weitz'}, {'@pid': '39/6736', 'text': 'Stefanie Speidel'}]","['Anticipation', 'Uncertainty', 'BayesianDeepLearning', 'Surgicalinstruments', 'Surgicaltools', 'Surgicalworkflowanalysis']","Intra-operative anticipation of instrument usage is a necessary component for context-aware assistance in surgery, e.g. for instrument preparation or semi-automation of robotic tasks. However, the sparsity of instrument occurrences in long videos poses a challenge. Current approaches are limited as they assume knowledge on the timing of future actions or require dense temporal segmentations during training and inference. We propose a novel learning task for anticipation of instrument usage in laparoscopic videos that overcomes these limitations. During training, only sparse instrument annotations are required and inference is done solely on image data. We train a probabilistic model to address the uncertainty associated with future events. Our approach outperforms several baselines and is competitive to a variant using richer annotations. We demonstrate the model¡¯s ability to quantify task-relevant uncertainties. To the best of our knowledge, we are the first to propose a method for anticipating instruments in surgery.",2020/9/29,1,10.1007/978-3-030-59716-0_72
Automated Detection of Cortical Lesions in Multiple Sclerosis Patients with 7T MRI.,"[{'@pid': '42/6863', 'text': 'Francesco La Rosa'}, {'@pid': '272/9122', 'text': 'Erin S. Beck'}, {'@pid': '55/10749', 'text': 'Ahmed Abdulkadir'}, {'@pid': 't/JeanPhilippeThiran', 'text': 'Jean-Philippe Thiran'}, {'@pid': '00/7427', 'text': 'Daniel S. Reich'}, {'@pid': '59/10745', 'text': 'Pascal Sati'}, {'@pid': '53/6044', 'text': 'Meritxell Bach Cuadra'}]","['MRI', 'Ultra-highfield', 'Multiplesclerosis', 'Corticallesions', 'Segmentation', 'CNN']","The automated detection of cortical lesions (CLs) in patients with multiple sclerosis (MS) is a challenging task that, despite its clinical relevance, has received very little attention. Accurate detection of the small and scarce lesions requires specialized sequences and high or ultra-high field MRI. For supervised training based on multimodal structural MRI at 7T, two experts generated ground truth segmentation masks of 60 patients with 2014 CLs. We implemented a simplified 3D U-Net with three resolution levels (3D U-Net-). By increasing the complexity of the task (adding brain tissue segmentation), while randomly dropping input channels during training, we improved the performance compared to the baseline. Considering a minimum lesion size of 0.75 \(\upmu \)L, we achieved a lesion-wise cortical lesion detection rate of 67% and a false positive rate of 42%. However, 393 (24%) of the lesions reported as false positives were post-hoc confirmed as potential or definite lesions by an expert. This indicates the potential of the proposed method to support experts in the tedious process of CL manual segmentation.",2020/9/29,1,10.1007/978-3-030-59719-1_57
Ultra2Speech - A Deep Learning Framework for Formant Frequency Estimation and Tracking from Ultrasound Tongue Images.,"[{'@pid': '167/6720', 'text': 'Pramit Saha'}, {'@pid': '75/5732', 'text': 'Yadong Liu'}, {'@pid': '74/9229', 'text': 'Bryan Gick'}, {'@pid': 'f/SidneyFels', 'text': 'Sidney S. Fels'}]","['Silentspeechinterface', 'Ultrasoundtonguecontour', 'Formant', 'Spatio-temporalfeature', 'Deepneuralnetwork', 'Articulatory-to-acoustics']","Thousands of individuals need surgical removal of their larynx due to critical diseases every year and therefore, require an alternative form of communication to articulate speech sounds after the loss of their voice box. This work addresses the articulatory-to-acoustic mapping problem based on ultrasound (US) tongue images for the development of a silent-speech interface (SSI) that can provide them with an assistance in their daily interactions. Our approach targets automatically extracting tongue movement information by selecting an optimal feature set from US images and mapping these features to the acoustic space. We use a novel deep learning architecture to map US tongue images from the US probe placed beneath a subject¡¯s chin to formants that we call, Ultrasound2Formant (U2F) Net. It uses hybrid spatio-temporal 3D convolutions followed by feature shuffling, for the estimation and tracking of vowel formants from US images. The formant values are then utilized to synthesize continuous time-varying vowel trajectories, via Klatt Synthesizer. Our best model achieves R-squared (\(R^2\)) measure of 99.96% for the regression task. Our network lays the foundation for an SSI as it successfully tracks the tongue contour automatically as an internal representation without any explicit annotation.",2020/9/29,1,10.1007/978-3-030-59716-0_45
Anatomical Data Augmentation via Fluid-Based Image Registration.,"[{'@pid': '163/9959', 'text': 'Zhengyang Shen'}, {'@pid': '66/5350', 'text': 'Zhenlin Xu'}, {'@pid': '218/5667', 'text': 'Sahin Olut'}, {'@pid': '88/3304', 'text': 'Marc Niethammer'}]",[],"We introduce a fluid-based image augmentation method for medical image analysis. In contrast to existing methods, our framework generates anatomically meaningful images via interpolation from the geodesic subspace underlying given samples. Our approach consists of three steps: 1) given a source image and a set of target images, we construct a geodesic subspace using the Large Deformation Diffeomorphic Metric Mapping (LDDMM) model; 2) we sample transformations from the resulting geodesic subspace; 3) we obtain deformed images and segmentations via interpolation. Experiments on brain (LPBA) and knee (OAI) data illustrate the performance of our approach on two tasks: 1) data augmentation during training and testing for image segmentation; 2) one-shot learning for single atlas image segmentation. We demonstrate that our approach generates anatomically meaningful data and improves performance on these tasks over competing approaches. Code is available at https://github.com/uncbiag/easyreg.",2020/9/29,1,10.1007/978-3-030-59716-0_31
Deep Small Bowel Segmentation with Cylindrical Topological Constraints.,"[{'@pid': '175/2005', 'text': 'Seung Yeon Shin'}, {'@pid': '61/5584', 'text': 'Sungwon Lee'}, {'@pid': '230/4553', 'text': 'Daniel C. Elton'}, {'@pid': '181/0167', 'text': 'James L. Gulley'}, {'@pid': '25/2710', 'text': 'Ronald M. Summers'}]","['Smallbowelsegmentation', 'Topologicalconstraint', 'Persistenthomology', 'Innercylinder', 'Abdominalcomputedtomography']","We present a novel method for small bowel segmentation where a cylindrical topological constraint based on persistent homology is applied. To address the touching issue which could break the applied constraint, we propose to augment a network with an additional branch to predict an inner cylinder of the small bowel. Since the inner cylinder is free of the touching issue, a cylindrical shape constraint applied on this augmented branch guides the network to generate a topologically correct segmentation. For strict evaluation, we achieved an abdominal computed tomography dataset with dense segmentation ground-truths. The proposed method showed clear improvements in terms of four different metrics compared to the baseline method, and also showed the statistical significance from a paired t-test.",2020/9/29,1,10.1007/978-3-030-59719-1_21
Learning 3D Features with 2D CNNs via Surface Projection for CT Volume Segmentation.,"[{'@pid': '160/6064', 'text': 'Youyi Song'}, {'@pid': '24/2541', 'text': 'Zhen Yu'}, {'@pid': '186/1991', 'text': 'Teng Zhou'}, {'@pid': '229/1035', 'text': 'Jeremy Yuen-Chun Teoh'}, {'@pid': '49/9638', 'text': 'Baiying Lei'}, {'@pid': '76/4931', 'text': 'Kup-Sze Choi'}, {'@pid': '00/1015-1', 'text': 'Jing Qin 0001'}]","['Learning3Dfeaturesby2DCNNs', 'Surfaceprojection', 'CTimagesegmentation.']","3D features are desired in nature for segmenting CT volumes. It is, however, computationally expensive to employ a 3D convolutional neural network (CNN) to learn 3D features. Existing methods hence learn 3D features by still relying on 2D CNNs while attempting to consider more 2D slices, but up until now it is difficulty for them to consider the whole volumetric data, resulting in information loss and performance degradation. In this paper, we propose a simple and effective technique that allows a 2D CNN to learn 3D features for segmenting CT volumes. Our key insight is that all boundary voxels of a 3D object form a surface that can be represented by using a 2D matrix, and therefore they can be perfectly recognized by a 2D CNN in theory. We hence learn 3D features for recognizing these boundary voxels by learning the projection distance between a set of prescribed spherical surfaces and the object¡¯s surface, which can be readily performed by a 2D CNN. By doing so, we can consider the whole volumetric data when spherical surfaces are sampled sufficiently dense, without any information loss. We assessed the proposed method on a publicly available dataset. The experimental evidence shows that the proposed method is effective, outperforming existing methods.
",2020/9/29,1,10.1007/978-3-030-59719-1_18
Multiatlas Calibration of Biophysical Brain Tumor Growth Models with Mass Effect.,"[{'@pid': '228/7963', 'text': 'Shashank Subramanian'}, {'@pid': '188/9218', 'text': 'Klaudius Scheufele'}, {'@pid': '228/7801', 'text': 'Naveen Himthani'}, {'@pid': '67/5816', 'text': 'George Biros'}]","['Glioblastoma', 'Masseffect', 'Tumorgrowthmodels', 'Inverseproblems']","We present a 3D fully-automatic method for the calibration of partial differential equation (PDE) models of glioblastoma (GBM) growth with ¡°mass effect¡±, the deformation of brain tissue due to the tumor. We quantify the mass effect, tumor proliferation, tumor migration, and the localized tumor initial condition from a single multiparameteric Magnetic Resonance Imaging (mpMRI) patient scan. The PDE is a reaction-advection-diffusion partial differential equation coupled with linear elasticity equations to capture mass effect. The single-scan calibration model is notoriously difficult because the precancerous (healthy) brain anatomy is unknown. To solve this inherently ill-posed and ill-conditioned optimization problem, we introduce a novel inversion scheme that uses multiple brain atlases as proxies for the healthy precancer patient brain resulting in robust and reliable parameter estimation. We apply our method on both synthetic and clinical datasets representative of the heterogeneous spatial landscape typically observed in glioblastomas to demonstrate the validity and performance of our methods. In the synthetic data, we report calibration errors (due to the ill-posedness and our solution scheme) in the 10%¨C20% range. In the clinical data, we report good quantitative agreement with the observed tumor and qualitative agreement with the mass effect (for which we do not have a ground truth). Our method uses a minimal set of parameters and provides both global and local quantitative measures of tumor infiltration and mass effect.",2020/9/29,1,10.1007/978-3-030-59713-9_53
Automatic Tooth Segmentation and Dense Correspondence of 3D Dental Model.,"[{'@pid': '226/4893', 'text': 'Diya Sun'}, {'@pid': '93/6606', 'text': 'Yuru Pei'}, {'@pid': '159/9378', 'text': 'Peixin Li'}, {'@pid': '264/4244', 'text': 'Guangying Song'}, {'@pid': '205/3597', 'text': 'Yuke Guo'}, {'@pid': '20/5020', 'text': 'Hongbin Zha'}, {'@pid': '71/5365', 'text': 'Tianmin Xu'}]","['Toothannotation', 'Densecorrespondence', 'Instance-awaregeodesicmap']","In this paper, we propose an end-to-end coupled 3D tooth segmentation and dense correspondence network (c-SCN) for annotation of individual teeth and gingiva of clinically-obtained 3D dental models. The proposed model can be stacked on an existing graph convolutional network (GCN) for feature extraction from dental meshes. We devise a branch network for the instance-aware geodesic maps with respect to virtual tooth crown centroids for feature enhancement. The geodesic map encodes the spatial relationship of an individual tooth with the remaining dental model, and is concatenated with the GCN-based vertex-wise feature fields for simultaneous tooth segmentation and labeling. Furthermore, the label probability matrix from the multi-category classifier, indicating individual tooth regions and boundaries, is used to enhance the inference of dense correspondence. By utilizing the smooth semantic correspondence with the preservation of geometric topology, our approach addresses the attribute transfer-based landmark location. The qualitative and quantitative evaluations on the clinically-obtained dental models of orthodontic patients demonstrate that our approach achieves effective tooth annotation and dense correspondence, outperforming the compared state-of-the-art.",2020/9/29,1,10.1007/978-3-030-59719-1_68
A Semi-supervised Joint Network for Simultaneous Left Ventricular Motion Tracking and Segmentation in 4D Echocardiography.,"[{'@pid': '218/2102', 'text': 'Kevinminh Ta'}, {'@pid': '266/7123', 'text': 'Shawn S. Ahn'}, {'@pid': '187/6098', 'text': 'John C. Stendahl'}, {'@pid': '90/635', 'text': 'Albert J. Sinusas'}, {'@pid': '96/4489', 'text': 'James S. Duncan'}]","['Echocardiography', 'Motiontracking', 'Segmentation']","This work presents a novel deep learning method to combine segmentation and motion tracking in 4D echocardiography. The network iteratively trains a motion branch and a segmentation branch. The motion branch is initially trained entirely unsupervised and learns to roughly map the displacements between a source and a target frame. The estimated displacement maps are then used to generate pseudo-ground truth labels to train the segmentation branch. The labels predicted by the trained segmentation branch are fed back into the motion branch and act as landmarks to help retrain the branch to produce smoother displacement estimations. These smoothed out displacements are then used to obtain smoother pseudo-labels to retrain the segmentation branch. Additionally, a biomechanically-inspired incompressibility constraint is implemented in order to encourage more realistic cardiac motion. The proposed method is evaluated against other approaches using synthetic and in-vivo canine studies. Both the segmentation and motion tracking results of our model perform favorably against competing methods.",2020/9/29,1,10.1007/978-3-030-59725-2_45
E2Net - An Edge Enhanced Network for Accurate Liver and Tumor Segmentation on CT Scans.,"[{'@pid': '20/8578', 'text': 'Youbao Tang'}, {'@pid': '03/7238', 'text': 'Yuxing Tang'}, {'@pid': '40/5552', 'text': 'Yingying Zhu'}, {'@pid': '67/4008-6', 'text': 'Jing Xiao 0006'}, {'@pid': '25/2710', 'text': 'Ronald M. Summers'}]","['Edgeenhancednetwork', 'Crossfeaturefusion', 'Liversegmentation', 'Tumorsegmentation', 'CTscans']","Developing an effective liver and liver tumor segmentation model from CT scans is very important for the success of liver cancer diagnosis, surgical planning and cancer treatment. In this work, we propose a two-stage framework for 2D liver and tumor segmentation. The first stage is a coarse liver segmentation network, while the second stage is an edge enhanced network (E\(^2\)Net) for more accurate liver and tumor segmentation. E\(^2\)Net explicitly models complementary objects (liver and tumor) and their edge information within the network to preserve the organ and lesion boundaries. We introduce an edge prediction module in E\(^2\)Net and design an edge distance map between liver and tumor boundaries, which is used as an extra supervision signal to train the edge enhanced network. We also propose a deep cross feature fusion module to refine multi-scale features from both objects and their edges. E\(^2\)Net is more easily and efficiently trained with a small labeled dataset, and it can be trained/tested on the original 2D CT slices (resolve resampling error issue in 3D models). The proposed framework has shown superior performance on both liver and liver tumor segmentation compared to several state-of-the-art 2D, 3D and 2D/3D hybrid frameworks.",2020/9/29,1,10.1007/978-3-030-59719-1_50
Revisiting Rubik&apos;s Cube - Self-supervised Learning with Volume-Wise Transformation for 3D Medical Image Segmentation.,"[{'@pid': '213/1924', 'text': 'Xing Tao'}, {'@pid': '165/6204', 'text': 'Yuexiang Li'}, {'@pid': '58/2694', 'text': 'Wenhui Zhou'}, {'@pid': '86/7113-2', 'text': 'Kai Ma 0002'}, {'@pid': '44/6510', 'text': 'Yefeng Zheng'}]","['3dmedicalimagesegmentation', 'Self-supervisedlearning', 'Rubik¡¯scube', 'Volume-wisetransformation']","Deep learning highly relies on the quantity of annotated data. However, the annotations for 3D volumetric medical data require experienced physicians to spend hours or even days for investigation. Self-supervised learning is a potential solution to get rid of the strong requirement of training data by deeply exploiting raw data information. In this paper, we propose a novel self-supervised learning framework for volumetric medical images. Specifically, we propose a context restoration task, i.e., Rubik¡¯s cube++, to pre-train 3D neural networks. Different from the existing context-restoration-based approaches, we adopt a volume-wise transformation for context permutation, which encourages network to better exploit the inherent 3D anatomical information of organs. Compared to the strategy of training from scratch, fine-tuning from the Rubik¡¯s cube++ pre-trained weight can achieve better performance in various tasks such as pancreas segmentation and brain tissue segmentation. The experimental results show that our self-supervised learning method can significantly improve the accuracy of 3D deep learning networks on volumetric medical datasets without the use of extra data.",2020/9/29,1,10.1007/978-3-030-59719-1_24
Generation of Annotated Brain Tumor MRIs with Tumor-induced Tissue Deformations for Training and Assessment of Neural Networks.,"[{'@pid': '197/4985', 'text': 'Hristina Uzunova'}, {'@pid': '82/3657', 'text': 'Jan Ehrhardt'}, {'@pid': '79/4807', 'text': 'Heinz Handels'}]","['Tumorgeneration', 'Transferlearning', 'Domaintranslation']","Machine learning methods heavily rely on the availability of large annotated datasets of a certain domain for training. However, freely available datasets of patients with pathologies rarely contain annotations of normal structures, thus cannot be used as ground truth for various image processing methods. To overcome this issue, we propose a topology preserving unpaired domain translation method, including an explicit pathology integration to generate annotated ground truth data of pathological domains. Moreover, we integrate a novel inverse probabilistic approach to generate deformations of the surrounding caused by pathological tissue. Our experiments show the necessity for annotated pathological data for algorithm evaluation. Furthermore, when training neural networks on healthy data and testing on real pathological images, the results are strongly impaired. By generating training data with pathologies using the proposed method, the performance of segmentation and registration methods increases significantly. The best results are achieved by also integrating pathology-induced tissue deformations.",2020/9/29,1,10.1007/978-3-030-59719-1_49
Statistical Atlas of C. elegans Neurons.,"[{'@pid': '117/0511', 'text': 'Erdem Varol'}, {'@pid': '242/9017', 'text': 'Amin Nejatbakhsh'}, {'@pid': '72/7683', 'text': 'Ruoxi Sun'}, {'@pid': '157/8701', 'text': 'Gonzalo E. Mena'}, {'@pid': '215/4616', 'text': 'Eviatar Yemini'}, {'@pid': '83/4368', 'text': 'Oliver Hobert'}, {'@pid': '94/2691', 'text': 'Liam Paninski'}]",[],"Constructing a statistical atlas of neuron positions in the nematode Caenorhabditis elegans enables a wide range of applications that require neural identity. These applications include annotating gene expression, extracting calcium activity, and evaluating nervous-system mutations. Large complete sets of neural annotations are necessary to determine canonical neuron positions and their associated confidence regions. Recently, a transgene of C. elegans (¡°NeuroPAL¡±) has been introduced to assign correct identities to all neurons in the worm via a deterministic, fluorescent colormap. This strain has enabled efficient and accurate annotation of worm neurons. Using a dataset of 10 worms, we propose a statistical model that captures the latent means and covariances of neuron locations, with efficient optimization strategies to infer model parameters. We demonstrate the utility of this model in two critical applications. First, we use our trained atlas to automatically annotate neuron identities in C. elegans at the state-of-the-art rate. Second, we use our atlas to compute correlations between neuron positions, thereby determining covariance in neuron placement. The code to replicate the statistical atlas is distributed publicly at https://github.com/amin-nejat/StatAtlas.",2020/9/29,1,10.1007/978-3-030-59722-1_12
Uncertainty Estimates as Data Selection Criteria to Boost Omni-Supervised Learning.,"[{'@pid': '257/1925', 'text': 'Lorenzo Venturini'}, {'@pid': '39/9774', 'text': 'Aris T. Papageorghiou'}, {'@pid': 'n/JAlisonNoble', 'text': 'J. Alison Noble'}, {'@pid': '141/8805', 'text': 'Ana I. L. Namburete'}]","['Uncertainty', 'Omni-supervisedlearning', 'Boosting']","For many medical applications, large quantities of imaging data are routinely obtained but it can be difficult and time-consuming to obtain high-quality labels for that data. We propose a novel uncertainty-based method to improve the performance of segmentation networks when limited manual labels are available in a large dataset. We estimate segmentation uncertainty on unlabeled data using test-time augmentation and test-time dropout. We then use uncertainty metrics to select unlabeled samples for further training in a semi-supervised learning framework. Compared to random data selection, our method gives a significant boost in Dice coefficient for semi-supervised volume segmentation on the EADC-ADNI/HARP MRI dataset and the large-scale INTERGROWTH-21st ultrasound dataset. Our results show a greater performance boost on the ultrasound dataset, suggesting that our method is most useful with data of lower or more variable quality.",2020/9/29,1,10.1007/978-3-030-59710-8_67
Uncertainty-Guided Efficient Interactive Refinement of Fetal Brain Segmentation from Stacks of MRI Slices.,"[{'@pid': '149/7441', 'text': 'Guotai Wang'}, {'@pid': '168/5574', 'text': 'Michael Aertsen'}, {'@pid': '168/5453', 'text': 'Jan Deprest'}, {'@pid': '40/2838', 'text': 'S¨¦bastien Ourselin'}, {'@pid': '99/4387', 'text': 'Tom Vercauteren'}, {'@pid': '53/3894', 'text': 'Shaoting Zhang'}]","['Uncertainty', 'Interactivesegmentation', 'Fetalbrain']","Segmentation of the fetal brain from stacks of motion-corrupted fetal MRI slices is important for motion correction and high-resolution volume reconstruction. Although Convolutional Neural Networks (CNNs) have been widely used for automatic segmentation of the fetal brain, their results may still benefit from interactive refinement for challenging slices. To improve the efficiency of interactive refinement process, we propose an Uncertainty-Guided Interactive Refinement (UGIR) framework. We first propose a grouped convolution-based CNN to obtain multiple automatic segmentation predictions with uncertainty estimation in a single forward pass, then guide the user to provide interactions only in a subset of slices with the highest uncertainty. A novel interactive level set method is also proposed to obtain a refined result given the initial segmentation and user interactions. Experimental results show that: (1) our proposed CNN obtains uncertainty estimation in real time which correlates well with mis-segmentations, (2) the proposed interactive level set is effective and efficient for refinement, (3) UGIR obtains accurate refinement results with around 30% improvement of efficiency by using uncertainty to guide user interactions. Our code is available online (https://github.com/HiLab-git/UGIR).",2020/9/29,1,10.1007/978-3-030-59719-1_28
Improving Amide Proton Transfer-Weighted MRI Reconstruction Using T2-Weighted Images.,"[{'@pid': '201/7129', 'text': 'Puyang Wang'}, {'@pid': '76/7058', 'text': 'Pengfei Guo'}, {'@pid': '59/3894', 'text': 'Jianhua Lu'}, {'@pid': '24/569', 'text': 'Jinyuan Zhou'}, {'@pid': '65/4185', 'text': 'Shanshan Jiang'}, {'@pid': '76/6100', 'text': 'Vishal M. Patel'}]","['Magneticresonanceimaging', 'Imagereconstruction', 'Amideprotontransferimaging']","Current protocol of Amide Proton Transfer-weighted (APTw) imaging commonly starts with the acquisition of high-resolution T2-weighted (T2w) images followed by APTw imaging at particular geometry and locations (i.e. slice) determined by the acquired T2w images. Although many advanced MRI reconstruction methods have been proposed to accelerate MRI, existing methods for APTw MRI lacks the capability of taking advantage of structural information in the acquired T2w images for reconstruction. In this paper, we present a novel APTw image reconstruction framework that can accelerate APTw imaging by reconstructing APTw images directly from highly undersampled k-space data and corresponding T2w image at the same location. The proposed framework starts with a novel sparse representation-based slice matching algorithm that aims to find the matched T2w slice given only the undersampled APTw image. A Recurrent Feature Sharing Reconstruction network (RFS-Rec) is designed to utilize intermediate features extracted from the matched T2w image by a Convolutional Recurrent Neural Network (CRNN), so that the missing structural information can be incorporated into the undersampled APT raw image thus effectively improving the image quality of the reconstructed APTw image. We evaluate the proposed method on two real datasets consisting of brain data from rats and humans. Extensive experiments demonstrate that the proposed RFS-Rec approach can outperform the state-of-the-art methods.
",2020/9/29,1,10.1007/978-3-030-59713-9_1
Anterior Segment Eye Lesion Segmentation with Advanced Fusion Strategies and Auxiliary Tasks.,"[{'@pid': '181/2613', 'text': 'Ke Wang'}, {'@pid': '95/2454', 'text': 'Xiaohong Liu'}, {'@pid': '29/177', 'text': 'Kang Zhang'}, {'@pid': '19/1766-6', 'text': 'Ting Chen 0006'}, {'@pid': '53/35', 'text': 'Guangyu Wang'}]","['Anterioreyediseases', 'Fusionstrategy', 'Auxiliarytask']","Anterior segment diseases of the eye cover a wide range of pathologies that occur at the front part of the eye such as conjunctival hemorrhage, corneal neovascularization and cataract. A tool for fast and accurate segmentation of anterior eye lesions can greatly benefit research and clinical care. However, the majority of current studies only pay attention to retinal-related diseases such as retinal diabetic retinopathy or macular degeneration and hence overlook this area. To enhance and boost research interest in this field, we construct the first anterior eye lesion segmentation dataset with pixel level annotations of four categories of lesions as well as the underlying eye structures. We propose a novel simultaneous eye lesion and structure segmentation model with two advanced feature fusion strategies and two auxiliary tasks. The fusion strategies enable knowledge gained from eye structure segmentation branch to be effectively transferred to the lesion segmentation branch, utilizing the intrinsic relationship between lesion and eye structure. The auxiliary tasks, which do not require extra manual annotations, are set to encourage the model to learn more general representations of features, thus improving the segmentation performance. Experimental results on the newly constructed dataset indicate that our model can exceed baseline models consistently and significantly. The source code and the newly constructed dataset are made public to benefit future research.",2020/9/29,1,10.1007/978-3-030-59722-1_63
Auto-weighting for Breast Cancer Classification in Multimodal Ultrasound.,"[{'@pid': '39/449', 'text': 'Jian Wang'}, {'@pid': '272/5426', 'text': 'Juzheng Miao'}, {'@pid': '44/1152-9', 'text': 'Xin Yang 0009'}, {'@pid': '96/4282', 'text': 'Rui Li'}, {'@pid': '142/5663', 'text': 'Guangquan Zhou'}, {'@pid': '219/6363', 'text': 'Yuhao Huang'}, {'@pid': '211/3312', 'text': 'Zehui Lin'}, {'@pid': '79/10698', 'text': 'Wufeng Xue'}, {'@pid': '06/4060', 'text': 'Xiaohong Jia'}, {'@pid': '233/7561', 'text': 'Jianqiao Zhou'}, {'@pid': '167/9731', 'text': 'Ruobing Huang'}, {'@pid': '02/450-1', 'text': 'Dong Ni 0001'}]","['Ultrasound', 'Breastcancer', 'Multi-modality']","Breast cancer is the most common invasive cancer in women. Besides the primary B-mode ultrasound screening, sonographers have explored the inclusion of Doppler, strain and shear-wave elasticity imaging to advance the diagnosis. However, recognizing useful patterns in all types of images and weighing up the significance of each modality can elude less-experienced clinicians. In this paper, we explore, for the first time, an automatic way to combine the four types of ultrasonography to discriminate between benign and malignant breast nodules. A novel multimodal network is proposed, along with promising learnability and simplicity to improve classification accuracy. The key is using a weight-sharing strategy to encourage interactions between modalities and adopting an additional cross-modalities objective to integrate global information. In contrast to hardcoding the weights of each modality in the model, we embed it in a Reinforcement Learning framework to learn this weighting in an end-to-end manner. Thus the model is trained to seek the optimal multimodal combination without handcrafted heuristics. The proposed framework is evaluated on a dataset contains 1616 sets of multimodal images. Results showed that the model scored a high classification accuracy of 95.4%, which indicates the efficiency of the proposed method.",2020/9/29,1,10.1007/978-3-030-59725-2_19
Deep Generative Model-Based Quality Control for Cardiac MRI Segmentation.,"[{'@pid': '63/1591', 'text': 'Shuo Wang'}, {'@pid': '83/11440', 'text': 'Giacomo Tarroni'}, {'@pid': '70/8589', 'text': 'Chen Qin'}, {'@pid': '198/0698', 'text': 'Yuanhan Mo'}, {'@pid': '171/8003', 'text': 'Chengliang Dai'}, {'@pid': '65/4423-42', 'text': 'Chen Chen 0042'}, {'@pid': '86/2890', 'text': 'Ben Glocker'}, {'@pid': 'g/YikeGuo', 'text': 'Yike Guo'}, {'@pid': '69/2478', 'text': 'Daniel Rueckert'}, {'@pid': '20/5519', 'text': 'Wenjia Bai'}]","['Cardiacsegmentation', 'Qualitycontrol', 'Generativemodel']","In recent years, convolutional neural networks have demonstrated promising performance in a variety of medical image segmentation tasks. However, when a trained segmentation model is deployed into the real clinical world, the model may not perform optimally. A major challenge is the potential poor-quality segmentations generated due to degraded image quality or domain shift issues. There is a timely need to develop an automated quality control method that can detect poor segmentations and feedback to clinicians. Here we propose a novel deep generative model-based framework for quality control of cardiac MRI segmentation. It first learns a manifold of good-quality image-segmentation pairs using a generative model. The quality of a given test segmentation is then assessed by evaluating the difference from its projection onto the good-quality manifold. In particular, the projection is refined through iterative search in the latent space. The proposed method achieves high prediction accuracy on two publicly available cardiac MRI datasets. Moreover, it shows better generalisation ability than traditional regression-based methods. Our approach provides a real-time and model-agnostic quality control for cardiac MRI segmentation, which has the potential to be integrated into clinical image analysis workflows.",2020/9/29,1,10.1007/978-3-030-59719-1_9
ICA-UNet - ICA Inspired Statistical UNet for Real-Time 3D Cardiac Cine MRI Segmentation.,"[{'@pid': '154/2965', 'text': 'Tianchen Wang'}, {'@pid': '181/2733-4', 'text': 'Xiaowei Xu 0004'}, {'@pid': '81/1130', 'text': 'Jinjun Xiong'}, {'@pid': '244/9459', 'text': 'Qianjun Jia'}, {'@pid': '239/3624', 'text': 'Haiyun Yuan'}, {'@pid': '227/7633', 'text': 'Meiping Huang'}, {'@pid': '30/6224', 'text': 'Jian Zhuang'}, {'@pid': '94/5536', 'text': 'Yiyu Shi'}]",[],"Real-time cine magnetic resonance imaging (MRI) plays an increasingly important role in various cardiac interventions. In order to enable fast and accurate visual assistance, the temporal frames need to be segmented on-the-fly. However, state-of-the-art MRI segmentation methods are used either offline because of their high computation complexity, or in real-time but with significant accuracy loss and latency increase (causing visually noticeable lag). As such, they can hardly be adopted to assist visual guidance. In this work, inspired by a new interpretation of Independent Component Analysis (ICA) 
[11] for learning, we propose a novel ICA-UNet for real-time 3D cardiac cine MRI segmentation. Experiments using the MICCAI ACDC 2017 dataset show that, compared with the state-of-the-arts, ICA-UNet not only achieves higher Dice scores, but also meets the real-time requirements for both throughput and latency (up?to 12.6\(\times \) reduction), enabling real-time guidance for cardiac interventions without visual lag.",2020/9/29,1,10.1007/978-3-030-59725-2_43
Improve Bone Age Assessment by Learning from Anatomical Local Regions.,"[{'@pid': '40/3934', 'text': 'Dong Wang'}, {'@pid': '119/0668', 'text': 'Kexin Zhang'}, {'@pid': '25/10869', 'text': 'Jia Ding'}, {'@pid': '47/1798-1', 'text': 'Liwei Wang 0001'}]","['Boneageassessment', 'Medicalimaging', 'Anatomicalinformation']","Skeletal bone age assessment (BAA), as an essential imaging examination, aims at evaluating the biological and structural maturation of human bones. In the clinical practice, Tanner and Whitehouse (TW2) method is a widely-used method for radiologists to perform BAA. The TW2 method splits the hands into Region Of Interests (ROI) and analyzes each of the anatomical ROI separately to estimate the bone age. Because of considering the analysis of local information, the TW2 method shows accurate results in practice. Following the spirit of TW2, we propose a novel model called Anatomical Local-Aware Network (ALA-Net) for automatic bone age assessment. In ALA-Net, anatomical local extraction module is introduced to learn the hand structure and extract local information. Moreover, we design an anatomical patch training strategy to provide extra regularization during the training process. Our model can detect the anatomical ROIs and estimate bone age jointly in an end-to-end manner. The experimental results show that our ALA-Net achieves a new state-of-the-art single model performance of 3.91 mean absolute error (MAE) on the public available RSNA dataset. Since the design of our model is well consistent with the well recognized TW2 method, it is interpretable and reliable for clinical usage.",2020/9/29,1,10.1007/978-3-030-59725-2_61
RVSeg-Net - An Efficient Feature Pyramid Cascade Network for Retinal Vessel Segmentation.,"[{'@pid': '35/7092', 'text': 'Wei Wang'}, {'@pid': '275/6864', 'text': 'Jiafu Zhong'}, {'@pid': '52/8869', 'text': 'Huisi Wu'}, {'@pid': '137/9246', 'text': 'Zhenkun Wen'}, {'@pid': '00/1015-1', 'text': 'Jing Qin 0001'}]","['Retinalvesselsegmentation', 'Featurepyramidcascadenetwork', 'Retinalfundusimageanalysis']","Accurate retinal vessel segmentation plays a critical role in the diagnosis of many relevant diseases. However, it remains a challenging task due to (1) the great scale variation of retinal vessels, (2) the existence of a large number of capillaries in the vascular network, and (3) the interactions of the retinal vessels and other structures in the images, which easily results in the discontinuities in the segmentation results. In addition, limited training data also often prohibit current deep learning models from being efficiently trained because of the overfitting problem. In this paper, we propose a novel and efficient feature pyramid cascade network for retinal vessel segmentation to comprehensively address these challenges; we call it RVSeg-Net. The main component of the proposed RVSeg-Net is a feature pyramid cascade (FPC) module, which is capable of capturing multi-scale features to tackle scale variations of retinal vessels and aggregating local and global context information to solve the discontinuity problem. In order to overcome the overfitting problem, we further employ octave convolution to replace the traditional vanilla convolution to greatly reduce the parameters by avoiding spatial redundancy information. We conducted extensive experiments on two mainstream retinal vessel datasets (DRIVE and CHASE\(\_\)DB1) to validate the proposed RVSeg-Net. Experimental results demonstrate the effectiveness of the proposed method, outperforming start-of-the-art approaches with much fewer parameters.",2020/9/29,1,10.1007/978-3-030-59722-1_77
BR-GAN - Bilateral Residual Generating Adversarial Network for Mammogram Classification.,"[{'@pid': '248/5699', 'text': 'Chu-ran Wang'}, {'@pid': '195/8230', 'text': 'Fandong Zhang'}, {'@pid': '90/6896', 'text': 'Yizhou Yu'}, {'@pid': '71/3387-1', 'text': 'Yizhou Wang 0001'}]","['Mammogramclassification', 'Domainknowledge', 'Cycleconsistencymechanism']","Mammogram malignancy classification with only image-level annotations is challenging due to a lack of lesion annotations. If we can generate the healthy version of the diseased data, we can easily explore the lesion features. An intuitive idea of such generation is to use existing Cycle-GAN based methods. They achieve the healthy generation regarding healthy images as reference domain, while maintaining the original content by cycle consistency mechanism. However, healthy mammogram patterns are diverse which may lead to uncertain generations. Moreover, the back translation from healthy to the original remains an ill-posed problem due to lack of lesion information. To address these problems, we propose a novel model called bilateral residual generating adversarial network(BR-GAN). We use the Cycle-GAN as a basic framework while regarding the contralateral as generation reference based on the bilateral symmetry prior. To address the ill-posed back translation problem, we propose a residual-preserved mechanism to try to preserve the lesion features from the original features. The generated features and the original features are aggregated for further classification. BR-GAN outperforms current state-of-the-art methods on INBreast and in-house datasets.",2020/9/29,1,10.1007/978-3-030-59713-9_63
Learning and Exploiting Interclass Visual Correlations for Medical Image Classification.,"[{'@pid': '34/4292-4', 'text': 'Dong Wei 0004'}, {'@pid': '194/4227-1', 'text': 'Shilei Cao 0001'}, {'@pid': '86/7113-2', 'text': 'Kai Ma 0002'}, {'@pid': '44/6510', 'text': 'Yefeng Zheng'}]","['Computer-aideddiagnosis', 'Softlabel', 'Deepmetriclearning']","Deep neural network-based medical image classifications often use ¡°hard¡± labels for training, where the probability of the correct category is 1 and those of others are 0. However, these hard targets can drive the networks over-confident about their predictions and prone to overfit the training data, affecting model generalization and adaption. Studies have shown that label smoothing and softening can improve classification performance. Nevertheless, existing approaches are either non-data-driven or limited in applicability. In this paper, we present the Class-Correlation Learning Network (CCL-Net) to learn interclass visual correlations from given training data, and produce soft labels to help with classification tasks. Instead of letting the network directly learn the desired correlations, we propose to learn them implicitly via distance metric learning of class-specific embeddings with a lightweight plugin CCL block. An intuitive loss based on a geometrical explanation of correlation is designed for bolstering learning of the interclass correlations. We further present end-to-end training of the proposed CCL block as a plugin head together with the classification backbone while generating soft labels on the fly. Our experimental results on the International Skin Imaging Collaboration 2018 dataset demonstrate effective learning of the interclass correlations from training data, as well as consistent improvements in performance upon several widely used modern network structures with the CCL block.",2020/9/29,1,10.1007/978-3-030-59710-8_11
MitoEM Dataset - Large-Scale 3D Mitochondria Instance Segmentation from EM Images.,"[{'@pid': '89/10116', 'text': 'Donglai Wei'}, {'@pid': '245/9022', 'text': 'Zudi Lin'}, {'@pid': '275/6974', 'text': 'Daniel Franco-Barranco'}, {'@pid': '275/6997', 'text': 'Nils Wendt'}, {'@pid': '88/10181', 'text': 'Xingyu Liu'}, {'@pid': '179/9317', 'text': 'Wenjie Yin'}, {'@pid': '98/5766', 'text': 'Xin Huang'}, {'@pid': '223/4539', 'text': 'Aarush Gupta'}, {'@pid': '124/7034', 'text': 'Won-Dong Jang'}, {'@pid': '82/1045', 'text': 'Xueying Wang'}, {'@pid': '26/5176', 'text': 'Ignacio Arganda-Carreras'}, {'@pid': '70/6445', 'text': 'Jeff W. Lichtman'}, {'@pid': 'p/HanspeterPfister', 'text': 'Hanspeter Pfister'}]","['Mitochondria', 'EMdataset', '3Dinstancesegmentation']","Electron microscopy (EM) allows the identification of intracellular organelles such as mitochondria, providing insights for clinical and scientific studies. However, public mitochondria segmentation datasets only contain hundreds of instances with simple shapes. It is unclear if existing methods achieving human-level accuracy on these small datasets are robust in practice. To this end, we introduce the MitoEM dataset, a 3D mitochondria instance segmentation dataset with two (30?\(\upmu \)m)\(^3\) volumes from human and rat cortices respectively, 3,600\(\times \) larger than previous benchmarks.  With around 40K instances, we find a great diversity of mitochondria in terms of shape and density. For evaluation, we tailor the implementation of the average precision (AP) metric for 3D data with a 45\(\times \) speedup. On MitoEM, we find existing instance segmentation methods often fail to correctly segment mitochondria with complex shapes or close contacts with other instances. Thus, our MitoEM dataset poses new challenges to the field. We release our code and data: https://donglaiw.github.io/page/mitoEM/index.html.
",2020/9/29,1,10.1007/978-3-030-59722-1_7
Three-Dimensional Thyroid Assessment from Untracked 2D Ultrasound Clips.,"[{'@pid': '25/4025', 'text': 'Wolfgang Wein'}, {'@pid': '275/6754', 'text': 'Mattia Lupetti'}, {'@pid': '131/3109', 'text': 'Oliver Zettinig'}, {'@pid': '224/0524', 'text': 'Simon Jagoda'}, {'@pid': '48/3055', 'text': 'Mehrdad Salehi'}, {'@pid': '275/6906', 'text': 'Viktoria Markova'}, {'@pid': '64/5821', 'text': 'Dornoosh Zonoobi'}, {'@pid': '61/9219', 'text': 'Raphael Prevost'}]",[],"The diagnostic quantification of thyroid gland, mostly based on its volume, is commonly done by ultrasound. Typically, three orthogonal length measurements on 2D images are used to estimate the thyroid volume from an ellipsoid approximation, which may vary substantially from its true shape. In this work, we propose a more accurate direct volume determination using 3D reconstructions from two freehand clips in transverse and sagittal directions. A deep learning based trajectory estimation on individual clips is followed by an image-based 3D model optimization of the overlapping transverse and sagittal image data. The image data and automatic thyroid segmentation are then reconstructed and compared in 3D space. The algorithm is tested on 200 pairs of sweeps, and shows that it can provide fully automated, but also more accurate and consistent volume estimations than the standard ellipsoid method, with a median volume error of \(11\%\).",2020/9/29,1,10.1007/978-3-030-59716-0_49
A Kernelized Multi-level Localization Method for Flexible Shape Modeling with Few Training Data.,"[{'@pid': '15/9855', 'text': 'Matthias Wilms'}, {'@pid': '82/3657', 'text': 'Jan Ehrhardt'}, {'@pid': '87/1218', 'text': 'Nils D. Forkert'}]","['Statisticalshapemodels', 'Kernels', 'Fewtrainingdata']","Statistical shape models (SSMs) are a standard generative shape modeling technique and they are still successfully employed in modern deep learning-based solutions for data augmentation purposes or as shape priors. However, with few training samples they often fail to represent local shape variations. Recently, a new state-of-the-art method has been proposed to alleviate this problem via a multi-level model localization scheme using distance-based covariance manipulations and Grassmannian-based level fusion during model training. This method significantly improves a SSMs performance, but heavily relies on costly eigendecompositions of large covariance matrices. In this paper, we derive a novel computationally-efficient formulation of the original method using ideas from kernel theory and randomized eigendecomposition. The proposed extension leads to a multi-level localization method for large-scale shape modeling problems that preserves the key characteristics of the original method while also improving its performance. Furthermore, our extensive evaluation on two publicly available data sets reveals the benefits of Grassmannian-based level fusion in contrast to a method derived from the popular Gaussian Process Morphable Models framework.",2020/9/29,1,10.1007/978-3-030-59719-1_74
Leveraging Undiagnosed Data for Glaucoma Classification with Teacher-Student Learning.,"[{'@pid': '39/2439', 'text': 'Junde Wu'}, {'@pid': '25/6555', 'text': 'Shuang Yu'}, {'@pid': '135/7011', 'text': 'Wenting Chen'}, {'@pid': '86/7113-2', 'text': 'Kai Ma 0002'}, {'@pid': '47/4111', 'text': 'Rao Fu'}, {'@pid': '238/1460', 'text': 'Hanruo Liu'}, {'@pid': '237/9979', 'text': 'Xiaoguang Di'}, {'@pid': '44/6510', 'text': 'Yefeng Zheng'}]","['Glaucomaclassification', 'Teacher-StudentLearning', 'Unlabeleddata']","Recently, deep learning has been adopted to the glaucoma classification task with performance comparable to that of human experts. However, a well trained deep learning model demands a large quantity of properly labeled data, which is relatively expensive since the accurate labeling of glaucoma requires years of specialist training. In order to alleviate this problem, we propose a glaucoma classification framework which takes advantage of not only the properly labeled images, but also undiagnosed images without glaucoma labels. To be more specific, the proposed framework is adapted from the teacher-student-learning paradigm. The teacher model encodes the wrapped information of undiagnosed images to a latent feature space, meanwhile the student model learns from the teacher through knowledge transfer to improve the glaucoma classification. For the model training procedure, we propose a novel training strategy that simulates the real-world teaching practice named as ¡°Learning To Teach with Knowledge Transfer (L2T-KT)"", and establish a¡°Quiz Pool"" as the teacher¡¯s optimization target. Experiments show that the proposed framework is able to utilize the undiagnosed data effectively to improve the glaucoma prediction performance.",2020/9/29,1,10.1007/978-3-030-59710-8_71
Instance-Aware Self-supervised Learning for Nuclei Segmentation.,"[{'@pid': '215/8397', 'text': 'Xinpeng Xie'}, {'@pid': '03/1390', 'text': 'Jiawei Chen'}, {'@pid': '165/6204', 'text': 'Yuexiang Li'}, {'@pid': '88/5607', 'text': 'Linlin Shen'}, {'@pid': '86/7113-2', 'text': 'Kai Ma 0002'}, {'@pid': '44/6510', 'text': 'Yefeng Zheng'}]","['Self-supervisedlearning', 'Nucleiinstancesegmentation', 'Histopathologicalimages']","Due to the wide existence and large morphological variances of nuclei, accurate nuclei instance segmentation is still one of the most challenging tasks in computational pathology. The annotating of nuclei instances, requiring experienced pathologists to manually draw the contours, is extremely laborious and expensive, which often results in the deficiency of annotated data. The deep learning based segmentation approaches, which highly rely on the quantity of training data, are difficult to fully demonstrate their capacity in this area. In this paper, we propose a novel self-supervised learning framework to deeply exploit the capacity of widely-used convolutional neural networks (CNNs) on the nuclei instance segmentation task. The proposed approach involves two sub-tasks (i.e., scale-wise triplet learning and count ranking), which enable neural networks to implicitly leverage the prior-knowledge of nuclei size and quantity, and accordingly mine the instance-aware feature representations from the raw data. Experimental results on the publicly available MoNuSeg dataset show that the proposed self-supervised learning approach can remarkably boost the segmentation accuracy of nuclei instance¡ªa new state-of-the-art average Aggregated Jaccard Index (AJI) of 70.63%, is achieved by our self-supervised ResUNet-101. To our best knowledge, this is the first work focusing on the self-supervised learning for instance segmentation.",2020/9/29,1,10.1007/978-3-030-59722-1_33
Clinically Translatable Direct Patlak Reconstruction from Dynamic PET with Motion Correction Using Convolutional Neural Network.,"[{'@pid': '231/1840', 'text': 'Nuobei Xie'}, {'@pid': '207/0305', 'text': 'Kuang Gong'}, {'@pid': '89/2184', 'text': 'Ning Guo'}, {'@pid': '274/2922', 'text': 'ZhiXing Qin'}, {'@pid': '223/4640', 'text': 'Jianan Cui'}, {'@pid': '28/8387', 'text': 'Zhifang Wu'}, {'@pid': '48/4950', 'text': 'Huafeng Liu'}, {'@pid': '70/2532', 'text': 'Quanzheng Li'}]","['DynamicPET', 'Patlakmodel', 'Motioncorrection', 'Directparametricreconstruction', 'Convolutionalneuralnetwork']","Patlak model is widely used in 18F-FDG dynamic positron emission tomography (PET) imaging, where the estimated parametric images reveal important biochemical and physiology information. Because of better noise modeling and more information extracted from raw sinogram, direct Patlak reconstruction gains its popularity over the indirect approach which utilizes reconstructed dynamic PET images alone. As the prerequisite of direct Patlak methods, raw data from dynamic PET are rarely stored in clinics and difficult to obtain. In addition, the direct reconstruction is time-consuming due to the bottleneck of multiple-frame reconstruction. All of these impede the clinical adoption of direct Patlak reconstruction. In this work, we proposed a data-driven framework which maps the dynamic PET images to the high-quality motion-corrected direct Patlak images through a convolutional neural network. For the patient¡¯s motion during the long period of dynamic PET scan, we combined the correction with the backward/forward projection in direct reconstruction to better fit the statistical model. Results based on fifteen clinical 18F-FDG dynamic brain PET datasets demonstrates the superiority of the proposed framework over Gaussian, nonlocal mean and BM4D denoising, regarding the image bias and contrast-to-noise ratio.",2020/9/29,1,10.1007/978-3-030-59728-3_77
Improve Unseen Domain Generalization via Enhanced Local Color Transformation.,"[{'@pid': '275/6766', 'text': 'Jianhao Xiong'}, {'@pid': '275/6762', 'text': 'Andre Wang He'}, {'@pid': '221/9443', 'text': 'Meng Fu'}, {'@pid': '142/2109', 'text': 'Xinyue Hu'}, {'@pid': '57/4707', 'text': 'Yifan Zhang'}, {'@pid': '91/8265', 'text': 'Congxin Liu'}, {'@pid': '68/2766', 'text': 'Xin Zhao'}, {'@pid': '147/2757', 'text': 'Zongyuan Ge'}]","['Deeplearning', 'Fundusimage', 'Unseendomain']","Recent application of deep learning in medical image achieves expert-level accuracy. However, the accuracy often degrades greatly on unseen data, for example data from different device designs and population distributions. In this work, we consider a realistic problem of domain generalization in fundus image analysis: when a model is trained on a certain domain but tested on unseen domains. Here, the known domain data is taken from a single fundus camera manufacture, i.e. Canon. The unseen data are the image from different demographic population and with distinct photography styles. Specifically, the unseen images are taken from Topcom, Syseye and Crystalvue cameras. The model performance is evaluated by two objectives: age regression and diabetic retinopathy (DR) classification. We found that the model performance on unseen domain could decrease significantly. For example, the mean absolute error (MAE) of age prediction could increase by 57.7 %. To remedy this problem, we introduce an easy-to-use method, named enhanced domain transformation (EDT), to improve the performance on both seen and unseen data. The goal of EDT is to achieve domain adaptation without using labeling and training on unseen images. We evaluate our method comprehensively on seen and unseen data sets considering the factors of demographic distribution, image style and prediction task. All the results demonstrate that EDT improves the performance on seen and unseen data in the tasks of age prediction and DR classification. Equipped with EDT, the \(\mathrm{R}^2\) (coefficient of determination) of age prediction could be greatly improved from 0.599 to 0.765 (n?=?29,577) on Crystalvue images, and AUC (area under curve) of DR classification increases from 0.875 to 0.922 (n?=?1,015).",2020/9/29,1,10.1007/978-3-030-59713-9_42
Semi-supervised Learning for Fetal Brain MRI Quality Assessment with ROI Consistency.,"[{'@pid': '211/7048', 'text': 'Junshen Xu'}, {'@pid': '268/6584', 'text': 'Sayeri Lala'}, {'@pid': '134/9916', 'text': 'Borjan A. Gagoski'}, {'@pid': '237/9751', 'text': 'Esra Abaci Turk'}, {'@pid': '30/516', 'text': 'Patricia Ellen Grant'}, {'@pid': 'g/PolinaGolland', 'text': 'Polina Golland'}, {'@pid': '52/880', 'text': 'Elfar Adalsteinsson'}]","['Imagequalityassessment', 'Fetalmagneticresonanceimaging(MRI)', 'Semi-supervisedlearning', 'Convolutionalneuralnetwork(CNN)']","Fetal brain MRI is useful for diagnosing brain abnormalities but is challenged by fetal motion. The current protocol for T2-weighted fetal brain MRI is not robust to motion so image volumes are degraded by inter- and intra- slice motion artifacts. Besides, manual annotation for fetal MR image quality assessment are usually time-consuming. Therefore, in this work, a semi-supervised deep learning method that detects slices with artifacts during the brain volume scan is proposed. Our method is based on the mean teacher model, where we not only enforce consistency between student and teacher models on the whole image, but also adopt an ROI consistency loss to guide the network to focus on the brain region. The proposed method is evaluated on a fetal brain MR dataset with 11,223 labeled images and more than 200,000 unlabeled images. Results show that compared with supervised learning, the proposed method can improve model accuracy by about 6% and outperform other state-of-the-art semi-supervised learning methods. The proposed method is also implemented and evaluated on an MR scanner, which demonstrates the feasibility of online image quality assessment and image reacquisition during fetal MR scans.",2020/9/29,1,10.1007/978-3-030-59725-2_37
Cascaded Robust Learning at Imperfect Labels for Chest X-ray Segmentation.,"[{'@pid': '35/8195', 'text': 'Cheng Xue'}, {'@pid': '43/7034', 'text': 'Qiao Deng'}, {'@pid': '02/9850', 'text': 'Xiaomeng Li'}, {'@pid': '165/7846', 'text': 'Qi Dou'}, {'@pid': '52/2889', 'text': 'Pheng-Ann Heng'}]","['Robustlearning', 'Imperfectlabel', 'Lungsegmentation']","The superior performance of CNN on medical image analysis heavily depends on the annotation quality, such as the number of labeled images, the source of images, and the expert experience. The annotation requires great expertise and labor. To deal with the high inter-rater variability, the study of the imperfect label has great significance in medical image segmentation tasks. In this paper, we present a novel cascaded robust learning framework for chest X-ray segmentation with imperfect annotation at the boundary. Our model consists of three independent networks, which can effectively learn useful information from peer networks. The framework includes two stages. In the first stage, we select the clean annotated samples via a model committee setting, the networks are trained by minimizing a segmentation loss using the selected clean samples. In the second stage, we design a joint optimization framework with label correction to gradually correct the wrong annotation and improve the network performance. We conduct experiments on the public chest X-ray image datasets collected by Shenzhen Hospital. The results show that our methods could achieve a significant improvement on the accuracy in segmentation tasks compared to the previous methods.",2020/9/29,1,10.1007/978-3-030-59725-2_56
MS-NAS - Multi-scale Neural Architecture Search for Medical Image Segmentation.,"[{'@pid': '270/3464', 'text': 'Xingang Yan'}, {'@pid': '151/4529', 'text': 'Weiwen Jiang'}, {'@pid': '94/5536', 'text': 'Yiyu Shi'}, {'@pid': '05/853', 'text': 'Cheng Zhuo'}]",[],"The recent breakthroughs of Neural Architecture Search (NAS) have motivated various applications in medical image segmentation. However, most existing work either simply rely on hyper-parameter tuning or stick to a fixed network backbone, thereby limiting the underlying search space to identify more efficient architecture. This paper presents a Multi-Scale NAS (MS-NAS) framework that is featured with multi-scale search space from network backbone to cell operation, and multi-scale fusion capability to fuse features with different sizes. To mitigate the computational overhead due to the larger search space, a partial channel connection scheme and a two-step decoding method are utilized to reduce computational overhead while maintaining optimization quality. Experimental results show that on various datasets for segmentation, MS-NAS outperforms the state-of-the-art methods and achieves 0.6¨C5.4% mIOU and 0.4¨C3.5% DSC improvements, while the computational resource consumption is reduced by 18.0¨C24.9%.",2020/9/29,1,10.1007/978-3-030-59710-8_38
CircleNet - Anchor-Free Glomerulus Detection with Circle Representation.,"[{'@pid': '266/7392', 'text': 'Haichun Yang'}, {'@pid': '266/7853', 'text': 'Ruining Deng'}, {'@pid': '263/1308', 'text': 'Yuzhe Lu'}, {'@pid': '266/7683', 'text': 'Zheyu Zhu'}, {'@pid': '14/3310', 'text': 'Ye Chen'}, {'@pid': '266/7757', 'text': 'Joseph T. Roland'}, {'@pid': '78/6574-1', 'text': 'Le Lu 0001'}, {'@pid': '74/1206', 'text': 'Bennett A. Landman'}, {'@pid': '266/7761', 'text': 'Agnes B. Fogo'}, {'@pid': '69/10047', 'text': 'Yuankai Huo'}]","['Detection', 'CircleNet', 'Anchor-free', 'Pathology']","Object detection networks are powerful in computer vision, but not necessarily optimized for biomedical object detection. In this work, we propose CircleNet, a simple anchor-free detection method with circle representation for detection of the ball-shaped glomerulus. Different from the traditional bounding box based detection method, the bounding circle (1) reduces the degrees of freedom of detection representation, (2) is naturally rotation invariant, (3) and optimized for ball-shaped objects. The key innovation to enable this representation is the anchor-free framework with the circle detection head. We evaluate CircleNet in the context of detection of glomerulus. CircleNet increases average precision of the glomerulus detection from 0.598 to 0.647. Another key advantage is that CircleNet achieves better rotation consistency compared with bounding box representations.",2020/9/29,1,10.1007/978-3-030-59719-1_4
Model-Driven Deep Attention Network for Ultra-fast Compressive Sensing MRI Guided by Cross-contrast MR Image.,"[{'@pid': '37/1091', 'text': 'Yan Yang'}, {'@pid': '44/5770', 'text': 'Na Wang'}, {'@pid': '02/9829', 'text': 'Heran Yang'}, {'@pid': '68/4942-9', 'text': 'Jian Sun 0009'}, {'@pid': '25/3264', 'text': 'Zongben Xu'}]","['Multi-contrastMRI', 'Priorlearning', 'Model-drivennet']","Speeding up Magnetic Resonance Imaging (MRI) is an inevitable task in capturing multi-contrast MR images for medical diagnosis. In MRI, some sequences, e.g., in T2 weighted imaging, require long scanning time, while T1 weighted images are captured by short-time sequences. To accelerate MRI, in this paper, we propose a model-driven deep attention network, dubbed as MD-DAN, to reconstruct highly under-sampled long-time sequence MR image with the guidance of a certain short-time sequence MR image. MD-DAN is a novel deep architecture inspired by the iterative algorithm optimizing a novel MRI reconstruction model regularized by cross-contrast prior using a guided contrast image. The network is designed to automatically learn cross-contrast prior by learning corresponding proximal operator. The backbone network to model the proximal operator is designed as a dual-path convolutional network with channel and spatial attention modules. Experimental results on a brain MRI dataset substantiate the superiority of our method with significantly improved accuracy. For example, MD-DAN achieves PSNR up?to 35.04?dB at the ultra-fast 1/32 sampling rate.",2020/9/29,1,10.1007/978-3-030-59713-9_19
DeepPrognosis - Preoperative Prediction of Pancreatic Cancer Survival and Surgical Margin via Contrast-Enhanced CT Imaging.,"[{'@pid': '150/6696', 'text': 'Jiawen Yao'}, {'@pid': '55/4736', 'text': 'Yu Shi'}, {'@pid': '78/6574-1', 'text': 'Le Lu 0001'}, {'@pid': '67/4008-6', 'text': 'Jing Xiao 0006'}, {'@pid': '76/5973', 'text': 'Ling Zhang'}]","['PancreaticDuctalAdenocarcinoma(PDAC)', '3DContrast-EnhancedConvolutionalLSTM(CE-ConvLSTM)', 'Preoperativesurvivalprediction']","Pancreatic ductal adenocarcinoma (PDAC) is one of the most lethal cancers and carries a dismal prognosis. Surgery remains the best chance of a potential cure for patients who are eligible for initial resection of PDAC. However, outcomes vary significantly even among the resected patients of the same stage and received similar treatments. Accurate preoperative prognosis of resectable PDACs for personalized treatment is thus highly desired. Nevertheless, there are no automated methods yet to fully exploit the contrast-enhanced computed tomography (CE-CT) imaging for PDAC. Tumor attenuation changes across different CT phases can reflect the tumor internal stromal fractions and vascularization of individual tumors that may impact the clinical outcomes. In this work, we propose a novel deep neural network for the survival prediction of resectable PDAC patients, named as 3D Contrast-Enhanced Convolutional Long Short-Term Memory network (CE-ConvLSTM), which can derive the tumor attenuation signatures or patterns from CE-CT imaging studies. We present a multi-task CNN to accomplish both tasks of outcome and margin prediction where the network benefits from learning the tumor resection margin related features to improve survival prediction. The proposed framework can improve the prediction performances compared with existing state-of-the-art survival analysis approaches. The tumor signature built from our model has evidently added values to be combined with the existing clinical staging system.",2020/9/29,1,10.1007/978-3-030-59713-9_27
Multi-scale Enhanced Graph Convolutional Network for Early Mild Cognitive Impairment Detection.,"[{'@pid': '250/8742', 'text': 'Shuangzhi Yu'}, {'@pid': '130/5774', 'text': 'Shuqiang Wang'}, {'@pid': '226/3490', 'text': 'Xiaohua Xiao'}, {'@pid': '62/5220', 'text': 'Jiuwen Cao'}, {'@pid': '179/3244', 'text': 'Guanghui Yue'}, {'@pid': '45/8193', 'text': 'Dongdong Liu'}, {'@pid': '25/3611', 'text': 'Tianfu Wang'}, {'@pid': '272/9600', 'text': 'Yanwu Xu'}, {'@pid': '49/9638', 'text': 'Baiying Lei'}]","['Earlymildcognitiveimpairment', 'Multi-scaleenhancedgraphconvolutionalnetwork', 'Functionalandstructuralinformation']","Early mild cognitive impairment (EMCI) is an early stage of MCI, which can be detected by brain connectivity networks. To detect EMCI, we design a novel framework based on multi-scale enhanced GCN (MSE-GCN) in this paper, which fuses the functional and structural information from the resting-state functional magnetic resonance imaging and diffusion tensor imaging, respectively. Then both functional and structural information in connectivity networks are integrated via the local weighted clustering coefficients (LWCC), which are concatenated as the feature vectors to represent the vertices of population graph. Simultaneously, the subject¡¯s gender and age in-formation is combined with the multi-modal neuroimaging feature to build a sparse graph. Then, we design multiple parallel GCN layers with different inputs by random walk embedding, which can identify the intrinsic MCI graph information from the embedding in GCN. Finally, we concatenate the output of all the GCN layers in the full connection layer for detection. The proposed method is capable of simultaneously representing the individual features and information associations among subjects from potential patients. The experimental results on the public Alzheimer¡¯s Disease Neuroimaging Initiative (ADNI) dataset demonstrate that our proposed method achieves impressive EMCI identification performance compared with all competing methods.",2020/9/29,1,10.1007/978-3-030-59728-3_23
Characterizing Label Errors - Confident Learning for Noisy-Labeled Image Segmentation.,"[{'@pid': '26/7487', 'text': 'Minqing Zhang'}, {'@pid': '265/1310', 'text': 'Jiantao Gao'}, {'@pid': '211/4287', 'text': 'Zhen Lyu'}, {'@pid': '239/7036', 'text': 'Weibing Zhao'}, {'@pid': '35/1647', 'text': 'Qin Wang'}, {'@pid': '194/1109', 'text': 'Weizhen Ding'}, {'@pid': '85/1868', 'text': 'Sheng Wang'}, {'@pid': '74/2397-26', 'text': 'Zhen Li 0026'}, {'@pid': '48/4914', 'text': 'Shuguang Cui'}]",[],"Convolutional neural networks (CNNs) have achieved remarkable performance in image processing for its mighty capability to fit huge amount of data. However, if the training data are corrupted by noisy labels, the resulting performance might be deteriorated. In the domain of medical image analysis, this dilemma becomes extremely severe. This is because the medical image annotation always requires medical expertise and clinical experience, which would inevitably introduce subjectivity. In this paper, we design a novel algorithm based on the teacher-student architecture for noisy-labeled medical image segmentation. Creatively, We introduce confident learning (CL) method to identify the corrupted labels and endow CNN an anti-interference ability to the noises. Specifically, the CL technique is introduced to the teacher model to characterize the suspected wrong-labeled pixels. Since the noise identification maps are a little away from sufficient precision, the spatial label smoothing regularization technique is utilized to generate soft-corrected masks for training the student model. Since our method identifies and revises the noisy labels of the training data in a pixel-level rather than simply assigns lower weights to the noisy masks, it outperforms the state-of-the-art method in the noisy-labeled image segmentation task on the JSRT dataset, especially when the training data are severely corrupted by noises.",2020/9/29,1,10.1007/978-3-030-59710-8_70
Robust Layer Segmentation Against Complex Retinal Abnormalities for en face OCTA Generation.,"[{'@pid': '06/7406', 'text': 'Yuhan Zhang'}, {'@pid': '05/8125', 'text': 'Chen Huang'}, {'@pid': '89/7774', 'text': 'Mingchao Li'}, {'@pid': '246/2825', 'text': 'Sha Xie'}, {'@pid': '250/6020', 'text': 'Keren Xie'}, {'@pid': '64/7567', 'text': 'Zexuan Ji'}, {'@pid': '183/2121', 'text': 'Songtao Yuan'}, {'@pid': '62/2719-4', 'text': 'Qiang Chen 0004'}]","['Deeplearning', 'Layersegmentation', 'SD-OCT', 'OCTA']","On en face optical coherence tomography angiography (OCTA), the vascular patterns from superficial vascular complex (SVC) and deep vascular complex (DVC) are distinguishable, while outer retina is normally avascular. To visualize en face OCTA images of different vascular patterns, it is inevitable to segment the three regions. However, the automated layer segmentation still faces huge challenge towards manifold advanced tissue lesions affected eyes. In this paper, we first design a region segmentation based augmented 3D U-Net network to fuse spectral domain optical coherence tomography (SD-OCT) structural information and OCTA vascular distribution. Subsequently, an innovative multitask layer-by-layer recoding module breaks up voxel-wise region segmentation probability maps into independent refinement task aiming at further weakening the influence of retinal abnormal regions on layer segmentation. In the end, a simple and effective layer surface encoding module converts the refined region segmentation result of each layer to its continuous surface vector, which advantages are that eliminates the outlier error segmentation in region segmentation tasks and guarantees the uniqueness and strict order constraint of each retinal layer surface in each column. The model validation is carried out on 262 eyes, including 95 normal eyes and 167 multifarious abnormalities affected eyes. The experimental results demonstrate that our method achieves higher segmentation accuracy and stronger ability to fight diseases compared with state-of-the-art segmentation methods.",2020/9/29,1,10.1007/978-3-030-59722-1_62
Recovering Brain Structural Connectivity from Functional Connectivity via Multi-GCN Based Generative Adversarial Network.,"[{'@pid': '82/10609-50', 'text': 'Lu Zhang 0050'}, {'@pid': '58/6810', 'text': 'Li Wang'}, {'@pid': '69/8693', 'text': 'Dajiang Zhu'}]","['Structuralconnectivity', 'Functionalconnectivity', 'Graphconvolutionnetworks', 'Generativeadversarialnetwork']","Understanding brain structure-function relationship, e.g., the relations between brain structural connectivity (SC) and functional connectivity (FC), is critical for revealing organizational principles of human brain. However, brain¡¯s many-to-one function-structure mode, i.e., diverse functional patterns may be associated with the same SC, and the complex direct/indirect interactions in both structural and functional connectivity make it challenge to infer a reliable relationship between SC and FC. Benefiting from the advances in deep neural networks, many deep learning based approaches are developed to model the complex and non-linear relations that can be overlooked by traditional shallow methods. In this work, we proposed a multi-GCN based generative adversarial network (MGCN-GAN) to infer individual SC based on corresponding FC. The generator of MGCN-GAN is composed by multiple multi-layer graph convolution networks (GCNs) which have the capability to model complex indirect connections in brain connectivity. The discriminator of MGCN-GAN is a single multi-layer GCN which aims to distinguish predicted SC from real SC. To overcome the inherent unstable behavior of GAN, we designed a new structure-preserving (SP) loss function to guide the generator to learn the intrinsic SC patterns more effectively. We tested our model on Human Connectome Project (HCP) dataset and the proposed MGCN-GAN model can generate reliable individual SC based on FC. This result implies that there may exist a common regulation between specific brain structural and functional architectures across different individuals.",2020/9/29,1,10.1007/978-3-030-59728-3_6
Revisiting 3D Context Modeling with Supervised Pre-training for Universal Lesion Detection in CT Slices.,"[{'@pid': '30/2700', 'text': 'Shu Zhang'}, {'@pid': '118/1112', 'text': 'Jincheng Xu'}, {'@pid': '07/607', 'text': 'Yu-Chun Chen'}, {'@pid': '232/2263', 'text': 'Jiechao Ma'}, {'@pid': '175/8858', 'text': 'Zihao Li'}, {'@pid': '71/3387-1', 'text': 'Yizhou Wang 0001'}, {'@pid': '90/6896', 'text': 'Yizhou Yu'}]","['Lesiondetection', '3Dcontextmodeling', '3Dnetworkpre-training.']","Universal lesion detection from computed tomography (CT) slices is important for comprehensive disease screening. Since each lesion can locate in multiple adjacent slices, 3D context modeling is of great significance for developing automated lesion detection algorithms. In this work, we propose a Modified Pseudo-3D Feature Pyramid Network (MP3D FPN) that leverages depthwise separable convolutional filters and a group transform module (GTM) to efficiently extract 3D context enhanced 2D features for universal lesion detection in CT slices. To facilitate faster convergence, a novel 3D network pre-training method is derived using solely large-scale 2D object detection dataset in the natural image domain. We demonstrate that with the novel pre-training method, the proposed MP3D FPN achieves state-of-the-art detection performance on the DeepLesion dataset (3.48% absolute improvement in the sensitivity of FPs@0.5), significantly surpassing the baseline method by up?to 6.06% (in MAP@0.5) which adopts 2D convolution for 3D context modeling. Moreover, the proposed 3D pre-trained weights can potentially be used to boost the performance of other 3D medical image analysis tasks.",2020/9/29,1,10.1007/978-3-030-59719-1_53
Robust Medical Image Segmentation from Non-expert Annotations with Tri-network.,"[{'@pid': '77/7902', 'text': 'Tianwei Zhang'}, {'@pid': '165/8092', 'text': 'Lequan Yu'}, {'@pid': '41/10186', 'text': 'Na Hu'}, {'@pid': '275/6749', 'text': 'Su Lv'}, {'@pid': '175/1269', 'text': 'Shi Gu'}]","['Non-expertannotations', 'Tri-network', 'Collaborativelearning', 'Segmentation']","Deep convolutional neural networks (CNNs) have achieved commendable results on a variety of medical image segmentation tasks. However, CNNs usually require a large amount of training samples with accurate annotations, which are extremely difficult and expensive to obtain in medical image analysis field. In practice, we notice that the junior trainees after training can label medical images in some medical image segmentation applications. These non-expert annotations are more easily accessible and can be regarded as a source of weak annotation to guide network learning. In this paper, we propose a novel Tri-network learning framework to alleviate the problem of insufficient accurate annotations in medical segmentation tasks by utilizing the non-expert annotations. To be specific, we maintain three networks in our framework, and each pair of networks alternatively select informative samples for the third network learning, according to the consensus and difference between their predictions. The three networks are jointly optimized in such a collaborative manner. We evaluated our method on real and simulated non-expert annotated datasets. The experiment results show that our method effectively mines informative information from the non-expert annotations for improved segmentation performance and outperforms other competing methods.",2020/9/29,1,10.1007/978-3-030-59719-1_25
Deep Representation Learning for Multimodal Brain Networks.,"[{'@pid': '43/2368-10', 'text': 'Wen Zhang 0010'}, {'@pid': '33/7424', 'text': 'Liang Zhan'}, {'@pid': 't/PaulMThompson', 'text': 'Paul M. Thompson'}, {'@pid': '88/128-1', 'text': 'Yalin Wang 0001'}]","['Multimodality', 'Brainnetworks', 'Networkrepresentation', 'Deeplearning', 'Graphtopology']","Applying network science approaches to investigate the functions and anatomy of the human brain is prevalent in modern medical imaging analysis. Due to the complex network topology, for an individual brain, mining a discriminative network representation from the multimodal brain networks is non-trivial. The recent success of deep learning techniques on graph-structured data suggests a new way to model the non-linear cross-modality relationship. However, current deep brain network methods either ignore the intrinsic graph topology or require a network basis shared within a group. To address these challenges, we propose a novel end-to-end deep graph representation learning (Deep Multimodal Brain Networks - DMBN) to fuse multimodal brain networks. Specifically, we decipher the cross-modality relationship through a graph encoding and decoding process. The higher-order network mappings from brain structural networks to functional networks are learned in the node domain. The learned network representation is a set of node features that are informative to induce brain saliency maps in a supervised manner. We test our framework in both synthetic and real image data. The experimental results show the superiority of the proposed method over some other state-of-the-art deep brain network models.",2020/9/29,1,10.1007/978-3-030-59728-3_60
Cartilage Segmentation in High-Resolution 3D Micro-CT Images via Uncertainty-Guided Self-training with Very Sparse Annotation.,"[{'@pid': '31/6916-6', 'text': 'Hao Zheng 0006'}, {'@pid': '275/6942', 'text': 'Susan M. Motch Perrine'}, {'@pid': '275/6988', 'text': 'M. Kathleen Pitirri'}, {'@pid': '275/6879', 'text': 'Kazuhiko Kawasaki'}, {'@pid': 'w/ChaoliWang', 'text': 'Chaoli Wang 0001'}, {'@pid': '73/3760', 'text': 'Joan T. Richtsmeier'}, {'@pid': 'c/DannyZChen', 'text': 'Danny Z. Chen'}]","['Cartilagesegmentation', 'Uncertainty', 'Sparseannotation']","Craniofacial syndromes often involve skeletal defects of the head. Studying the development of the chondrocranium (the part of the endoskeleton that protects the brain and other sense organs) is crucial to understanding genotype-phenotype relationships and early detection of skeletal malformation. Our goal is to segment craniofacial cartilages in 3D micro-CT images of embryonic mice stained with phosphotungstic acid. However, due to high image resolution, complex object structures, and low contrast, delineating fine-grained structures in these images is very challenging, even manually. Specifically, only experts can differentiate cartilages, and it is unrealistic to manually label whole volumes for deep learning model training. We propose a new framework to progressively segment cartilages in high-resolution 3D micro-CT images using extremely sparse annotation (e.g., annotating only a few selected slices in a volume). Our model consists of a lightweight fully convolutional network (FCN) to accelerate the training speed and generate pseudo labels (PLs) for unlabeled slices. Meanwhile, we take into account the reliability of PLs using a bootstrap ensemble based uncertainty quantification method. Further, our framework gradually learns from the PLs with the guidance of the uncertainty estimation via self-training. Experiments show that our method achieves high segmentation accuracy compared to prior arts and obtains performance gains by iterative self-training.",2020/9/29,1,10.1007/978-3-030-59710-8_78
Weakly Supervised Deep Learning for Breast Cancer Segmentation with Coarse Annotations.,"[{'@pid': '31/6916', 'text': 'Hao Zheng'}, {'@pid': '275/6764', 'text': 'Zhiguo Zhuang'}, {'@pid': '226/3329', 'text': 'Yulei Qin'}, {'@pid': '54/10782', 'text': 'Yun Gu'}, {'@pid': '12/1198-2', 'text': 'Jie Yang 0002'}, {'@pid': '14/3693', 'text': 'Guang-Zhong Yang'}]","['Coarseannotations', 'Adaptiveweightedconstrainedloss', 'Uncertainty-basedboundarymap']","Cancer lesion segmentation plays a vital role in breast cancer diagnosis and treatment planning. As creating labels for large medical image datasets can be time-consuming, laborious and error prone, a framework is proposed in this paper by using coarse annotations generated from boundary scribbles for training deep convolutional neural networks. These coarse annotations include locations of lesions but are lack of accurate information about boundaries. To mitigate the negative impact of annotation errors, we propose an adaptive weighted constrained loss that can change the weight of the task-specific penalty term according to the learning process. To impose further supervision about the boundaries, uncertainty-based boundary maps are generated, which can provide better descriptions for the blurry boundaries. Validation on a dataset containing 154 MRI scans has shown an average Dice coefficient of \(82.25\%\), which is comparable to results from fine annotations, demonstrating the efficacy of the proposed approach.",2020/9/29,1,10.1007/978-3-030-59719-1_44
Deep Semi-supervised Knowledge Distillation for Overlapping Cervical Cell Instance Segmentation.,"[{'@pid': '211/7760-1', 'text': 'Yanning Zhou 0001'}, {'@pid': '86/475-11', 'text': 'Hao Chen 0011'}, {'@pid': '155/1390', 'text': 'Huangjing Lin'}, {'@pid': '52/2889', 'text': 'Pheng-Ann Heng'}]",[],"Deep learning methods show promising results for overlapping cervical cell instance segmentation. However, in order to train a model with good generalization ability, voluminous pixel-level annotations are demanded which is quite expensive and time-consuming for acquisition. In this paper, we propose to leverage both labeled and unlabeled data for instance segmentation with improved accuracy by knowledge distillation. We propose a novel Mask-guided Mean Teacher framework with Perturbation-sensitive Sample Mining (MMT-PSM), which consists of a teacher and a student network during training. Two networks are encouraged to be consistent both in feature and semantic level under small perturbations. The teacher¡¯s self-ensemble predictions from K-time augmented samples are used to construct the reliable pseudo-labels for optimizing the student. We design a novel strategy to estimate the sensitivity to perturbations for each proposal and select informative samples from massive cases to facilitate fast and effective semantic distillation. In addition, to eliminate the unavoidable noise from the background region, we propose to use the predicted segmentation mask as guidance to enforce the feature distillation in the foreground region. Experiments show that the proposed method improves the performance significantly compared with the supervised method learned from labeled data only, and outperforms state-of-the-art semi-supervised methods. Code: https://github.com/SIAAAAAA/MMT-PSM.",2020/9/29,1,10.1007/978-3-030-59710-8_51
M2 Net - Multi-modal Multi-channel Network for Overall Survival Time Prediction of Brain Tumor Patients.,"[{'@pid': '98/4450-2', 'text': 'Tao Zhou 0002'}, {'@pid': '63/7767', 'text': 'Huazhu Fu'}, {'@pid': '50/671', 'text': 'Yu Zhang'}, {'@pid': '78/2668', 'text': 'Changqing Zhang'}, {'@pid': '153/2122', 'text': 'Xiankai Lu'}, {'@pid': '38/5435', 'text': 'Jianbing Shen'}, {'@pid': '75/1281', 'text': 'Ling Shao 0001'}]",[],"Early and accurate prediction of overall survival (OS) time can help to obtain better treatment planning for brain tumor patients. Although many OS time prediction methods have been developed and obtain promising results, there are still several issues. First, conventional prediction methods rely on radiomic features at the local lesion area of a magnetic resonance (MR) volume, which may not represent the full image or model complex tumor patterns. Second, different types of scanners (i.e., multi-modal data) are sensitive to different brain regions, which makes it challenging to effectively exploit the complementary information across multiple modalities and also preserve the modality-specific properties. Third, existing methods focus on prediction models, ignoring complex data-to-label relationships. To address the above issues, we propose an end-to-end OS time prediction model; namely, Multi-modal Multi-channel Network (\(\text {M}^2\text {Net}\)). Specifically, we first project the 3D MR volume onto 2D images in different directions, which reduces computational costs, while preserving important information and enabling pre-trained models to be transferred from other tasks. Then, we use a modality-specific network to extract implicit and high-level features from different MR scans. A multi-modal shared network is built to fuse these features using a bilinear pooling model, exploiting their correlations to provide complementary information. Finally, we integrate the outputs from each modality-specific network and the multi-modal shared network to generate the final prediction result. Experimental results demonstrate the superiority of our \(\text {M}^2\text {Net}\) model over other methods.",2020/9/29,1,10.1007/978-3-030-59713-9_22
Divide-and-Rule - Self-Supervised Learning for Survival Analysis in Colorectal Cancer.,"[{'@pid': '225/5502', 'text': 'Christian Abbet'}, {'@pid': '266/7116', 'text': 'Inti Zlobec'}, {'@pid': '59/10419', 'text': 'Behzad Bozorgtabar'}, {'@pid': 't/JeanPhilippeThiran', 'text': 'Jean-Philippe Thiran'}]","['Self-supervisedlearning', 'Histology', 'Survivalanalysis', 'Colorectalcancer']","With the long-term rapid increase in incidences of colorectal cancer (CRC), there is an urgent clinical need to improve risk stratification. The conventional pathology report is usually limited to only a few histopathological features. However, most of the tumor microenvironments used to describe patterns of aggressive tumor behavior are ignored. In this work, we aim to learn histopathological patterns within cancerous tissue regions that can be used to improve prognostic stratification for colorectal cancer. To do so, we propose a self-supervised learning method that jointly learns a representation of tissue regions as well as a metric of the clustering to obtain their underlying patterns. These histopathological patterns are then used to represent the interaction between complex tissues and predict clinical outcomes directly. We furthermore show that the proposed approach can benefit from linear predictors to avoid overfitting in patient outcomes predictions. To this end, we introduce a new well-characterized clinicopathological dataset, including a retrospective collective of 374 patients, with their survival time and treatment information. Histomorphological clusters obtained by our method are evaluated by training survival models. The experimental results demonstrate statistically significant patient stratification, and our approach outperformed the state-of-the-art deep clustering methods.",2020/9/29,2,10.1007/978-3-030-59722-1_46
Tractogram Filtering of Anatomically Non-plausible Fibers with Geometric Deep Learning.,"[{'@pid': '208/4543', 'text': 'Pietro Astolfi'}, {'@pid': '262/0425', 'text': 'Ruben Verhagen'}, {'@pid': '82/4101', 'text': 'Laurent Petit'}, {'@pid': '13/2659', 'text': 'Emanuele Olivetti'}, {'@pid': '45/9222', 'text': 'Jonathan Masci'}, {'@pid': '129/6623', 'text': 'Davide Boscaini'}, {'@pid': '57/831', 'text': 'Paolo Avesani'}]",[],"Tractograms are virtual representations of the white matter fibers of the brain. They are of primary interest for tasks like presurgical planning, and investigation of neuroplasticity or brain disorders. Each tractogram is composed of millions of fibers encoded as 3D polylines. Unfortunately, a large portion of those fibers are not anatomically plausible and can be considered artifacts of the tracking algorithms. Common methods for tractogram filtering are based on signal reconstruction, a principled approach, but unable to consider the knowledge of brain anatomy. In this work, we address the problem of tractogram filtering as a supervised learning problem by exploiting the ground truth annotations obtained with a recent heuristic method, which labels fibers as either anatomically plausible or non-plausible according to well-established anatomical properties. The intuitive idea is to model a fiber as a point cloud and the goal is to investigate whether and how a geometric deep learning model might capture its anatomical properties. Our contribution is an extension of the Dynamic Edge Convolution model that exploits the sequential relations of points in a fiber and discriminates with high accuracy plausible/non-plausible fibers.",2020/9/29,2,10.1007/978-3-030-59728-3_29
3d-SMRnet - Achieving a New Quality of MPI System Matrix Recovery by Deep Learning.,"[{'@pid': '215/6564', 'text': 'Ivo M. Baltruschat'}, {'@pid': '189/2357', 'text': 'Patryk Szwargulski'}, {'@pid': '54/9855', 'text': 'Florian Griese'}, {'@pid': '241/6916', 'text': 'Mirco Grosser'}, {'@pid': '46/898', 'text': 'Ren¨¦ Werner'}, {'@pid': '76/4648-1', 'text': 'Tobias Knopp 0001'}]","['Magneticparticleimaging', 'Systemmatrixrecovering', 'Deeplearning', 'Singleimagesuper-resolution']","Magnetic particle imaging (MPI) data is commonly reconstructed using a system matrix acquired in a time-consuming calibration measurement. Compared to model-based reconstruction, the calibration approach has the important advantage that it takes into account both complex particle physics and system imperfections. However, this has the disadvantage that the system matrix has to be re-calibrated each time the scan parameters, the particle types or even the particle environment (e.g. viscosity or temperature) changes. One way to shorten the calibration time is to scan the system matrix at a subset of the spatial positions of the intended field-of-view and use the system matrix recovery. Recent approaches used compressed sensing (CS) and achieved subsampling factors up?to 28, which still allowed the reconstruction of MPI images with sufficient quality. In this work we propose a novel framework with a 3d system matrix recovery network and show that it recovers a 3d system matrix with a subsampling factor of 64 in less than a minute and outperforms CS in terms of system matrix quality, reconstructed image quality, and processing time. The advantage of our method is demonstrated by reconstructing open access MPI datasets. Furthermore, it is also shown that the model is capable of recovering system matrices for different particle types.",2020/9/29,2,10.1007/978-3-030-59713-9_8
Move Over There - One-Click Deformation Correction for Image Fusion During Endovascular Aortic Repair.,"[{'@pid': '146/6667', 'text': 'Katharina Breininger'}, {'@pid': '77/4028', 'text': 'Marcus Pfister'}, {'@pid': '17/5276', 'text': 'Markus Kowarschik'}, {'@pid': '131/7133', 'text': 'Andreas K. Maier'}]","['Deformationmodeling', 'Endovascularaorticrepair', 'EVAR', 'Imagefusion', 'Fluoroscopy']","Fusing intraoperative X-ray with information from preoperative computed tomography for endovascular aortic repair has been shown to reduce radiation exposure, need for contrast agent, and procedure time. However, due to the instruments inserted during the intervention, the vasculature deforms and the fusion loses accuracy. In this paper, we propose an approach to use minimal user input obtained from a single 2D image for deformation correction in 3D. We integrate the 2D positional information as a projective constraint in an as-rigid-as-possible deformation model that allows for intraoperative correction of the deformation. Our method achieves clinically relevant accuracies while keeping user inputs to a minimum. We are able to recover the deformation at the right and left internal iliac bifurcation up?to a 3D error of \({1.9}\,\text {mm}\), with an error of \({0.5}\,\text {mm}\) orthogonal to the viewing direction, and an error of \({1.7}\,\text {mm}\) in depth, while keeping the mean computation time below \({6}\,\text {s}\).",2020/9/29,2,10.1007/978-3-030-59719-1_69
Lymph Node Gross Tumor Volume Detection in Oncology Imaging via Relationship Learning Using Graph Neural Network.,"[{'@pid': '222/1831', 'text': 'Chun-Hung Chao'}, {'@pid': '151/6411', 'text': 'Zhuotun Zhu'}, {'@pid': '160/2794', 'text': 'Dazhou Guo'}, {'@pid': '28/7692-6', 'text': 'Ke Yan 0006'}, {'@pid': '130/6972', 'text': 'Tsung-Ying Ho'}, {'@pid': '185/6840', 'text': 'Jinzheng Cai'}, {'@pid': '60/11273', 'text': 'Adam P. Harrison'}, {'@pid': '21/9164', 'text': 'Xianghua Ye'}, {'@pid': '67/4008-6', 'text': 'Jing Xiao 0006'}, {'@pid': 'y/AlanLYuille', 'text': 'Alan L. Yuille'}, {'@pid': '62/2750', 'text': 'Min Sun'}, {'@pid': '78/6574-1', 'text': 'Le Lu 0001'}, {'@pid': '133/9440', 'text': 'Dakai Jin'}]","['Lymphnodegrosstumorvolume', 'Relationshiplearning', 'Graphneuralnetwork', 'Oncologyimaging', 'Radiotherapy']","Determining the spread of lymph node gross tumor volume (GTV\(_{LN}\)) is essential in defining the respective resection or irradiating regions for the downstream workflows of surgical resection and radiotherapy for many cancers. Different from the more common enlarged lymph node (LN), GTV\(_{LN}\) also includes smaller ones if associated with high positron emission tomography signals and/or any metastasis signs in CT. This is a daunting task. In this work, we propose a unified LN appearance and inter-LN relationship learning framework to detect the true GTV\(_{LN}\). This is motivated by the prior clinical knowledge that LNs form a connected lymphatic system, and the spread of cancer cells among LNs often follows certain pathways. Specifically, we first utilize a 3D convolutional neural network with ROI-pooling to extract the GTV\(_{LN}\)¡¯s instance-wise appearance features. Next, we introduce a graph neural network to further model the inter-LN relationships where the global LN-tumor spatial priors are included in the learning process. This leads to an end-to-end trainable network to detect by classifying GTV\(_{LN}\). We operate our model on a set of GTV\(_{LN}\) candidates generated by a preliminary 1st-stage method, which has a sensitivity of \({&gt;}85\%\) at the cost of high false positive (FP) (\({&gt;}15\) FPs per patient). We validate our approach on a radiotherapy dataset with 142 paired PET/RTCT scans containing the chest and upper abdominal body parts. The proposed method significantly improves over the state-of-the-art (SOTA) LN classification method by \(5.5\%\) and \(13.1\%\) in F1 score and the averaged sensitivity value at 2,?3,?4,?6 FPs per patient, respectively.",2020/9/29,2,10.1007/978-3-030-59728-3_75
Estimating Common Harmonic Waves of Brain Networks on Stiefel Manifold.,"[{'@pid': '47/8066', 'text': 'Jiazhou Chen'}, {'@pid': '66/984', 'text': 'Guoqiang Han'}, {'@pid': '50/3384', 'text': 'Hongmin Cai'}, {'@pid': '226/3693', 'text': 'Junbo Ma'}, {'@pid': '47/3680', 'text': 'Minjeong Kim'}, {'@pid': '64/5632', 'text': 'Paul Laurienti'}, {'@pid': '03/5225-1', 'text': 'Guorong Wu 0001'}]","['Brainnetwork', 'Manifoldoptimization', 'Harmonics', 'Fr¨¦chetmean', 'Geometricmedian']","Network neuroscience has been widely studied in understanding brain functions as well as the neurobiological underpinnings of cognition and behavior that are related to the development of neuro-disorders. Since the network organization is inherently governed by the harmonic waves (Eigensystem) of the underlying Laplacian matrix, discovering the harmonic-like alterations emerges as a new research interest in understanding the factors behind brain developmental and neurodegenerative diseases, where an unbiased reference space of harmonic waves is often required to quantify the difference across individuals with standard measurement. However, simple arithmetic averaging over the individual harmonic waves is commonly used in current studies, despite that such Euclidean operation might break down the intrinsic data geometry of harmonic waves. To overcome this limitation, we propose a novel manifold optimization framework to find the group mean (aka. common harmonic waves), where each set of harmonic waves from the individual subject is treated as a data sample residing on the Stiefel manifold. To further improve the robustness of learned common harmonic waves to possible outliers, we promote the common harmonic waves to the setting of a geometric median on Stiefel manifold, instead of Fr¨¦chet mean, by optimizing towards the \( \ell_{1} \)-norm shortest overall geodesic distance on the manifold. We have compared our proposed method with the existing methods on both synthetic and real network data. The experimental results indicate that our proposed approach shows improvements in accuracy and statistical power.",2020/9/29,2,10.1007/978-3-030-59728-3_36
Realistic Adversarial Data Augmentation for MR Image Segmentation.,"[{'@pid': '65/4423-42', 'text': 'Chen Chen 0042'}, {'@pid': '70/8589', 'text': 'Chen Qin'}, {'@pid': '247/5881', 'text': 'Huaqi Qiu'}, {'@pid': '13/10121', 'text': 'Cheng Ouyang'}, {'@pid': '63/1591', 'text': 'Shuo Wang'}, {'@pid': '01/5394', 'text': 'Liang Chen'}, {'@pid': '83/11440', 'text': 'Giacomo Tarroni'}, {'@pid': '20/5519', 'text': 'Wenjia Bai'}, {'@pid': '69/2478', 'text': 'Daniel Rueckert'}]","['Imagesegmentation', 'Adversarialdataaugmentation', 'MR']","Neural network-based approaches can achieve high accuracy in various medical image segmentation tasks. However, they generally require large labelled datasets for supervised learning. Acquiring and manually labelling a large medical dataset is expensive and sometimes impractical due to data sharing and privacy issues. In this work, we propose an adversarial data augmentation method for training neural networks for medical image segmentation. Instead of generating pixel-wise adversarial attacks, our model generates plausible and realistic signal corruptions, which models the intensity inhomogeneities caused by a common type of artefacts in MR imaging: bias field. The proposed method does not rely on generative networks, and can be used as a plug-in module for general segmentation networks in both supervised and semi-supervised learning. Using cardiac MR imaging we show that such an approach can improve the generalization ability and robustness of models as well as provide significant improvements in low-data scenarios.",2020/9/29,2,10.1007/978-3-030-59710-8_65
A Disentangled Latent Space for Cross-Site MRI Harmonization.,"[{'@pid': '180/9476', 'text': 'Blake E. Dewey'}, {'@pid': '239/2014', 'text': 'Lianrui Zuo'}, {'@pid': '40/2041', 'text': 'Aaron Carass'}, {'@pid': '167/4028', 'text': 'Yufan He'}, {'@pid': '200/6534', 'text': 'Yihao Liu'}, {'@pid': '275/8007', 'text': 'Ellen M. Mowry'}, {'@pid': '275/7235', 'text': 'Scott Newsome'}, {'@pid': '138/7879', 'text': 'Jiwon Oh'}, {'@pid': '76/502', 'text': 'Peter A. Calabresi'}, {'@pid': 'p/JerryLPrince', 'text': 'Jerry L. Prince'}]","['MRharmonization', 'Multiplesclerosis', 'Deeplearning']","Accurate interpretation and quantification of magnetic resonance imaging (MRI) is vital to medical research and clinical practice. However, lack of MRI standardization and differences in acquisition protocols often lead to measurement inconsistencies across sites. Image harmonization techniques have been shown to improve qualitative and quantitative consistency between differently acquired scans. Unfortunately, these methods typically require paired training data from traveling subjects (for supervised methods) or assumptions about anatomical similarities between the populations (for unsupervised methods). We propose a deep?learning-based harmonization technique with limited supervision for use in standardization across scanners and sites. By leveraging a disentangled latent space represented by a high-resolution anatomical information component (\(\beta \)) and a low-dimensional contrast component (\(\theta \)), the proposed method trains a cross-site harmonization model using databases of multi-modal image pairs acquired separately from each of the scanners to be harmonized. In this manuscript, we show that by using T\(_1\)-weighted and T\(_2\)-weighted images acquired from different subjects at three different sites, we can achieve a stable extraction of \(\beta \) with a continuous representation of \(\theta \). We also demonstrate that this allows cross-site harmonization without the need for paired data between sites.",2020/9/29,2,10.1007/978-3-030-59728-3_70
Automatic Probe Movement Guidance for Freehand Obstetric Ultrasound.,"[{'@pid': '237/9972', 'text': 'Richard Droste'}, {'@pid': '237/9787', 'text': 'Lior Drukker'}, {'@pid': '39/9774', 'text': 'Aris T. Papageorghiou'}, {'@pid': 'n/JAlisonNoble', 'text': 'J. Alison Noble'}]","['Fetalultrasound', 'Probeguidance', 'Ultrasoundnavigation']","We present the first system that provides real-time probe movement guidance for acquiring standard planes in routine freehand obstetric ultrasound scanning. Such a system can contribute to the worldwide deployment of obstetric ultrasound scanning by lowering the required level of operator expertise. The system employs an artificial neural network that receives the ultrasound video signal and the motion signal of an inertial measurement unit (IMU) that is attached to the probe, and predicts a guidance signal. The network termed US-GuideNet predicts either the movement towards the standard plane position (goal prediction), or the next movement that an expert sonographer would perform (action prediction). While existing models for other ultrasound applications are trained with simulations or phantoms, we train our model with real-world ultrasound video and probe motion data from 464 routine clinical scans by 17 accredited sonographers. Evaluations for 3 standard plane types show that the model provides a useful guidance signal with an accuracy of 88.8% for goal prediction and 90.9% for action prediction.",2020/9/29,2,10.1007/978-3-030-59716-0_56
"Prediction of Pathological Complete Response to Neoadjuvant Chemotherapy in Breast Cancer Using Deep Learning with Integrative Imaging, Molecular and Demographic Data.","[{'@pid': '266/7048', 'text': 'Hongyi Duanmu'}, {'@pid': '275/7021', 'text': 'Pauline Boning Huang'}, {'@pid': '275/6929', 'text': 'Srinidhi Brahmavar'}, {'@pid': '55/11455', 'text': 'Stephanie Lin'}, {'@pid': '275/7053', 'text': 'Thomas Ren'}, {'@pid': '95/203', 'text': 'Jun Kong'}, {'@pid': '69/3405-1', 'text': 'Fusheng Wang 0001'}, {'@pid': '66/11369', 'text': 'Tim Q. Duong'}]","['Artificialintelligence', 'Convolutionalneuralnetwork', 'Magneticresonanceimaging']","Neoadjuvant chemotherapy is widely used to reduce tumor size to make surgical excision manageable and to minimize distant metastasis. Assessing and accurately predicting pathological complete response is important in treatment planing for breast cancer patients. In this study, we propose a novel approach integrating 3D MRI imaging data, molecular data and demographic data using convolutional neural network to predict the likelihood of pathological complete response to neoadjuvant chemotherapy in breast cancer. We take post-contrast T1-weighted 3D MRI images without the need of tumor segmentation, and incorporate molecular subtypes and demographic data. In our predictive model, MRI data and non-imaging data are convolved to inform each other through interactions, instead of a concatenation of multiple data type channels. This is achieved by channel-wise multiplication of the intermediate results of imaging and non-imaging data. We use a subset of curated data from the I-SPY-1 TRIAL of 112 patients with stage 2 or 3 breast cancer with breast tumors underwent standard neoadjuvant chemotherapy. Our method yielded an accuracy of 0.83, AUC of 0.80, sensitivity of 0.68 and specificity of 0.88. Our model significantly outperforms models using imaging data only or traditional concatenation models. Our approach has the potential to aid physicians to identify patients who are likely to respond to neoadjuvant chemotherapy at diagnosis or early treatment, thus facilitate treatment planning, treatment execution, or mid-treatment adjustment.",2020/9/29,2,10.1007/978-3-030-59713-9_24
Deep Attentive Wasserstein Generative Adversarial Networks for MRI Reconstruction with Recurrent Context-Awareness.,"[{'@pid': '47/7790', 'text': 'Yifeng Guo'}, {'@pid': '217/2237', 'text': 'Chengjia Wang'}, {'@pid': '25/1234', 'text': 'Heye Zhang'}, {'@pid': '25/5712-6', 'text': 'Guang Yang 0006'}]","['Recurrentneuralnetwork', 'Wassersteingenerativeadversarialnetworks', 'MRIreconstruction']","The performance of traditional compressive sensing-based MRI (CS-MRI) reconstruction is affected by its slow iterative procedure and noise-induced artefacts. Although many deep learning-based CS-MRI methods have been proposed to mitigate the problems of traditional methods, they have not been able to achieve more robust results at higher acceleration factors. Most of the deep learning-based CS-MRI methods still can not fully mine the information from the k-space, which leads to unsatisfactory results in the MRI reconstruction. In this study, we propose a new deep learning-based CS-MRI reconstruction method to fully utilise the relationship among sequential MRI slices by coupling Wasserstein Generative Adversarial Networks (WGAN) with Recurrent Neural Networks. Further development of an attentive unit enables our model to reconstruct more accurate anatomical structures for the MRI data. By experimenting on different MRI datasets, we have demonstrated that our method can not only achieve better results compared to the state-of-the-arts but can also effectively reduce residual noise generated during the reconstruction process.",2020/9/29,2,10.1007/978-3-030-59713-9_17
Hierarchical Bayesian Regression for Multi-site Normative Modeling of Neuroimaging Data.,"[{'@pid': '134/1827', 'text': 'Seyed Mostafa Kia'}, {'@pid': '266/1743', 'text': 'Hester Huijsdens'}, {'@pid': '266/1587', 'text': 'Richard Dinga'}, {'@pid': '266/1713', 'text': 'Thomas Wolfers'}, {'@pid': '68/11383', 'text': 'Maarten Mennes'}, {'@pid': '125/4395', 'text': 'Ole A. Andreassen'}, {'@pid': '56/11232', 'text': 'Lars T. Westlye'}, {'@pid': '87/5305', 'text': 'Christian F. Beckmann'}, {'@pid': '74/10749', 'text': 'Andre F. Marquand'}]","['Machinelearning', 'Bigdata', 'Precisionpsychiatry']","Clinical neuroimaging has recently witnessed explosive growth in data availability which brings studying heterogeneity in clinical cohorts to the spotlight. Normative modeling is an emerging statistical tool for achieving this objective. However, its application remains technically challenging due to difficulties in properly dealing with nuisance variation, for example due to variability in image acquisition devices. Here, in a fully probabilistic framework, we propose an application of hierarchical Bayesian regression (HBR) for multi-site normative modeling. Our experimental results confirm the superiority of HBR in deriving more accurate normative ranges on large multi-site neuroimaging data compared to widely used methods. This provides the possibility i) to learn the normative range of structural and functional brain measures on large multi-site data; ii) to recalibrate and reuse the learned model on local small data; therefore, HBR closes the technical loop for applying normative modeling as a medical tool for the diagnosis and prognosis of mental disorders.",2020/9/29,2,10.1007/978-3-030-59728-3_68
"DISCo - Deep Learning, Instance Segmentation, and Correlations for Cell Segmentation in Calcium Imaging.","[{'@pid': '209/4854', 'text': 'Elke Kirschbaum'}, {'@pid': '227/6453', 'text': 'Alberto Bailoni'}, {'@pid': '18/4529', 'text': 'Fred A. Hamprecht'}]","['Calciumimaging', 'Cellsegmentation', 'Neuroimaginganalysis']","Calcium imaging is one of the most important tools in neurophysiology as it enables the observation of neuronal activity for hundreds of cells in parallel and at single-cell resolution. In order to use the data gained with calcium imaging, it is necessary to extract individual cells and their activity from the recordings. We present DISCo, a novel approach for the cell segmentation in calcium imaging videos. We use temporal information from the recordings in a computationally efficient way by computing correlations between pixels and combine it with shape-based information to identify active as well as non-active cells. We first learn to predict whether two pixels belong to the same cell; this information is summarized in an undirected, edge-weighted graph which we then partition. Evaluating our method on the Neurofinder public benchmark shows that DISCo outperforms all existing models trained on these datasets.",2020/9/29,2,10.1007/978-3-030-59722-1_15
Weakly Supervised Multiple Instance Learning Histopathological Tumor Segmentation.,"[{'@pid': '239/9835', 'text': 'Marvin Lerousseau'}, {'@pid': '169/9108', 'text': 'Maria Vakalopoulou'}, {'@pid': '262/7465', 'text': 'Marion Classe'}, {'@pid': '188/1244', 'text': 'Julien Adam'}, {'@pid': '216/8496', 'text': 'Enzo Battistella'}, {'@pid': '250/3858', 'text': 'Alexandre Carr¨¦'}, {'@pid': '235/3498', 'text': 'Th¨¦o Estienne'}, {'@pid': '262/7676', 'text': 'Th¨¦ophraste Henry'}, {'@pid': '235/3472', 'text': 'Eric Deutsch'}, {'@pid': 'p/NikosParagios', 'text': 'Nikos Paragios'}]","['Weaklysupervisedlearning', 'Histopathologicalsegmentation', 'Multipleinstancelearning', 'Tumorsegmentation']","Histopathological image segmentation is a challenging and important topic in medical imaging with tremendous potential impact in clinical practice. State of the art methods rely on hand-crafted annotations which hinder clinical translation since histology suffers from significant variations between cancer phenotypes. In this paper, we propose a weakly supervised framework for whole slide imaging segmentation that relies on standard clinical annotations, available in most medical systems. In particular, we exploit a multiple instance learning scheme for training models. The proposed framework has been evaluated on multi-locations and multi-centric public data from The Cancer Genome Atlas and the PatchCamelyon dataset. Promising results when compared with experts¡¯ annotations demonstrate the potentials of the presented approach. The complete framework, including 6481 generated tumor maps and data processing, is available at https://github.com/marvinler/tcga_segmentation.",2020/9/29,2,10.1007/978-3-030-59722-1_45
Self-Loop Uncertainty - A Novel Pseudo-Label for Semi-supervised Medical Image Segmentation.,"[{'@pid': '165/6204', 'text': 'Yuexiang Li'}, {'@pid': '03/1390', 'text': 'Jiawei Chen'}, {'@pid': '215/8397', 'text': 'Xinpeng Xie'}, {'@pid': '86/7113-2', 'text': 'Kai Ma 0002'}, {'@pid': '44/6510', 'text': 'Yefeng Zheng'}]","['Semi-supervisedlearning', 'Pseudo-label', 'Jigsawpuzzles']","Witnessing the success of deep learning neural networks in natural image processing, an increasing number of studies have been proposed to develop deep-learning-based frameworks for medical image segmentation. However, since the pixel-wise annotation of medical images is laborious and expensive, the amount of annotated data is usually deficient to well-train a neural network. In this paper, we propose a semi-supervised approach to train neural networks with limited labeled data and a large quantity of unlabeled images for medical image segmentation. A novel pseudo-label (namely self-loop uncertainty), generated by recurrently optimizing the neural network with a self-supervised task, is adopted as the ground-truth for the unlabeled images to augment the training set and boost the segmentation accuracy. The proposed self-loop uncertainty can be seen as an approximation of the uncertainty estimation yielded by ensembling multiple models with a significant reduction of inference time. Experimental results on two publicly available datasets demonstrate the effectiveness of our semi-supervised approach.",2020/9/29,2,10.1007/978-3-030-59710-8_60
Joint Image Quality Assessment and Brain Extraction of Fetal MRI Using Deep Learning.,"[{'@pid': '230/4255', 'text': 'Lufan Liao'}, {'@pid': '76/1584-13', 'text': 'Xin Zhang 0013'}, {'@pid': '236/1791', 'text': 'Fenqiang Zhao'}, {'@pid': '24/6538', 'text': 'Tao Zhong'}, {'@pid': '207/0651', 'text': 'Yuchen Pei'}, {'@pid': '28/9939', 'text': 'Xiangmin Xu'}, {'@pid': '58/6810-26', 'text': 'Li Wang 0026'}, {'@pid': '24/2058', 'text': 'He Zhang'}, {'@pid': '14/4383', 'text': 'Dinggang Shen'}, {'@pid': '62/2655-1', 'text': 'Gang Li 0001'}]","['FetalMRI', 'Qualityassessment', 'Brainextraction']","Quality assessment (QA) and brain extraction (BE) are two fundamental steps in 3D fetal brain MRI reconstruction and quantification. Conventionally, QA and BE are performed independently, ignoring the inherent relation of the two closely-related tasks. However, both of them focus on the brain region representation, so they can be jointly optimized to ensure the network to learn shared features and avoid overfitting. To this end, we propose a novel multi-stage deep learning model for joint QA and BE of fetal MRI. The locations and orientations of fetal brains are randomly variable, and the shapes and appearances of fetal brains change remarkably across gestational ages, thus imposing great challenges to extract shared features of QA and BE. To address these problems, we firstly design a brain detector to locate the brain region. Then we introduce the deformable convolution to adaptively adjust the receptive field for dealing with variable brain shapes. Finally, a task-specific module is used for image QA and BE simultaneously. To obtain a well-trained model, we further propose a multi-step training strategy. We cross validate our method on two independent fetal MRI datasets acquired from different scanners with different imaging protocols, and achieve promising performance.",2020/9/29,2,10.1007/978-3-030-59725-2_40
Unsupervised Surgical Instrument Segmentation via Anchor Generation and Semantic Diffusion.,"[{'@pid': '222/2701', 'text': 'Daochang Liu'}, {'@pid': '244/8599', 'text': 'Yuhui Wei'}, {'@pid': '72/2833', 'text': 'Tingting Jiang'}, {'@pid': '71/3387-1', 'text': 'Yizhou Wang 0001'}, {'@pid': '250/4145', 'text': 'Rulin Miao'}, {'@pid': '194/8543', 'text': 'Fei Shan'}, {'@pid': '177/3057', 'text': 'Ziyu Li'}]","['Surgicalinstrumentsegmentation', 'Unsupervisedlearning', 'Semanticdiffusion']","Surgical instrument segmentation is a key component in developing context-aware operating rooms. Existing works on this task heavily rely on the supervision of a large amount of labeled data, which involve laborious and expensive human efforts. In contrast, a more affordable unsupervised approach is developed in this paper. To train our model, we first generate anchors as pseudo labels for instruments and background tissues respectively by fusing coarse handcrafted cues. Then a semantic diffusion loss is proposed to resolve the ambiguity in the generated anchors via the feature correlation between adjacent video frames. In the experiments on the binary instrument segmentation task of the 2017 MICCAI EndoVis Robotic Instrument Segmentation Challenge dataset, the proposed method achieves 0.71 IoU and 0.81 Dice score without using a single manual annotation, which is promising to show the potential of unsupervised learning for surgical tool segmentation.",2020/9/29,2,10.1007/978-3-030-59716-0_63
Encoding Metal Mask Projection for Metal Artifact Reduction in Computed Tomography.,"[{'@pid': '171/7711', 'text': 'Yuanyuan Lyu'}, {'@pid': '123/7222', 'text': 'Wei-An Lin'}, {'@pid': '188/5698', 'text': 'Haofu Liao'}, {'@pid': '80/199', 'text': 'Jingjing Lu'}, {'@pid': '57/98', 'text': 'S. Kevin Zhou'}]","['Artifactreduction', 'Sinograminpainting', 'Imageenhancement']","Metal artifact reduction (MAR) in computed tomography (CT) is a notoriously challenging task because the artifacts are structured and non-local in the image domain. However, they are inherently local in the sinogram domain. Thus, one possible approach to MAR is to exploit the latter characteristic by learning to reduce artifacts in the sinogram. However, if we directly treat the metal-affected regions in sinogram as missing and replace them with the surrogate data generated by a neural network, the artifact-reduced CT images tend to be over-smoothed and distorted since fine-grained details within the metal-affected regions are completely ignored. In this work, we provide analytical investigation to the issue and propose to address the problem by (1) retaining the metal-affected regions in sinogram and (2) replacing the binarized metal trace with the metal mask projection such that the geometry information of metal implants is encoded. Extensive experiments on simulated datasets and expert evaluations on clinical images demonstrate that our novel network yields anatomically more precise artifact-reduced images than the state-of-the-art approaches, especially when metallic objects are large.
",2020/9/29,2,10.1007/978-3-030-59713-9_15
Are Fast Labeling Methods Reliable? A Case Study of Computer-Aided Expert Annotations on Microscopy Slides.,"[{'@pid': '125/6134', 'text': 'Christian Marzahl'}, {'@pid': '203/9018', 'text': 'Christof A. Bertram'}, {'@pid': '166/6549', 'text': 'Marc Aubreville'}, {'@pid': '262/6526', 'text': 'Anne Petrick'}, {'@pid': '262/6107', 'text': 'Kristina Weiler'}, {'@pid': '262/6396', 'text': 'Agnes C. Gl?sel'}, {'@pid': '236/4855', 'text': 'Marco Fragoso'}, {'@pid': '236/4943', 'text': 'Sophie Merz'}, {'@pid': '236/5072', 'text': 'Florian Bartenschlager'}, {'@pid': '262/6525', 'text': 'Judith Hoppe'}, {'@pid': '262/6568', 'text': 'Alina Langenhagen'}, {'@pid': '247/0882', 'text': 'Anne-Katherine Jasensky'}, {'@pid': '235/2628', 'text': 'J?rn Voigt'}, {'@pid': '203/8433', 'text': 'Robert Klopfleisch'}, {'@pid': '131/7133', 'text': 'Andreas Maier 0001'}]","['Expert-algorithmcollaboration', 'Computer-aidedlabelling', 'Microscopy', 'Pathology']","Deep-learning-based pipelines have shown the potential to revolutionalize microscopy image diagnostics by providing visual augmentations and evaluations to a trained pathology expert. However, to match human performance, the methods rely on the availability of vast amounts of high-quality labeled data, which poses a significant challenge. To circumvent this, augmented labeling methods, also known as expert-algorithm-collaboration, have recently become popular. However, potential biases introduced by this operation mode and their effects for training deep neuronal networks are not entirely understood. This work aims to shed light on some of the effects by providing a case study for three pathologically relevant diagnostic settings. Ten trained pathology experts performed a labeling tasks first without and later with computer-generated augmentation. To investigate different biasing effects, we intentionally introduced errors to the augmentation. In total, the pathology experts annotated 26,015 cells on 1,200 images in this novel annotation study. Backed by this extensive data set, we found that the concordance of multiple experts was significantly increased in the computer-aided setting, versus the unaided annotation. However, a significant percentage of the deliberately introduced false labels was not identified by the experts.",2020/9/29,2,10.1007/978-3-030-59710-8_3
CNN-GCN Aggregation Enabled Boundary Regression for Biomedical Image Segmentation.,"[{'@pid': '275/6822', 'text': 'Yanda Meng'}, {'@pid': '53/9868', 'text': 'Meng Wei'}, {'@pid': '150/4082', 'text': 'Dongxu Gao'}, {'@pid': '17/9876', 'text': 'Yitian Zhao'}, {'@pid': '54/230', 'text': 'Xiaoyun Yang'}, {'@pid': '60/5414-1', 'text': 'Xiaowei Huang 0001'}, {'@pid': '15/952', 'text': 'Yalin Zheng'}]","['Regression', 'Segmentation', 'GCN', 'Attention', 'Aggregation']","Accurate segmentation of anatomic structure is an essential task for biomedical image analysis. Recent popular object contours regression based segmentation methods have increasingly attained researchers¡¯ attentions. They made a new starting point to tackle segmentation tasks instead of commonly used dense pixels classification methods. However, because of the nature of CNN based network (lack of spatial information) and the difficulty of this methodology itself (need of more spatial information), these methods needed extra process to maintain more spatial features, which may cause longer inference time or tedious design and inference process. To address the issue, this paper proposes a simple, intuitive deep learning based contour regression model. We develop a novel multi-level, multi-stage aggregated network to regress the coordinates of the contour of instances directly in an end-to-end manner. The proposed network seamlessly links convolution neural network (CNN) with Attention Refinement module (AR) and Graph Convolution Network (GCN). By hierarchically and iteratively combining features over different layers of the CNN, the proposed model obtains sufficient low-level features and high-level semantic information from the input image. Besides, our model pays distinct attention to the objects¡¯ contours with the help of AR and GCN. Primarily, thanks to the proposed aggregated GCN and vertices sampling method, our model benefits from direct feature learning of the objects¡¯ contour locations from sparse to dense and the spatial information propagation across the whole input image. Experiments on the segmentation of fetal head (FH) in ultrasound images and of the optic disc (OD) and optic cup (OC) in color fundus images demonstrate that our method outperforms state-of-the-art methods in terms of effectiveness and efficiency.",2020/9/29,2,10.1007/978-3-030-59719-1_35
DECAPS - Detail-Oriented Capsule Networks.,"[{'@pid': '207/7586', 'text': 'Aryan Mobiny'}, {'@pid': '263/4078', 'text': 'Pengyu Yuan'}, {'@pid': '263/4153', 'text': 'Pietro Antonio Cicalese'}, {'@pid': '59/9550', 'text': 'Hien Van Nguyen'}]","['Capsulenetwork', 'Chestradiography', 'Pneumonia']","Capsule Networks (CapsNets) have demonstrated to be a promising alternative to Convolutional Neural Networks (CNNs). However, they often fall short of state-of-the-art accuracies on large-scale high-dimensional datasets. We propose a Detail-Oriented Capsule Network (DECAPS) that combines the strength of CapsNets with several novel techniques to boost its classification accuracies. First, DECAPS uses an Inverted Dynamic Routing (IDR) mechanism to group lower-level capsules into heads before sending them to higher-level capsules. This strategy enables capsules to selectively attend to small but informative details within the data which may be lost during pooling operations in CNNs. Second, DECAPS employs a Peekaboo training procedure, which encourages the network to focus on fine-grained information through a second-level attention scheme. Finally, the distillation process improves the robustness of DECAPS by averaging over the original and attended image region predictions. We provide extensive experiments on the CheXpert and RSNA Pneumonia datasets to validate the effectiveness of DECAPS. Our networks achieve state-of-the-art accuracies not only in classification (increasing the average area under ROC curves from 87.24% to 92.82% on the CheXpert dataset) but also in the weakly-supervised localization of diseased areas (increasing average precision from 41.7% to 80% for the RSNA Pneumonia detection dataset).
",2020/9/29,2,10.1007/978-3-030-59710-8_15
Recognition of Instrument-Tissue Interactions in Endoscopic Videos via Action Triplets.,"[{'@pid': '175/8669', 'text': 'Chinedu Innocent Nwoye'}, {'@pid': '270/3649', 'text': 'Cristians Gonzalez'}, {'@pid': '32/1593-9', 'text': 'Tong Yu 0009'}, {'@pid': '270/2969', 'text': 'Pietro Mascagni'}, {'@pid': '43/1475', 'text': 'Didier Mutter'}, {'@pid': '18/1992', 'text': 'Jacques Marescaux'}, {'@pid': '60/6834', 'text': 'Nicolas Padoy'}]","['Surgicalactivityrecognition', 'Actiontriplet', 'Tool-tissueinteraction', 'Deeplearning', 'Endoscopicvideo', 'CholecT40']","Recognition of surgical activity is an essential component to develop context-aware decision support for the operating room. In this work, we tackle the recognition of fine-grained activities, modeled as action triplets \(\langle instrument, verb, target \rangle \) representing the tool activity. To this end, we introduce a new laparoscopic dataset, CholecT40, consisting of 40 videos from the public dataset Cholec80 in which all frames have been annotated using 128 triplet classes. Furthermore, we present an approach to recognize these triplets directly from the video data. It relies on a module called class activation guide, which uses the instrument activation maps to guide the verb and target recognition. To model the recognition of multiple triplets in the same frame, we also propose a trainable 3D interaction space, which captures the associations between the triplet components. Finally, we demonstrate the significance of these contributions via several ablation studies and comparisons to baselines on CholecT40.",2020/9/29,2,10.1007/978-3-030-59716-0_35
Searching for Efficient Architecture for Instrument Segmentation in Robotic Surgery.,"[{'@pid': '198/1260', 'text': 'Daniil Pakhomov'}, {'@pid': 'n/NassirNavab', 'text': 'Nassir Navab'}]",[],"Segmentation of surgical instruments is an important problem in robot-assisted surgery: it is a crucial step towards full instrument pose estimation and is directly used for masking of augmented reality overlays during surgical procedures. Most applications rely on accurate real-time segmentation of high-resolution surgical images. While previous research focused primarily on methods that deliver high accuracy segmentation masks, majority of them can not be used for real-time applications due to their computational cost. In this work, we design a light-weight and highly-efficient deep residual architecture which is tuned to perform real-time inference of high-resolution images. To account for reduced accuracy of the discovered light-weight deep residual network and avoid adding any additional computational burden, we perform a differentiable search over dilation rates for residual units of our network. We test our discovered architecture on the EndoVis 2017 Robotic Instruments dataset and verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff with a speed of up?to 125 FPS on high resolution images.",2020/9/29,2,10.1007/978-3-030-59716-0_62
Simulation of Brain Resection for Cavity Segmentation Using Self-supervised and Semi-supervised Learning.,"[{'@pid': '260/6785', 'text': 'Fernando P¨¦rez-Garc¨ªa'}, {'@pid': '93/11388', 'text': 'Roman Rodionov'}, {'@pid': '268/6795', 'text': 'Ali Alim-Marvasti'}, {'@pid': '09/8486', 'text': 'Rachel Sparks'}, {'@pid': '33/6054', 'text': 'John S. Duncan'}, {'@pid': '40/2838', 'text': 'S¨¦bastien Ourselin'}]","['Neurosurgery', 'Segmentation', 'Self-supervisedlearning']","Resective surgery may be curative for drug-resistant focal epilepsy, but only 40% to 70% of patients achieve seizure freedom after surgery. Retrospective quantitative analysis could elucidate patterns in resected structures and patient outcomes to improve resective surgery. However, the resection cavity must first be segmented on the postoperative MR image. Convolutional neural networks (CNNs) are the state-of-the-art image segmentation technique, but require large amounts of annotated data for training. Annotation of medical images is a time-consuming process requiring highly-trained raters, and often suffering from high inter-rater variability. Self-supervised learning can be used to generate training instances from unlabeled data. We developed an algorithm to simulate resections on preoperative MR images. We curated a new dataset, EPISURG, comprising 431 postoperative and 269 preoperative MR images from 431 patients who underwent resective surgery. In addition to EPISURG, we used three public datasets comprising 1813 preoperative MR images for training. We trained a 3D CNN on artificially resected images created on the fly during training, using images from 1) EPISURG, 2) public datasets and 3) both. To evaluate trained models, we calculate Dice score (DSC) between model segmentations and 200 manual annotations performed by three human raters. The model trained on data with manual annotations obtained a median (interquartile range) DSC of 65.3 (30.6). The DSC of our best-performing model, trained with no manual annotations, is 81.7 (14.2). For comparison, inter-rater agreement between human annotators was 84.0 (9.9). We demonstrate a training method for CNNs using simulated resection cavities that can accurately segment real resection cavities, without manual annotations.",2020/9/29,2,10.1007/978-3-030-59716-0_12
Non-Rigid Volume to Surface Registration Using a Data-Driven Biomechanical Model.,"[{'@pid': '192/4799', 'text': 'Micha Pfeiffer'}, {'@pid': '217/5128', 'text': 'Carina Riediger'}, {'@pid': '192/1917', 'text': 'Stefan Leger'}, {'@pid': '40/9596', 'text': 'Jens-Peter K¨¹hn'}, {'@pid': '266/3073', 'text': 'Danilo Seppelt'}, {'@pid': '54/3451', 'text': 'Ralf-Thorsten Hoffmann'}, {'@pid': '217/5026', 'text': 'J¨¹rgen Weitz'}, {'@pid': '39/6736', 'text': 'Stefanie Speidel'}]","['Liverregistration', 'Soft-tissue', 'Surgicalnavigation', 'CNN']"," Non-rigid registration is a key component in soft-tissue navigation. We focus on laparoscopic liver surgery, where we register the organ model obtained from a preoperative CT scan to the intraoperative partial organ surface, reconstructed from the laparoscopic video. This is a challenging task due to sparse and noisy intraoperative data, real-time requirements and many unknowns - such as tissue properties and boundary conditions. Furthermore, establishing correspondences between pre- and intraoperative data can be extremely difficult since the liver usually lacks distinct surface features and the used imaging modalities suffer from very different types of noise. In this work, we train a convolutional neural network to perform both the search for surface correspondences as well as the non-rigid registration in one step. The network is trained on physically accurate biomechanical simulations of randomly generated, deforming organ-like structures. This enables the network to immediately generalize to a new patient organ without the need to re-train. We add various amounts of noise to the intraoperative surfaces during training, making the network robust to noisy intraoperative data. During inference, the network outputs the displacement field which matches the preoperative volume to the partial intraoperative surface. In multiple experiments, we show that the network translates well to real data while maintaining a high inference speed. Our code is made available online.",2020/9/29,2,10.1007/978-3-030-59719-1_70
Simultaneous Estimation of X-Ray Back-Scatter and Forward-Scatter Using Multi-task Learning.,"[{'@pid': '235/2478', 'text': 'Philipp Roser'}, {'@pid': '142/2265', 'text': 'Xia Zhong'}, {'@pid': '216/2210', 'text': 'Annette Birkhold'}, {'@pid': '173/2255', 'text': 'Alexander Preuhs'}, {'@pid': '197/5011', 'text': 'Christopher Syben'}, {'@pid': '205/6637', 'text': 'Elisabeth Hoppe'}, {'@pid': '42/3080', 'text': 'Norbert Strobel'}, {'@pid': '17/5276', 'text': 'Markus Kowarschik'}, {'@pid': '79/3714', 'text': 'Rebecca Fahrig'}, {'@pid': '131/7133', 'text': 'Andreas K. Maier'}]","['X-rayscatter', 'Skindose', 'Multi-tasklearning']","Scattered radiation is a major concern impacting X-ray image-guided procedures in two ways. First, back-scatter significantly contributes to patient (skin) dose during complicated interventions. Second, forward-scattered radiation reduces contrast in projection images and introduces artifacts in 3-D reconstructions. While conventionally employed anti-scatter grids improve image quality by blocking X-rays, the additional attenuation due to the anti-scatter grid at the detector needs to be compensated for by a higher patient entrance dose. This also increases the room dose affecting the staff caring for the patient. For skin dose quantification, back-scatter is usually accounted for by applying pre-determined scalar back-scatter factors or linear point spread functions to a primary kerma forward projection onto a patient surface point. However, as patients come in different shapes, the generalization of conventional methods is limited. Here, we propose a novel approach combining conventional techniques with learning-based methods to simultaneously estimate the forward-scatter reaching the detector as well as the back-scatter affecting the patient skin dose. Knowing the forward-scatter, we can correct X-ray projections, while a good estimate of the back-scatter component facilitates an improved skin dose assessment. To simultaneously estimate forward-scatter as well as back-scatter, we propose a multi-task approach for joint back- and forward-scatter estimation by combining X-ray physics with neural networks. We show that, in theory, highly accurate scatter estimation in both cases is possible. In addition, we identify research directions for our multi-task framework and learning-based scatter estimation in general.
",2020/9/29,2,10.1007/978-3-030-59713-9_20
Endo-Sim2Real - Consistency Learning-Based Domain Adaptation for Instrument Segmentation.,"[{'@pid': '188/6278', 'text': 'Manish Sahu'}, {'@pid': '271/0881', 'text': 'Ronja Str?msd?rfer'}, {'@pid': '64/1706-3', 'text': 'Anirban Mukhopadhyay 0003'}, {'@pid': '42/294', 'text': 'Stefan Zachow'}]","['Endoscopicinstrumentsegmentation', 'Unsuperviseddomainadaptation', 'Self-supervisedlearning', 'Consistencylearning']","Surgical tool segmentation in endoscopic videos is an important component of computer assisted interventions systems. Recent success of image-based solutions using fully-supervised deep learning approaches can be attributed to the collection of big labeled datasets. However, the annotation of a big dataset of real videos can be prohibitively expensive and time consuming. Computer simulations could alleviate the manual labeling problem, however, models trained on simulated data do not generalize to real data. This work proposes a consistency-based framework for joint learning of simulated and real (unlabeled) endoscopic data to bridge this performance generalization issue. Empirical results on two data sets (15 videos of the Cholec80 and EndoVis¡¯15 dataset) highlight the effectiveness of the proposed Endo-Sim2Real method for instrument segmentation. We compare the segmentation of the proposed approach with state-of-the-art solutions and show that our method improves segmentation both in terms of quality and quantity.",2020/9/29,2,10.1007/978-3-030-59716-0_75
A Deformable CRF Model for Histopathology Whole-Slide Image Classification.,"[{'@pid': '76/9743-3', 'text': 'Yiqing Shen 0003'}, {'@pid': '192/3852', 'text': 'Jing Ke'}]","['Deformableconditionalrandomfield', 'Auto-labelling', 'Spatialcorrelation', 'Whole-slideimage']","To detect abnormality from histopathology images in a patch-based convolutional neural network (CNN), spatial context is an important cue. However, whole-slide image (WSI) is characterized by high morphological heterogeneity in the shape and scale of tissues, hence a simple visual span to a larger context may not well capture the information associated with the central patch or disease of interest. In this paper, we propose a Deformable Conditional Random Field (DCRF) model to learn the offsets and weights of neighboring patches in a spatial-adaptive manner. Additionally, rather than using regularly tessellated or overlapped patches, we localize patches with more powerful feature representations by the adaptively adjusted offsets in a WSI. Both the employment of DCRF for better feature extraction from spatial sampling patches, as well as utilization of the auto-generated patches as training input, can achieve performance improvement in the target task. This model is feasible to the widespread annotation strategies in histopathology images, either with a contoured region of interest (ROI) or patch-wise multi-tissue labels. The proposed model is validated on the patient cohorts from The Cancer Genome Atlas (TCGA) dataset and the Camelyon16 dataset for performance evaluation. The experimental results demonstrate the advantage of the proposed model in the classification task, by the comparison against the baseline models.",2020/9/29,2,10.1007/978-3-030-59722-1_48
Deep Active Learning for Breast Cancer Segmentation on Immunohistochemistry Images.,"[{'@pid': '160/3613', 'text': 'Haocheng Shen'}, {'@pid': '176/2936', 'text': 'Kuan Tian'}, {'@pid': '83/8502', 'text': 'Pei Dong'}, {'@pid': '29/4190', 'text': 'Jun Zhang'}, {'@pid': '226/7785', 'text': 'Kezhou Yan'}, {'@pid': '275/6894', 'text': 'Shannon Che'}, {'@pid': '02/1079', 'text': 'Jianhua Yao'}, {'@pid': '250/5974', 'text': 'Pifu Luo'}, {'@pid': '01/2095', 'text': 'Xiao Han'}]","['IHCimages', 'Activelearning', 'Segmentation']","Immunohistochemistry (IHC) plays an essential role in breast cancer diagnosis and treatment. Reliable and automatic segmentation of breast cancer regions on IHC images would be of considerable value for further analysis. However, the prevalent fully convolutional networks (FCNs) suffer from difficulties in obtaining sufficient annotated training data. Active learning, on the other hand, aims to reduce the cost of annotation by selecting an informative and effective subset for labeling. In this paper, we present a novel deep active learning framework for breast cancer segmentation on IHC images. Three criteria are explicitly designed to select training samples: dissatisfaction, representativeness and diverseness. Dissatisfaction, consisting of both pixel-level and image-level dissatisfaction, focuses on selecting samples that the network does not segment well. Representativeness chooses samples that can mostly represent all the other unlabeled samples and diverseness further makes the chosen samples different from those already in the training set. We evaluate the proposed method on a large-scale in-house breast cancer IHC dataset and demonstrate that our method outperforms the state-of-the-art suggestive annotation (SA)
[1] and representative annotation (RA)
[5] on two test sets and achieves competitive or even superior performance using 40% of training data to using the full set of training data.",2020/9/29,2,10.1007/978-3-030-59722-1_49
One Click Lesion RECIST Measurement and Segmentation on CT Scans.,"[{'@pid': '20/8578', 'text': 'Youbao Tang'}, {'@pid': '28/7692-6', 'text': 'Ke Yan 0006'}, {'@pid': '67/4008-6', 'text': 'Jing Xiao 0006'}, {'@pid': '25/2710', 'text': 'Ronald M. Summers'}]","['LesionRECISTestimation', 'Lesionsegmentation', 'One-clickhumanguidance', 'CTscans']","In clinical trials, one of the radiologists¡¯ routine work is to measure tumor sizes on medical images using the RECIST criteria (Response Evaluation Criteria In Solid Tumors). However, manual measurement is tedious and subject to inter-observer variability. We propose a unified framework named SEENet for semi-automatic lesion SEgmentation and RECIST Estimation on a variety of lesions over the entire human body. The user is only required to provide simple guidance by clicking once near the lesion. SEENet consists of two main parts. The first one extracts the lesion of interest with the one-click guidance, roughly segments the lesion, and estimates its RECIST measurement. Based on the results of the first network, the second one refines the lesion segmentation and RECIST estimation. SEENet achieves state-of-the-art performance in lesion segmentation and RECIST estimation on the large-scale public DeepLesion dataset. It offers a practical tool for radiologists to generate reliable lesion measurements (i.e. segmentation mask and RECIST) with minimal human effort and greatly reduced time.",2020/9/29,2,10.1007/978-3-030-59719-1_56
Can You Trust Predictive Uncertainty Under Real Dataset Shifts in Digital Pathology?,"[{'@pid': '235/2950', 'text': 'Jeppe Thagaard'}, {'@pid': '39/7226', 'text': 'S?ren Hauberg'}, {'@pid': '275/6808', 'text': 'Bert van der Vegt'}, {'@pid': '178/3710', 'text': 'Thomas Ebstrup'}, {'@pid': '275/6933', 'text': 'Johan D. Hansen'}, {'@pid': '00/5632', 'text': 'Anders B. Dahl'}]","['Deeplearning', 'Digitalpathology', 'Predictiveuncertainty']","Deep learning-based algorithms have shown great promise for assisting pathologists in detecting lymph node metastases when evaluated based on their predictive accuracy. However, for clinical adoption, we need to know what happens when the test set dramatically changes from the training distribution. In such settings, we should estimate the uncertainty of the predictions, so we know when to trust the model (and when not to). Here, we i) investigate current popular methods for improving the calibration of predictive uncertainty, and ii) compare the performance and calibration of the methods under clinically relevant in-distribution dataset shifts. Furthermore, we iii) evaluate their performance on the task of out-of-distribution detection of a different histological cancer type not seen during training. Of the investigated methods, we show that deep ensembles are more robust in respect of both performance and calibration for in-distribution dataset shifts and allows us to better detect incorrect predictions. Our results also demonstrate that current methods for uncertainty quantification are not necessarily able to detect all dataset shifts, and we emphasize the importance of monitoring and controlling the input distribution when deploying deep learning for digital pathology.",2020/9/29,2,10.1007/978-3-030-59710-8_80
KiU-Net - Towards Accurate Segmentation of Biomedical Images Using Over-Complete Representations.,"[{'@pid': '275/7027', 'text': 'Jeya Maria Jose Valanarasu'}, {'@pid': '165/1783', 'text': 'Vishwanath A. Sindagi'}, {'@pid': '81/5273', 'text': 'Ilker Hacihaliloglu'}, {'@pid': '76/6100', 'text': 'Vishal M. Patel'}]","['Over-completerepresentations', 'Ultrasound', 'Brain', 'Deeplearning', 'Segmentation', 'Pretermneonate']","Due to its excellent performance, U-Net is the most widely used backbone architecture for biomedical image segmentation in the recent years. However, in our studies, we observe that there is a considerable performance drop in the case of detecting smaller anatomical structures with blurred noisy boundaries. We analyze this issue in detail, and address it by proposing an over-complete architecture (Ki-Net) which involves projecting the data onto higher dimensions (in the spatial sense). This network, when augmented with U-Net, results in significant improvements in the case of segmenting small anatomical landmarks and blurred noisy boundaries while obtaining better overall performance. Furthermore, the proposed network has additional benefits like faster convergence and fewer number of parameters. We evaluate the proposed method on the task of brain anatomy segmentation from 2D Ultrasound (US) of preterm neonates, and achieve an improvement of around \(4\%\) in terms of the DICE accuracy and Jaccard index as compared to the standard-U-Net, while outperforming the recent best methods by \(2\%\). Code: https://github.com/jeya-maria-jose/KiU-Net-pytorch",2020/9/29,2,10.1007/978-3-030-59719-1_36
Test-Time Unsupervised Domain Adaptation.,"[{'@pid': '223/5635', 'text': 'Thomas Varsavsky'}, {'@pid': '155/3180', 'text': 'Mauricio Orbes-Arteaga'}, {'@pid': '150/2899', 'text': 'Carole H. Sudre'}, {'@pid': '164/0687', 'text': 'Mark S. Graham'}, {'@pid': '40/11373', 'text': 'Parashkev Nachev'}, {'@pid': '17/7426', 'text': 'M. Jorge Cardoso'}]","['Domainadaptation', 'One-shot', 'BrainMRI']","Convolutional neural networks trained on publicly available medical imaging datasets (source domain) rarely generalise to different scanners or acquisition protocols (target domain). This motivates the active field of domain adaptation. While some approaches to the problem require labelled data from the target domain, others adopt an unsupervised approach to domain adaptation (UDA). Evaluating UDA methods consists of measuring the model¡¯s ability to generalise to unseen data in the target domain. In this work, we argue that this is not as useful as adapting to the test set directly. We therefore propose an evaluation framework where we perform test-time UDA on each subject separately. We show that models adapted to a specific target subject from the target domain outperform a domain adaptation method which has seen more data of the target domain but not this specific target subject. This result supports the thesis that unsupervised domain adaptation should be used at test-time, even if only using a single target-domain subject.",2020/9/29,2,10.1007/978-3-030-59710-8_42
Automatic Plane Adjustment of Orthopedic Intraoperative Flat Panel Detector CT-Volumes.,"[{'@pid': '275/6744', 'text': 'Celia Mart¨ªn Vicario'}, {'@pid': '235/2584', 'text': 'Florian Kordon'}, {'@pid': '250/4005', 'text': 'Felix Denzinger'}, {'@pid': '52/2930', 'text': 'Markus Weiten'}, {'@pid': '197/4990', 'text': 'Sarina Thomas'}, {'@pid': '182/1176', 'text': 'Lisa Kausch'}, {'@pid': '55/2384', 'text': 'Jochen Franke'}, {'@pid': '177/7986', 'text': 'Holger Keil'}, {'@pid': '131/7133', 'text': 'Andreas Maier 0001'}, {'@pid': '23/22', 'text': 'Holger Kunze'}]","['Orthopedics', 'FlatplanelCT', 'Multiplanarreconstruction']","Flat panel computed tomography is used intraoperatively to assess the result of surgery. Due to workflow issues, the acquisition typically cannot be carried out in such a way that the axis aligned multiplanar reconstructions (MPR) of the volume match the anatomically aligned MPRs. This needs to be performed manually, adding additional effort during viewing the datasets. A PoseNet convolutional neural network (CNN) is trained such that parameters of anatomically aligned MPR planes are regressed. Different mathematical approaches to describe plane rotation are compared, as well as a cost function is optimized to incorporate orientation constraints. The CNN is evaluated on two anatomical regions. For one of these regions, one plane is not orthogonal to the other two planes. The plane¡¯s normal can be estimated with a median accuracy of 5¡ã, the in-plane rotation with an accuracy of 6¡ã, and the position with an accuracy of 6?mm. Compared to state-of-the-art algorithms the labeling effort for this method is much lower as no segmentation is required. The computation time during inference is less than 0.05?s.",2020/9/29,2,10.1007/978-3-030-59713-9_47
Towards Accurate and Interpretable Surgical Skill Assessment - A Video-Based Method Incorporating Recognized Surgical Gestures and Skill Levels.,"[{'@pid': '35/8397-7', 'text': 'Tianyu Wang 0007'}, {'@pid': '91/1726', 'text': 'Yijie Wang'}, {'@pid': '30/3672', 'text': 'Mian Li'}]","['Surgicalskillassessment', 'Incorporatingrecognizedsurgicalgesturesandskilllevels', 'Interpretablefeedback']","Nowadays, surgical skill assessment becomes increasingly important for surgical training, given the explosive growth of automation technologies. Existing work on skill score prediction is limited and deserves more promising outcomes. The challenges lie on complicated surgical tasks and new subjects as trial performers. Moreover, previous work mostly provides local feedback involving each individual video frame or clip that does not manifest human-interpretable semantics itself. To overcome these issues and facilitate more accurate and interpretable skill score prediction, we propose a novel video-based method incorporating recognized surgical gestures (segments) and skill levels (for both performers and gestures). Our method consists of two correlated multi-task learning frameworks. The main task of the first framework is to predict final skill scores of surgical trials and the auxiliary tasks are to recognize surgical gestures and to classify performers¡¯ skills into self-proclaimed skill levels. The second framework, which is based on gesture-level features accumulated until the end of each previously identified gesture, incrementally generates running intermediate skill scores for feedback decoding. Experiments on JIGSAWS dataset show our first framework on C3D features pushes state-of-the-art prediction performance further to 0.83, 0.86 and 0.69 of Spearman¡¯s correlation for the three surgical tasks under LOUO validation scheme. It even achieves 0.68 when generalizing across these tasks. For the second framework, additional gesture-level skill levels and captions are annotated by experts. The trend of predicted intermediate skill scores indicating problematic gestures is demonstrated as interpretable feedback. It turns out such trend resembles human¡¯s scoring process.",2020/9/29,2,10.1007/978-3-030-59716-0_64
Class-Aware Multi-window Adversarial Lung Nodule Synthesis Conditioned on Semantic Features.,"[{'@pid': '55/7780-1', 'text': 'Qiuli Wang 0001'}, {'@pid': '275/6851', 'text': 'Xingpeng Zhang'}, {'@pid': '181/2832', 'text': 'Wei Chen'}, {'@pid': '05/1958', 'text': 'Kun Wang'}, {'@pid': '91/2989', 'text': 'Xiaohong Zhang'}]","['Lungnodulesynthesis', 'Multi-window', 'GenerativeAdversarialNetworks', 'ComputedTomography']","Nodule CT image synthesis is effective as a data augmentation method for deep learning tasks about lung nodules. To advance the realistic malignant/benign lung nodule synthesis, the conditional Generative Adversarial Networks have been widely adopted. In this paper, we argue about an issue in the existing technique for class-aware nodule synthesis: the class-aware controllability of semantic features. To address this issue, we propose a adversarial lung nodule synthesis framework based on conditional Generative Adversarial Networks and class-aware multi-window semantic feature learning. By learning semantic features from multi-window CT images, our framework can generate realistic nodule CT images, and has better controllability of class-aware nodule features. Our framework provides a new perspective for nodule CT image synthesis that has never been noticed before. We train our framework on the public dataset LIDC-IDRI. Our framework improves the malignancy prediction F1 score by more than 3% and shows promising results as a solution for lung nodule augmentation. The source code can be found at https://github.com/qiuliwang/CA-MW-Adversarial-Synthesis.",2020/9/29,2,10.1007/978-3-030-59725-2_57
Voxel2Mesh - 3D Mesh Model Generation from Volumetric Data.,"[{'@pid': '249/2754', 'text': 'Udaranga Wickramasinghe'}, {'@pid': '209/5911', 'text': 'Edoardo Remelli'}, {'@pid': '03/8653', 'text': 'Graham Knott'}, {'@pid': 'f/PFua', 'text': 'Pascal Fua'}]","['Volumetricsegmentation', '3Dsurfaces', 'Deeplearning']","CNN-based volumetric methods that label individual voxels now dominate the field of biomedical segmentation. However, 3D surface representations are often required for proper analysis. They can be obtained by post-processing the labeled volumes which typically introduces artifacts and prevents end-to-end training. In this paper, we therefore introduce a novel architecture that goes directly from 3D image volumes to 3D surfaces without post-processing and with better accuracy than current methods. We evaluate it on Electron Microscopy and MRI brain images as well as CT liver scans. We will show that it outperforms state-of-the-art segmentation methods.",2020/9/29,2,10.1007/978-3-030-59719-1_30
Longitudinal Image Registration with Temporal-Order and Subject-Specificity Discrimination.,"[{'@pid': '213/7762', 'text': 'Qianye Yang'}, {'@pid': '223/4478', 'text': 'Yunguan Fu'}, {'@pid': '205/3550', 'text': 'Francesco Giganti'}, {'@pid': '209/9690', 'text': 'Nooshin Ghavami'}, {'@pid': '123/9213', 'text': 'Qingchao Chen'}, {'@pid': 'n/JAlisonNoble', 'text': 'J. Alison Noble'}, {'@pid': '99/4387', 'text': 'Tom Vercauteren'}, {'@pid': '63/4928', 'text': 'Dean C. Barratt'}, {'@pid': '45/5086', 'text': 'Yipeng Hu'}]","['Medicalimageregistration', 'Longitudinaldata', 'Maximummeandiscrepancy']","Morphological analysis of longitudinal MR images plays a key role in monitoring disease progression for prostate cancer patients, who are placed under an active surveillance program. In this paper, we describe a learning-based image registration algorithm to quantify changes on regions of interest between a pair of images from the same patient, acquired at two different time points. Combining intensity-based similarity and gland segmentation as weak supervision, the population-data-trained registration networks significantly lowered the target registration errors (TREs) on holdout patient data, compared with those before registration and those from an iterative registration algorithm. Furthermore, this work provides a quantitative analysis on several longitudinal-data-sampling strategies and, in turn, we propose a novel regularisation method based on maximum mean discrepancy, between differently-sampled training image pairs. Based on 216 3D MR images from 86 patients, we report a mean TRE of 5.6?mm and show statistically significant differences between the different training data sampling strategies.",2020/9/29,2,10.1007/978-3-030-59716-0_24
Learning Sample-Adaptive Intensity Lookup Table for Brain Tumor Segmentation.,"[{'@pid': '220/1584', 'text': 'Biting Yu'}, {'@pid': '45/933', 'text': 'Luping Zhou'}, {'@pid': 'w/LeiWang1', 'text': 'Lei Wang 0001'}, {'@pid': '117/3040', 'text': 'Wanqi Yang'}, {'@pid': '98/2604', 'text': 'Ming Yang'}, {'@pid': '43/6221', 'text': 'Pierrick Bourgeat'}, {'@pid': '21/6805', 'text': 'Jurgen Fripp'}]","['Braintumorsegmentation', 'Magneticresonanceimaging', 'Sample-adaptivelearning']","Intensity variation among MR images increases the difficulty of training a segmentation model and generalizing it to unseen MR images. To solve this problem, we propose to learn a sample-adaptive intensity lookup table (LuT) that adjusts each image¡¯s contrast dynamically so that the resulting images could better serve the subsequent segmentation task. Specifically, our proposed deep SA-LuT-Net consists of an LuT module and a segmentation module, trained in an end-to-end manner: the LuT module learns a sample-specific piece-wise linear intensity mapping function under the guide of the performance of the segmentation module. We develop our SA-LuT-Nets based on two backbone networks, DMFNet and the modified 3D Unet, respectively, and validate them on BRATS2018 dataset for brain tumor segmentation. Our experiment results clearly show the effectiveness of SA-LuT-Net in the scenarios of both single and multi-modalities, which is superior over the two baselines and many other relevant state-of-the-art segmentation models.",2020/9/29,2,10.1007/978-3-030-59719-1_22
Symmetric Dilated Convolution for Surgical Gesture Recognition.,"[{'@pid': '208/0855', 'text': 'Jinglu Zhang'}, {'@pid': '208/0835', 'text': 'Yinyu Nie'}, {'@pid': '240/1414', 'text': 'Yao Lyu'}, {'@pid': '42/4455', 'text': 'Hailin Li'}, {'@pid': '80/3338', 'text': 'Jian Chang'}, {'@pid': '14/5743', 'text': 'Xiaosong Yang'}, {'@pid': 'z/JianJZhang-1', 'text': 'Jian-Jun Zhang 0001'}]","['Surgicalgesturerecognition', 'Temporalconvolutionnetwork', 'Symmetricdilation', 'Self-attention']","Automatic surgical gesture recognition is a prerequisite of intra-operative computer assistance and objective surgical skill assessment. Prior works either require additional sensors to collect kinematics data or have limitations on capturing temporal information from long and untrimmed surgical videos. To tackle these challenges, we propose a novel temporal convolutional architecture to automatically detect and segment surgical gestures with corresponding boundaries only using RGB videos. We devise our method with a symmetric dilation structure bridged by a self-attention module to encode and decode the long-term temporal patterns and establish the frame-to-frame relationship accordingly. We validate the effectiveness of our approach on a fundamental robotic suturing task from the JIGSAWS dataset. The experiment results demonstrate the ability of our method on capturing long-term frame dependencies, which largely outperform the state-of-the-art methods on the frame-wise accuracy up?to \(\sim \)6 points and the F1@50 score \(\sim \)6 points.",2020/9/29,2,10.1007/978-3-030-59716-0_39
Learning to Segment When Experts Disagree.,"[{'@pid': '03/4043', 'text': 'Le Zhang'}, {'@pid': '187/6071', 'text': 'Ryutaro Tanno'}, {'@pid': '222/2775', 'text': 'Kevin Bronik'}, {'@pid': '73/4226', 'text': 'Chen Jin'}, {'@pid': '40/11373', 'text': 'Parashkev Nachev'}, {'@pid': '116/1982', 'text': 'Frederik Barkhof'}, {'@pid': '57/6075', 'text': 'Olga Ciccarelli'}, {'@pid': '37/6152', 'text': 'Daniel C. Alexander'}]",[],"Recent years have seen an increasing use of supervised learning methods for segmentation tasks. However, the predictive performance of these algorithms depend on the quality of labels, especially in medical image domain, where both the annotation cost and inter-observer variability are high. In a typical annotation collection process, different clinical experts provide their estimates of the ¡°true¡± segmentation labels under the influence of their levels of expertise and biases. Treating these noisy labels blindly as the ground truth can adversely affect the performance of supervised segmentation models. In this work, we present a neural network architecture for jointly learning, from noisy observations alone, both the reliability of individual annotators and the true segmentation label distributions. The separation of the annotators¡¯ characteristics and true segmentation label is achieved by encouraging the estimated annotators to be maximally unreliable while achieving high fidelity with the training data. Our method can also be viewed as a translation of STAPLE, an established label aggregation framework proposed in Warfield et al.
[1] to the supervised learning paradigm. We demonstrate first on a generic segmentation task using MNIST data and then adapt for usage with MRI scans of multiple sclerosis (MS) patients for lesion labelling. Our method shows considerable improvement over the relevant baselines on both datasets in terms of segmentation accuracy and estimation of annotator reliability, particularly when only a single label is available per image. An open-source implementation of our approach can be found at https://github.com/UCLBrain/MSLS.",2020/9/29,2,10.1007/978-3-030-59710-8_18
Brain Tumor Segmentation with Missing Modalities via Latent Multi-source Correlation Representation.,"[{'@pid': '224/9353', 'text': 'Tongxue Zhou'}, {'@pid': '17/122', 'text': 'St¨¦phane Canu'}, {'@pid': '92/9160', 'text': 'Pierre Vera'}, {'@pid': '22/4936', 'text': 'Su Ruan'}]","['Braintumorsegmentation', 'Multi-modal', 'Missingmodalities', 'Fusion', 'Latentcorrelationrepresentation', 'Deeplearning']","Multimodal MR images can provide complementary information for accurate brain tumor segmentation. However, it¡¯s common to have missing imaging modalities in clinical practice. Since there exists a strong correlation between multi modalities, a novel correlation representation block is proposed to specially discover the latent multi-source correlation. Thanks to the obtained correlation representation, the segmentation becomes more robust in the case of missing modalities. The model parameter estimation module first maps the individual representation produced by each encoder to obtain independent parameters, then, under these parameters, the correlation expression module transforms all the individual representations to form a latent multi-source correlation representation. Finally, the correlation representations across modalities are fused via the attention mechanism into a shared representation to emphasize the most important features for segmentation. We evaluate our model on BraTS 2018 datasets, it outperforms the current state-of-the-art method and produces robust results when one or more modalities are missing.",2020/9/29,2,10.1007/978-3-030-59719-1_52
Comparing to Learn - Surpassing ImageNet Pretraining on Radiographs by Comparing Image Representations.,"[{'@pid': '76/7825', 'text': 'Hong-Yu Zhou'}, {'@pid': '25/6555', 'text': 'Shuang Yu'}, {'@pid': '205/3556', 'text': 'Cheng Bian'}, {'@pid': '92/498', 'text': 'Yifan Hu'}, {'@pid': '86/7113-2', 'text': 'Kai Ma 0002'}, {'@pid': '44/6510', 'text': 'Yefeng Zheng'}]","['Pretrainedmodels', 'Self-supervisedlearning', 'Radiograph']","In deep learning era, pretrained models play an important role in medical image analysis, in which ImageNet pretraining has been widely adopted as the best way. However, it is undeniable that there exists an obvious domain gap between natural images and medical images. To bridge this gap, we propose a new pretraining method which learns from 700k radiographs given no manual annotations. We call our method as Comparing to Learn (C2L) because it learns robust features by comparing different image representations. To verify the effectiveness of C2L, we conduct comprehensive ablation studies and evaluate it on different tasks and datasets. The experimental results on radiographs show that C2L can outperform ImageNet pretraining and previous state-of-the-art approaches significantly. Code and models are available at https://github.com/funnyzhou/C2L_MICCAI2020.",2020/9/29,2,10.1007/978-3-030-59710-8_39
Synthetic and Real Inputs for Tool Segmentation in Robotic Surgery.,"[{'@pid': '245/5777', 'text': 'Emanuele Colleoni'}, {'@pid': '09/6217', 'text': 'Philip J. Edwards'}, {'@pid': '53/3543', 'text': 'Danail Stoyanov'}]","['Instrumentdetectionandsegmentation', 'Surgicalvision', 'Computerassistedinterventions']","Semantic tool segmentation in surgical videos is important for surgical scene understanding and computer-assisted interventions as well as for the development of robotic automation. The problem is challenging because different illumination conditions, bleeding, smoke and occlusions can reduce algorithm robustness. At present labelled data for training deep learning models is still lacking for semantic surgical instrument segmentation and in this paper we show that it may be possible to use robot kinematic data coupled with laparoscopic images to alleviate the labelling problem. We propose a new deep learning based model for parallel processing of both laparoscopic and simulation images for robust segmentation of surgical tools. Due to the lack of laparoscopic frames annotated with both segmentation ground truth and kinematic information a new custom dataset was generated using the da Vinci Research Kit (dVRK) and is made available.",2020/9/29,3,10.1007/978-3-030-59716-0_67
TeCNO - Surgical Phase Recognition with Multi-stage Temporal Convolutional Networks.,"[{'@pid': '262/0161', 'text': 'Tobias Czempiel'}, {'@pid': '198/1510', 'text': 'Magdalini Paschali'}, {'@pid': '150/2920', 'text': 'Matthias Keicher'}, {'@pid': '221/6174', 'text': 'Walter Simson'}, {'@pid': '20/258', 'text': 'Hubertus Feussner'}, {'@pid': '70/4799-1', 'text': 'Seong Tae Kim 0001'}, {'@pid': 'n/NassirNavab', 'text': 'Nassir Navab'}]","['Surgicalworkflow', 'Surgicalphaserecognition', 'TemporalConvolutionalNetworks', 'Endoscopicvideos', 'Cholecystectomy']","Automatic surgical phase recognition is a challenging and crucial task with the potential to improve patient safety and become an integral part of intra-operative decision-support systems. In this paper, we propose, for the first time in workflow analysis, a Multi-Stage Temporal Convolutional Network (MS-TCN) that performs hierarchical prediction refinement for surgical phase recognition. Causal, dilated convolutions allow for a large receptive field and online inference with smooth predictions even during ambiguous transitions. Our method is thoroughly evaluated on two datasets of laparoscopic cholecystectomy videos with and without the use of additional surgical tool information. Outperforming various state-of-the-art LSTM approaches, we verify the suitability of the proposed causal MS-TCN for surgical phase recognition.",2020/9/29,3,10.1007/978-3-030-59716-0_33
Cross-Modality Multi-atlas Segmentation Using Deep Neural Networks.,"[{'@pid': '272/5414', 'text': 'Wangbin Ding'}, {'@pid': '13/7007-20', 'text': 'Lei Li 0020'}, {'@pid': '69/1049', 'text': 'Xiahai Zhuang'}, {'@pid': '45/9302', 'text': 'Liqin Huang'}]","['MAS', 'Cross-modality', 'Similarity']","Both image registration and label fusion in the multi-atlas segmentation (MAS) rely on the intensity similarity between target and atlas images. However, such similarity can be problematic when target and atlas images are acquired using different imaging protocols. High-level structure information can provide reliable similarity measurement for cross-modality images when cooperating with deep neural networks (DNNs). This work presents a new MAS framework for cross-modality images, where both image registration and label fusion are achieved by DNNs. For image registration, we propose a consistent registration network, which can jointly estimate forward and backward dense displacement fields (DDFs). Additionally, an invertible constraint is employed in the network to reduce the correspondence ambiguity of the estimated DDFs. For label fusion, we adapt a few-shot learning network to measure the similarity of atlas and target patches. Moreover, the network can be seamlessly integrated into the patch-based label fusion. The proposed framework is evaluated on the MM-WHS dataset of MICCAI 2017. Results show that the framework is effective in both cross-modality registration and segmentation.",2020/9/29,3,10.1007/978-3-030-59716-0_23
Spatio-Temporal Graph Convolution for Resting-State fMRI Analysis.,"[{'@pid': '136/5972', 'text': 'Soham Gadgil'}, {'@pid': '60/1375', 'text': 'Qingyu Zhao'}, {'@pid': '46/3124', 'text': 'Adolf Pfefferbaum'}, {'@pid': '55/1055', 'text': 'Edith V. Sullivan'}, {'@pid': '93/2941', 'text': 'Ehsan Adeli 0001'}, {'@pid': '07/6023', 'text': 'Kilian M. Pohl'}]",[],"The Blood-Oxygen-Level-Dependent (BOLD) signal of resting-state fMRI (rs-fMRI) records the temporal dynamics of intrinsic functional networks in the brain. However, existing deep learning methods applied to rs-fMRI either neglect the functional dependency between different brain regions in a network or discard the information in the temporal dynamics of brain activity. To overcome those shortcomings, we propose to formulate functional connectivity networks within the context of spatio-temporal graphs. We train a spatio-temporal graph convolutional network (ST-GCN) on short sub-sequences of the BOLD time series to model the non-stationary nature of functional connectivity. Simultaneously, the model learns the importance of graph edges within ST-GCN to gain insight into the functional connectivities contributing to the prediction. In analyzing the rs-fMRI of the Human Connectome Project (HCP, N = 1,091) and the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA, N = 773), ST-GCN is significantly more accurate than common approaches in predicting gender and age based on BOLD signals. Furthermore, the brain regions and functional connections significantly contributing to the predictions of our model are important markers according to the neuroscience literature.",2020/9/29,3,10.1007/978-3-030-59728-3_52
ISINet - An Instance-Based Approach for Surgical Instrument Segmentation.,"[{'@pid': '200/5697', 'text': 'Cristina Gonz¨¢lez'}, {'@pid': '55/5241', 'text': 'Laura Bravo S¨¢nchez'}, {'@pid': '41/639', 'text': 'Pablo Arbelaez'}]","['Robotic-assistedsurgery', 'Instrumenttypesegmentation', 'Image-guidedsurgery', 'Computerassistedintervention', 'Medicalimagecomputing']","We study the task of semantic segmentation of surgical instruments in robotic-assisted surgery scenes. We propose the Instance-based Surgical Instrument Segmentation Network (ISINet), a method that addresses this task from an instance-based segmentation perspective. Our method includes a temporal consistency module that takes into account the previously overlooked and inherent temporal information of the problem. We validate our approach on the existing benchmark for the task, the Endoscopic Vision 2017 Robotic Instrument Segmentation Dataset 
[2], and on the 2018 version of the dataset 
[1], whose annotations we extended for the fine-grained version of instrument segmentation. Our results show that ISINet significantly outperforms state-of-the-art methods, with our baseline version duplicating the Intersection over Union (IoU) of previous methods and our complete model triplicating the IoU.",2020/9/29,3,10.1007/978-3-030-59716-0_57
Deep Interactive Learning - An Efficient Labeling Approach for Deep Learning-Based Osteosarcoma Treatment Response Assessment.,"[{'@pid': '201/7610', 'text': 'David Joon Ho'}, {'@pid': '268/7846', 'text': 'Narasimhan P. Agaram'}, {'@pid': '34/8484', 'text': 'Peter J. Sch¨¹ffler'}, {'@pid': '243/9189', 'text': 'Chad M. Vanderbilt'}, {'@pid': '268/7864', 'text': 'Marc-Henri Jean'}, {'@pid': '268/8214', 'text': 'Meera R. Hameed'}, {'@pid': '36/1644', 'text': 'Thomas J. Fuchs'}]","['Computationalpathology', 'Interactivelearning', 'Osteosarcoma']","Osteosarcoma is the most common malignant primary bone tumor. Standard treatment includes pre-operative chemotherapy followed by surgical resection. The response to treatment as measured by ratio of necrotic tumor area to overall tumor area is a known prognostic factor for overall survival. This assessment is currently done manually by pathologists by looking at glass slides under the microscope which may not be reproducible due to its subjective nature. Convolutional neural networks (CNNs) can be used for automated segmentation of viable and necrotic tumor on osteosarcoma whole slide images. One bottleneck for supervised learning is that large amounts of accurate annotations are required for training which is a time-consuming and expensive process. In this paper, we describe Deep Interactive Learning (DIaL) as an efficient labeling approach for training CNNs. After an initial labeling step is done, annotators only need to correct mislabeled regions from previous segmentation predictions to improve the CNN model until the satisfactory predictions are achieved. Our experiments show that our CNN model trained by only 7?h of annotation using DIaL can successfully estimate ratios of necrosis within expected inter-observer variation rate for non-standardized manual surgical pathology task.",2020/9/29,3,10.1007/978-3-030-59722-1_52
Knowledge Distillation from Multi-modal to Mono-modal Segmentation Networks.,"[{'@pid': '275/6897', 'text': 'Minhao Hu'}, {'@pid': '275/6967', 'text': 'Matthis Maillard'}, {'@pid': '85/3714', 'text': 'Ya Zhang'}, {'@pid': '275/6891', 'text': 'Tommaso Ciceri'}, {'@pid': '275/6896', 'text': 'Giammarco La Barbera'}, {'@pid': '33/1668', 'text': 'Isabelle Bloch'}, {'@pid': '134/9724', 'text': 'Pietro Gori'}]",[],"The joint use of multiple imaging modalities for medical image segmentation has been widely studied in recent years. The fusion of information from different modalities has demonstrated to improve the segmentation accuracy, with respect to mono-modal segmentations, in several applications. However, acquiring multiple modalities is usually not possible in a clinical setting due to a limited number of physicians and scanners, and to limit costs and scan time. Most of the time, only one modality is acquired. In this paper, we propose KD-Net, a framework to transfer knowledge from a trained multi-modal network (teacher) to a mono-modal one (student). The proposed method is an adaptation of the generalized distillation framework where the student network is trained on a subset (1 modality) of the teacher¡¯s inputs (n modalities). We illustrate the effectiveness of the proposed framework in brain tumor segmentation with the BraTS 2018 dataset. Using different architectures, we show that the student network effectively learns from the teacher and always outperforms the baseline mono-modal network in terms of segmentation accuracy.",2020/9/29,3,10.1007/978-3-030-59710-8_75
Multi-organ Segmentation via Co-training Weight-Averaged Models from Few-Organ Datasets.,"[{'@pid': '56/2875', 'text': 'Rui Huang'}, {'@pid': '44/3875', 'text': 'Yuanjie Zheng'}, {'@pid': '40/3396', 'text': 'Zhiqiang Hu'}, {'@pid': '53/3894', 'text': 'Shaoting Zhang'}, {'@pid': '27/7402-1', 'text': 'Hongsheng Li 0001'}]","['Multi-organsegmentation', 'Co-training', 'Few-organdatasets']","Multi-organ segmentation requires to segment multiple organs of interest from each image. However, it is generally quite difficult to collect full annotations of all the organs on the same images, as some medical centers might only annotate a portion of the organs due to their own clinical practice. In most scenarios, one might obtain annotations of a single or a few organs from one training set, and obtain annotations of the other organs from another set of training images. Existing approaches mostly train and deploy a single model for each subset of organs, which are memory intensive and also time inefficient. In this paper, we propose to co-train weight-averaged models for learning a unified multi-organ segmentation network from few-organ datasets. Specifically, we collaboratively train two networks and let the coupled networks teach each other on un-annotated organs. To alleviate the noisy teaching supervisions between the networks, the weighted-averaged models are adopted to produce more reliable soft labels. In addition, a novel region mask is utilized to selectively apply the consistent constraint on the un-annotated organ regions that require collaborative teaching, which further boosts the performance. Extensive experiments on three publicly available single-organ datasets LiTS
[1], KiTS
[8], Pancreas
[12] and manually-constructed single-organ datasets from MOBA
[7] show that our method can better utilize the few-organ datasets and achieves superior performance with less inference computational cost.
",2020/9/29,3,10.1007/978-3-030-59719-1_15
"Whole MILC - Generalizing Learned Dynamics Across Tasks, Datasets, and Populations.","[{'@pid': '201/0604', 'text': 'Usman Mahmood'}, {'@pid': '251/8587', 'text': 'Md Mahfuzur Rahman'}, {'@pid': '192/1394', 'text': 'Alex Fedorov'}, {'@pid': '202/6327', 'text': 'Noah Lewis'}, {'@pid': '133/4443', 'text': 'Zening Fu'}, {'@pid': '48/3821', 'text': 'Vince D. Calhoun'}, {'@pid': '07/227', 'text': 'Sergey M. Plis'}]","['Transferlearning', 'Self-supervised', 'Deeplearning', 'RestingstatefMRI.']","Behavioral changes are the earliest signs of a mental disorder, but arguably, the dynamics of brain function gets affected even earlier. Subsequently, spatio-temporal structure of disorder-specific dynamics is crucial for early diagnosis and understanding the disorder mechanism. A common way of learning discriminatory features relies on training a classifier and evaluating feature importance. Classical classifiers, based on handcrafted features are quite powerful, but suffer the curse of dimensionality when applied to large input dimensions of spatio-temporal data. Deep learning algorithms could handle the problem and a model introspection could highlight discriminatory spatio-temporal regions but need way more samples to train. In this paper we present a novel self supervised training schema which reinforces whole sequence mutual information local to context (whole MILC). We pre-train the whole MILC model on unlabeled and unrelated healthy control data. We test our model on three different disorders (i) Schizophrenia (ii) Autism and (iii) Alzheimers and four different studies. Our algorithm outperforms existing self-supervised pre-training methods and provides competitive classification results to classical machine learning algorithms. Importantly, whole MILC enables attribution of subject diagnosis to specific spatio-temporal regions in the fMRI signal.",2020/9/29,3,10.1007/978-3-030-59728-3_40
Self-supervised Skull Reconstruction in Brain CT Images with Decompressive Craniectomy.,"[{'@pid': '270/3494', 'text': 'Franco Matzkin'}, {'@pid': '60/11369', 'text': 'Virginia F. J. Newcombe'}, {'@pid': '265/2662', 'text': 'Susan Stevenson'}, {'@pid': '265/2733', 'text': 'Aneesh Khetani'}, {'@pid': '15/5017', 'text': 'Tom Newman'}, {'@pid': '265/2630', 'text': 'Richard Digby'}, {'@pid': '16/5584', 'text': 'Andrew Stevens'}, {'@pid': '86/2890', 'text': 'Ben Glocker'}, {'@pid': '134/9909', 'text': 'Enzo Ferrante'}]","['Skullreconstruction', 'Self-supervisedlearning', 'Decompressivecraniectomy']","Decompressive craniectomy (DC) is a common surgical procedure consisting of the removal of a portion of the skull that is performed after incidents such as stroke, traumatic brain injury (TBI) or other events that could result in acute subdural hemorrhage and/or increasing intracranial pressure. In these cases, CT scans are obtained to diagnose and assess injuries, or guide a certain therapy and intervention. We propose a deep learning based method to reconstruct the skull defect removed during DC performed after TBI from post-operative CT images. This reconstruction is useful in multiple scenarios, e.g. to support the creation of cranioplasty plates, accurate measurements of bone flap volume and total intracranial volume, important for studies that aim to relate later atrophy to patient outcome. We propose and compare alternative self-supervised methods where an encoder-decoder convolutional neural network (CNN) estimates the missing bone flap on post-operative CTs. The self-supervised learning strategy only requires images with complete skulls and avoids the need for annotated DC images. For evaluation, we employ real and simulated images with DC, comparing the results with other state-of-the-art approaches. The experiments show that the proposed model outperforms current manual methods, enabling reconstruction even in highly challenging cases where big skull defects have been removed during surgery.
",2020/9/29,3,10.1007/978-3-030-59713-9_38
Self-supervised Nuclei Segmentation in Histopathological Images Using Attention.,"[{'@pid': '144/1457', 'text': 'Mihir Sahasrabudhe'}, {'@pid': '167/1074', 'text': 'Stergios Christodoulidis'}, {'@pid': '270/3535', 'text': 'Roberto Salgado'}, {'@pid': '10/6036', 'text': 'Stefan Michiels'}, {'@pid': '54/8254', 'text': 'Sherene Loi'}, {'@pid': '179/5558', 'text': 'Fabrice Andr¨¦'}, {'@pid': 'p/NikosParagios', 'text': 'Nikos Paragios'}, {'@pid': '169/9108', 'text': 'Maria Vakalopoulou'}]","['Pathology', 'Wholeslideimages', 'Nucleisegmentation', 'Deeplearning', 'Self-supervision', 'Attentionmodels']","Segmentation and accurate localization of nuclei in histopathological images is a very challenging problem, with most existing approaches adopting a supervised strategy. These methods usually rely on manual annotations that require a lot of time and effort from medical experts. In this study, we present a self-supervised approach for segmentation of nuclei for whole slide histopathology images. Our method works on the assumption that the size and texture of nuclei can determine the magnification at which a patch is extracted. We show that the identification of the magnification level for tiles can generate a preliminary self-supervision signal to locate nuclei. We further show that by appropriately constraining our model it is possible to retrieve meaningful segmentation maps as an auxiliary output to the primary magnification identification task. Our experiments show that with standard post-processing, our method can outperform other unsupervised nuclei segmentation approaches and report similar performance with supervised ones on the publicly available MoNuSeg dataset. Our code and models are available online (https://github.com/msahasrabudhe/miccai2020_self_sup_nuclei_seg) to facilitate further research.",2020/9/29,3,10.1007/978-3-030-59722-1_38
MR-to-US Registration Using Multiclass Segmentation of Hepatic Vasculature with a Reduced 3D U-Net.,"[{'@pid': '275/6818', 'text': 'Bart R. Thomson'}, {'@pid': '275/6740', 'text': 'Jasper N. Smit'}, {'@pid': '275/6908', 'text': 'Oleksandra V. Ivashchenko'}, {'@pid': '275/6801', 'text': 'Niels F. M. Kok'}, {'@pid': '183/7580', 'text': 'Koert Kuhlmann'}, {'@pid': '183/7712', 'text': 'Theodoor Jacques Marie Ruers'}, {'@pid': '134/4948', 'text': 'Matteo Fusaglia'}]","['Computerassistedintervention', 'Liversurgery', 'Non-rigidregistration']","Accurate hepatic vessel segmentation and registration using ultrasound (US) can contribute to beneficial navigation during hepatic surgery. However, it is challenging due to noise and speckle in US imaging and liver deformation. Therefore, a workflow is developed using a reduced 3D U-Net for segmentation, followed by non-rigid coherent point drift (CPD) registration. By means of electromagnetically tracked US, 61 3D volumes were acquired during surgery. Dice scores of 0.77, 0.65 and 0.66 were achieved for segmentation of all vasculature, hepatic vein and portal vein respectively. This compares to inter-observer variabilities of 0.85, 0.88 and 0.74 respectively. Target registration error at a tumor lesion of interest was lower (7.1?mm) when registration is performed either on the hepatic or the portal vein, compared to using all vasculature (8.9?mm). Using clinical data, we developed a workflow consisting of multi-class segmentation combined with selective non-rigid registration that leads to sufficient accuracy for integration in computer assisted liver surgery.",2020/9/29,3,10.1007/978-3-030-59716-0_27
Modelling the Distribution of 3D Brain MRI Using a 2D Slice VAE.,"[{'@pid': '190/1677', 'text': 'Anna Volokitin'}, {'@pid': '84/8711', 'text': 'Ertunc Erdil'}, {'@pid': '201/1182', 'text': 'Neerav Karani'}, {'@pid': '210/0842', 'text': 'Kerem Can Tezcan'}, {'@pid': '184/3448', 'text': 'Xiaoran Chen'}, {'@pid': '61/5017', 'text': 'Luc Van Gool'}, {'@pid': '45/7041', 'text': 'Ender Konukoglu'}]","['Generativemodelling', 'VAE', '3D']","Probabilistic modelling has been an essential tool in medical image analysis, especially for analyzing brain Magnetic Resonance Images (MRI). Recent deep learning techniques for estimating high-dimensional distributions, in particular Variational Autoencoders (VAEs), opened up new avenues for probabilistic modeling. Modelling of volumetric data has remained a challenge, however, because constraints on available computation and training data make it difficult effectively leverage VAEs, which are well-developed for 2D images. We propose a method to model 3D MR brain volumes distribution by combining a 2D slice VAE with a Gaussian model that captures the relationships between slices. We do so by estimating the sample mean and covariance in the latent space of the 2D model over the slice direction. This combined model lets us sample new coherent stacks of latent variables to decode into slices of a volume. We also introduce a novel evaluation method for generated volumes that quantifies how well their segmentations match those of true brain anatomy. We demonstrate that our proposed model is competitive in generating high quality volumes at high resolutions according to both traditional metrics and our proposed evaluation. (Code is available at https://github.com/voanna/slices-to-3d-brain-vae/).",2020/9/29,3,10.1007/978-3-030-59728-3_64
Automatic Data Augmentation for 3D Medical Image Segmentation.,"[{'@pid': '220/5335', 'text': 'Ju Xu'}, {'@pid': '211/9442', 'text': 'Mengzhang Li'}, {'@pid': '87/7756', 'text': 'Zhanxing Zhu'}]","['Medicalimagesegmentation', 'Dataaugmentation', 'AutoML']","Data augmentation is an effective and universal technique for improving generalization performance of deep neural networks. It could enrich diversity of training samples that is essential in medical image segmentation tasks because 1) the scale of medical image dataset is typically smaller, which may increase the risk of overfitting; 2) the shape and modality of different objects such as organs or tumors are unique, thus requiring customized data augmentation policy. However, most data augmentation implementations are hand-crafted and suboptimal in medical image processing. To fully exploit the potential of data augmentation, we propose an efficient algorithm to automatically search for the optimal augmentation strategies. We formulate the coupled optimization w.r.t. network weights and augmentation parameters into a differentiable form by means of stochastic relaxation. This formulation allows us to apply alternative gradient-based methods to solve it, i.e. stochastic natural gradient method with adaptive step-size. To the best of our knowledge, it is the first time that differentiable automatic data augmentation is employed in medical image segmentation tasks. Our numerical experiments demonstrate that the proposed approach significantly outperforms existing build-in data augmentation of state-of-the-art models.",2020/9/29,3,10.1007/978-3-030-59710-8_37
ImageCHD - A 3D Computed Tomography Image Dataset for Classification of Congenital Heart Disease.,"[{'@pid': '181/2733-4', 'text': 'Xiaowei Xu 0004'}, {'@pid': '154/2965', 'text': 'Tianchen Wang'}, {'@pid': '30/6224', 'text': 'Jian Zhuang'}, {'@pid': '239/3624', 'text': 'Haiyun Yuan'}, {'@pid': '227/7633', 'text': 'Meiping Huang'}, {'@pid': '275/6790', 'text': 'Jianzheng Cen'}, {'@pid': '244/9459', 'text': 'Qianjun Jia'}, {'@pid': '232/7896', 'text': 'Yuhao Dong'}, {'@pid': '94/5536', 'text': 'Yiyu Shi'}]","['Dataset', 'Congenitalheartdisease', 'Automaticdiagnosis', 'Computedtomography']","Congenital heart disease (CHD) is the most common type of birth defects, which occurs 1 in every 110 births in the United States. CHD usually comes with severe variations in heart structure and great artery connections that can be classified into many types. Thus highly specialized domain knowledge and time-consuming human process is needed to analyze the associated medical images. On the other hand, due to the complexity of CHD and the lack of dataset, little has been explored on the automatic diagnosis (classification) of CHDs. In this paper, we present ImageCHD, the first medical image dataset for CHD classification. ImageCHD contains 110 3D Computed Tomography (CT) images covering most types of CHD, which is of decent size compared with existing medical imaging datasets. Classification of CHDs requires the identification of large structural changes without any local tissue changes, with limited data. It is an example of a larger class of problems that are quite difficult for current machine-learning based vision methods to solve. To demonstrate this, we further present a baseline framework for automatic classification of CHD, based on a state-of-the-art CHD segmentation method. Experimental results show that the baseline framework can only achieve a classification accuracy of 82.0% under selective prediction scheme with 88.4% coverage, leaving big room for further improvement. We hope that ImageCHD can stimulate further research and lead to innovative and generic solutions that would have an impact in multiple domains. Our dataset is released to the public
[1].",2020/9/29,3,10.1007/978-3-030-59719-1_8
Motion Pyramid Networks for Accurate and Efficient Cardiac Motion Estimation.,"[{'@pid': '69/9936', 'text': 'Hanchao Yu'}, {'@pid': '05/3054', 'text': 'Xiao Chen'}, {'@pid': '176/5516', 'text': 'Humphrey Shi'}, {'@pid': '51/4242', 'text': 'Terrence Chen'}, {'@pid': 'h/ThomasSHuang', 'text': 'Thomas S. Huang'}, {'@pid': '85/10238', 'text': 'Shanhui Sun'}]","['Motionpyramidnetwork', 'Motioncompensation', 'Cyclicknowledgedistillation']","Cardiac motion estimation plays a key role in MRI cardiac feature tracking and function assessment such as myocardium strain. In this paper, we propose Motion Pyramid Networks, a novel deep learning-based approach for accurate and efficient cardiac motion estimation. We predict and fuse a pyramid of motion fields from multiple scales of feature representations to generate a more refined motion field. We then use a novel cyclic teacher-student training strategy to make the inference end-to-end and further improve the tracking performance. Our teacher model provides more accurate motion estimation as supervision through progressive motion compensations. Our student model learns from the teacher model to estimate motion in a single step while maintaining accuracy. The teacher-student knowledge distillation is performed in a cyclic way for a further performance boost. Our proposed method outperforms a strong baseline model on two public available clinical datasets significantly, evaluated by a variety of metrics and the inference time. New evaluation metrics are also proposed to represent errors in a clinically meaningful manner.",2020/9/29,3,10.1007/978-3-030-59725-2_42
Perfusion Quantification from Endoscopic Videos - Learning to Read Tumor Signatures.,"[{'@pid': '50/9226', 'text': 'Sergiy Zhuk'}, {'@pid': '11/10880', 'text': 'Jonathan P. Epperlein'}, {'@pid': '76/4693', 'text': 'Rahul Nair'}, {'@pid': '200/8936', 'text': 'Seshu Tirupathi'}, {'@pid': '61/11344', 'text': 'P¨®l Mac Aonghusa'}, {'@pid': '83/7859', 'text': 'Donal O&apos;Shea'}, {'@pid': '159/0679', 'text': 'Ronan Cahill'}]","['Perfusionquantification', 'Bio-physicalmodeling', 'Explainablefeaturesdesign', 'Tissueclassification', 'Cancer']","Intra-operative (this work was partially supported by Disruptive Technologies Innovation Fund, Ireland, project code DTIF2018 240 CA) identification of malignant versus benign or healthy tissue is a major challenge in fluorescence guided cancer surgery. We propose a perfusion quantification method for computer-aided interpretation of subtle differences in dynamic perfusion patterns which can be used to distinguish between normal tissue and benign or malignant tumors intra-operatively by using multispectral endoscopic videos. The method exploits the fact that vasculature arising from cancer angiogenesis gives tumors differing perfusion patterns from the surrounding normal tissues. Experimental evaluation of our method on a cohort of colorectal cancer surgery endoscopic videos suggests that it discriminates between healthy, cancerous and benign tissues with 95% accuracy.",2020/9/29,3,10.1007/978-3-030-59716-0_68
Dual-Teacher - Integrating Intra-domain and Inter-domain Teachers for Annotation-Efficient Cardiac Segmentation.,"[{'@pid': '181/2763', 'text': 'Kang Li'}, {'@pid': '94/9916', 'text': 'Shujun Wang'}, {'@pid': '165/8092', 'text': 'Lequan Yu'}, {'@pid': '52/2889', 'text': 'Pheng-Ann Heng'}]","['Semi-superviseddomainadaptation', 'Cross-modalitysegmentation', 'Cardiacsegmentation']","Medical image annotations are prohibitively time-consuming and expensive to obtain. To alleviate annotation scarcity, many approaches have been developed to efficiently utilize extra information, e.g., semi-supervised learning further exploring plentiful unlabeled data, domain adaptation including multi-modality learning and unsupervised domain adaptation resorting to the prior knowledge from additional modality. In this paper, we aim to investigate the feasibility of simultaneously leveraging abundant unlabeled data and well-established cross-modality data for annotation-efficient medical image segmentation. To this end, we propose a novel semi-supervised domain adaptation approach, namely Dual-Teacher, where the student model not only learns from labeled target data (e.g., CT), but also explores unlabeled target data and labeled source data (e.g., MR) by two teacher models. Specifically, the student model learns the knowledge of unlabeled target data from intra-domain teacher by encouraging prediction consistency, as well as the shape priors embedded in labeled source data from inter-domain teacher via knowledge distillation. Consequently, the student model can effectively exploit the information from all three data resources and comprehensively integrate them to achieve improved performance. We conduct extensive experiments on MM-WHS 2017 dataset and demonstrate that our approach is able to concurrently utilize unlabeled data and cross-modality data with superior performance, outperforming semi-supervised learning and domain adaptation methods with a large margin.",2020/9/29,4,10.1007/978-3-030-59710-8_41
Large Deformation Diffeomorphic Image Registration with Laplacian Pyramid Networks.,"[{'@pid': '209/9797', 'text': 'Tony C. W. Mok'}, {'@pid': '45/6335', 'text': 'Albert C. S. Chung'}]","['Imageregistration', 'Diffeomorphicregistration', 'DeepLaplacianpyramidnetworks']","Deep learning-based methods have recently demonstrated promising results in deformable image registration for a wide range of medical image analysis tasks. However, existing deep learning-based methods are usually limited to small deformation settings, and desirable properties of the transformation including bijective mapping and topology preservation are often being ignored by these approaches. In this paper, we propose a deep Laplacian Pyramid Image Registration Network, which can solve the image registration optimization problem in a coarse-to-fine fashion within the space of diffeomorphic maps. Extensive quantitative and qualitative evaluations on two MR brain scan datasets show that our method outperforms the existing methods by a significant margin while maintaining desirable diffeomorphic properties and promising registration speed.",2020/9/29,4,10.1007/978-3-030-59716-0_21
End-to-End Variational Networks for Accelerated MRI Reconstruction.,"[{'@pid': '200/7951', 'text': 'Anuroop Sriram'}, {'@pid': '117/3283', 'text': 'Jure Zbontar'}, {'@pid': '251/9178', 'text': 'Tullie Murrell'}, {'@pid': '116/2969', 'text': 'Aaron Defazio'}, {'@pid': '10/6888', 'text': 'C. Lawrence Zitnick'}, {'@pid': '230/4467', 'text': 'Nafissa Yakubova'}, {'@pid': '20/9884', 'text': 'Florian Knoll'}, {'@pid': '251/4911', 'text': 'Patricia M. Johnson'}]","['MRIacceleration', 'End-to-endlearning', 'Deeplearning']","The slow acquisition speed of magnetic resonance imaging (MRI) has led to the development of two complementary methods: acquiring multiple views of the anatomy simultaneously (parallel imaging) and acquiring fewer samples than necessary for traditional signal processing methods (compressed sensing). While the combination of these methods has the potential to allow much faster scan times, reconstruction from such undersampled multi-coil data has remained an open problem. In this paper, we present a new approach to this problem that extends previously proposed variational methods by learning fully end-to-end. Our method obtains new state-of-the-art results on the fastMRI dataset?
[16] for both brain and knee MRIs.",2020/9/29,4,10.1007/978-3-030-59713-9_7
SAUNet - Shape Attentive U-Net for Interpretable Medical Image Segmentation.,"[{'@pid': '256/9968', 'text': 'Jesse Sun'}, {'@pid': '256/9981', 'text': 'Fatemeh Darbeha'}, {'@pid': '256/9878', 'text': 'Mark Zaidi'}, {'@pid': '72/6811', 'text': 'Bo Wang'}]","['Semanticsegmentation', 'Medicalimaging', 'Interpretability']","Medical image segmentation is a difficult but important task for many clinical operations such as cardiac bi-ventricular volume estimation. More recently, there has been a shift to utilizing deep learning and fully convolutional neural networks (CNNs) to perform image segmentation that has yielded state-of-the-art results in many public benchmark datasets. Despite the progress of deep learning in medical image segmentation, standard CNNs are still not fully adopted in clinical settings as they lack robustness and interpretability. Shapes are generally more meaningful features than solely textures of images, which are features regular CNNs learn, causing a lack of robustness. Likewise, previous works surrounding model interpretability have been focused on post hoc gradient-based saliency methods. However, gradient-based saliency methods typically require additional computations post hoc and have been shown to be unreliable for interpretability. Thus, we present a new architecture called Shape Attentive U-Net (SAUNet) which focuses on model interpretability and robustness. The proposed architecture attempts to address these limitations by the use of a secondary shape stream that captures rich shape-dependent information in parallel with the regular texture stream. Furthermore, we suggest multi-resolution saliency maps can be learned using our dual-attention decoder module which allows for multi-level interpretability and mitigates the need for additional computations post hoc. Our method also achieves state-of-the-art results on the two large public cardiac MRI image segmentation datasets of SUN09 and AC17.",2020/9/29,4,10.1007/978-3-030-59719-1_77
Highly Accurate and Memory Efficient Unsupervised Learning-Based Discrete CT Registration Using 2.5D Displacement Search.,"[{'@pid': '95/10171', 'text': 'Mattias P. Heinrich'}, {'@pid': '151/9486', 'text': 'Lasse Hansen'}]","['Deformableregistration', 'Deeplearning', 'Discreteoptimisation']","Learning-based registration, in particular unsupervised approaches that use a deep network to predict a displacement field that minimise a conventional similarity metric, has gained huge interest within the last two years. It has, however, not yet reached the high accuracy of specialised conventional algorithms for estimating large 3D deformations. Employing a dense set of discrete displacements (in a so-called correlation layer) has shown great success in learning 2D optical flow estimation, cf. FlowNet and PWC-Net, but comes at excessive memory requirements when extended to 3D medical registration. We propose a highly accurate unsupervised learning framework for 3D abdominal CT registration that uses a discrete displacement layer and a contrast-invariant metric (MIND descriptors) that is evaluated in a probabilistic fashion. We realise a substantial reduction in memory and computational demand by iteratively subdividing the 3D search space into orthogonal planes. In our experimental validation on inter-subject deformable 3D registration, we demonstrate substantial improvements in accuracy (at least \(\approx \)10% points Dice) compared to widely used conventional methods (ANTs SyN, NiftyReg, IRTK) and state-of-the-art U-Net based learning methods (VoxelMorph). We reduce the search space 5-fold, speed-up the run-time twice and are on-par in terms of accuracy with a fully 3D discrete network.",2020/9/29,6,10.1007/978-3-030-59716-0_19
Joint Left Atrial Segmentation and Scar Quantification Based on a DNN with Spatial Encoding and Shape Attention.,"[{'@pid': '13/7007-20', 'text': 'Lei Li 0020'}, {'@pid': '84/10674', 'text': 'Xin Weng'}, {'@pid': '80/2501', 'text': 'Julia A. Schnabel'}, {'@pid': '69/1049', 'text': 'Xiahai Zhuang'}]","['Atrialscarsegmentation', 'Spatialencoding', 'Shapeattention']","We propose an end-to-end deep neural network (DNN) which can simultaneously segment the left atrial (LA) cavity and quantify LA scars. The framework incorporates the continuous spatial information of the target by introducing a spatially encoded (SE) loss based on the distance transform map. Compared to conventional binary label based loss, the proposed SE loss can reduce noisy patches in the resulting segmentation, which is commonly seen for deep learning-based methods. To fully utilize the inherent spatial relationship between LA and LA scars, we further propose a shape attention (SA) mechanism through an explicit surface projection to build an end-to-end-trainable model. Specifically, the SA scheme is embedded into a two-task network to perform the joint LA segmentation and scar quantification. Moreover, the proposed method can alleviate the severe class-imbalance problem when detecting small and discrete targets like scars. We evaluated the proposed framework on 60 LGE MRI data from the MICCAI2018 LA challenge. For LA segmentation, the proposed method reduced the mean Hausdorff distance from 36.4?mm to 20.0?mm compared to the 3D basic U-Net using the binary cross-entropy loss. For scar quantification, the method was compared with the results or algorithms reported in the literature and demonstrated better performance.",2020/9/29,6,10.1007/978-3-030-59719-1_12
PraNet - Parallel Reverse Attention Network for Polyp Segmentation.,"[{'@pid': '205/3148', 'text': 'Deng-Ping Fan'}, {'@pid': '263/3561', 'text': 'Ge-Peng Ji'}, {'@pid': '98/4450-2', 'text': 'Tao Zhou 0002'}, {'@pid': '76/4764-1', 'text': 'Geng Chen 0001'}, {'@pid': '63/7767', 'text': 'Huazhu Fu'}, {'@pid': '38/5435', 'text': 'Jianbing Shen'}, {'@pid': '75/1281', 'text': 'Ling Shao 0001'}]","['Colonoscopy', 'Polypsegmentation', 'Colorectalcancer']","Colonoscopy is an effective technique for detecting colorectal polyps, which are highly related to colorectal cancer. In clinical practice, segmenting polyps from colonoscopy images is of great importance since it provides valuable information for diagnosis and surgery. However, accurate polyp segmentation is a challenging task, for two major reasons: (i) the same type of polyps has a diversity of size, color and texture; and (ii) the boundary between a polyp and its surrounding mucosa is not sharp. To address these challenges, we propose a parallel reverse attention network (PraNet) for accurate polyp segmentation in colonoscopy images. Specifically, we first aggregate the features in high-level layers using a parallel partial decoder (PPD). Based on the combined feature, we then generate a global map as the initial guidance area for the following components. In addition, we mine the boundary cues using the reverse attention (RA) module, which is able to establish the relationship between areas and boundary cues. Thanks to the recurrent cooperation mechanism between areas and boundaries, our PraNet?is capable of calibrating some misaligned predictions, improving the segmentation accuracy. Quantitative and qualitative evaluations on five challenging datasets across six metrics show that our PraNet?improves the segmentation accuracy significantly, and presents a number of advantages in terms of generalizability, and real-time segmentation efficiency (\(\varvec{\sim }\)50 fps).",2020/9/29,10,10.1007/978-3-030-59725-2_26
Fast Correction of Eddy-Current and Susceptibility-Induced Distortions Using Rotation-Invariant Contrasts.,"[{'@pid': '44/10619', 'text': 'Sahar Ahmad'}, {'@pid': '50/3141-1', 'text': 'Ye Wu 0001'}, {'@pid': '244/1829', 'text': 'Khoi Minh Huynh'}, {'@pid': '13/1611', 'text': 'Kim-Han Thung'}, {'@pid': '98/3393', 'text': 'Weili Lin'}, {'@pid': '14/4383', 'text': 'Dinggang Shen'}, {'@pid': '93/5188', 'text': 'Pew-Thian Yap'}]","['Eddy-currentdistortion', 'Susceptibility-induceddistortion', 'Rotation-invariantcontrasts']","Diffusion MRI (dMRI) is typically time consuming as it involves acquiring a series of 3D volumes, each associated with a wave-vector in q-space that determines the diffusion direction and strength. The acquisition time is further increased when ¡°blip-up blip-down¡± scans are acquired with opposite phase encoding directions (PEDs) to facilitate distortion correction. In this work, we show that geometric distortions can be corrected without acquiring with opposite PEDs for each wave-vector, and hence the acquisition time can be halved. Our method uses complimentary rotation-invariant contrasts across shells of different diffusion weightings. Distortion-free structural T1-/T2-weighted MRI is used as reference for nonlinear registration in correcting the distortions. Signal dropout and pileup are corrected with the help of spherical harmonics. To demonstrate that our method is robust to changes in image appearance, we show that distortion correction with good structural alignment can be achieved within minutes for dMRI data of infants between 1 to 24?months of age.",2020/9/29,None,10.1007/978-3-030-59713-9_4
Skip-StyleGAN - Skip-Connected Generative Adversarial Networks for Generating 3D Rendered Image of Hand Bone Complex.,"[{'@pid': '250/3882', 'text': 'Jaesin Ahn'}, {'@pid': '49/1638', 'text': 'Hyun-Joo Lee'}, {'@pid': '49/816', 'text': 'Inchul Choi'}, {'@pid': '99/2069', 'text': 'Minho Lee'}]","['Generativeadversarialnetworks', 'CT', 'X-ray', '3Drendering', 'Handbonecomplex', 'Skip-connection', 'Skip-StyleGAN.']"," Computed tomography (CT) is commonly used for fracture diagnosis because it provides accurate visualization of shape with 3-dimensional(3D) structure. However, CT has some disadvantages such as the high dose of radiation involved in scanning, and relatively high expense compared to X-ray. Also, it is difficult to scan CT in the operation room despite it is necessary to check 3D structure during operation. On the other hand, X-ray is often used in operating rooms because it is relatively simple to scan. However, since X-ray only provides overlapped 2D images, surgeons should rely on 2D images to imagine 3D structure of a target shape. If we can create a 3D structure from a single 2D X-ray image, then it will be clinically valuable. Therefore, we propose Skip-StyleGAN that can efficiently generate rotated images of a given 2D image from 3D rendered shape. Based on the StyleGAN, we arrange training sequence and add skip-connection from the discriminator to the generator. Important discriminative information is transferred through this skip-connection, and it allows the generator to easily produce an appropriately rotated image by making a little variation during the training process. With the effect of skip-connection, Skip-StyleGAN can efficiently generate high-quality 3D rendered images even with small-sized data. Our experiments show that the proposed model successfully generates 3D rendered images of the hand bone complex.",2020/9/29,None,10.1007/978-3-030-59719-1_72
GAN-Based Realistic Bone Ultrasound Image and Label Synthesis for Improved Segmentation.,"[{'@pid': '239/9107', 'text': 'Ahmed Z. Alsinan'}, {'@pid': '275/6977', 'text': 'Charles Rule'}, {'@pid': '268/6961', 'text': 'Michael Vives'}, {'@pid': '76/6100', 'text': 'Vishal M. Patel'}, {'@pid': '81/5273', 'text': 'Ilker Hacihaliloglu'}]","['Orthopedicsurgery', 'Segmentation', 'Ultrasound', 'Bone', 'Generativeadversarialnetwork', 'Deeplearning']","To provide a safe alternative, for intra-operative fluoroscopy, ultrasound (US) has been investigated as an alternative safe imaging modality for various computer assisted orthopedic surgery (CAOS) procedures. However, low signal to noise ratio, imaging artifacts and bone surfaces appearing several millimeters (mm) in thickness have hindered the wide spread application of US in CAOS. In order to provide a solution for these problems, research has focused on the development of accurate, robust and real-time bone segmentation methods. Most recently methods based on deep learning have shown very promising results. However, scarcity of bone US data introduces significant challenges when training deep learning models. In this work, we propose a computational method, based on a novel generative adversarial network (GAN) architecture, to (1) produce synthetic B-mode US images and (2) their corresponding segmented bone surface masks in real-time. We show how a duality concept can be implemented for such tasks. Armed by two convolutional blocks, referred to as self-projection and self-attention blocks, our proposed GAN model synthesizes realistic B-mode bone US image and segmented bone masks. Quantitative and qualitative evaluation studies are performed on 1235 scans collected from 27 subjects using two different US machines to show comparison results of our model against state-of-the-art GANs for the task of bone surface segmentation using U-net.",2020/9/29,None,10.1007/978-3-030-59725-2_77
XCAT-GAN for Synthesizing 3D Consistent Labeled Cardiac MR Images on Anatomically Variable XCAT Phantoms.,"[{'@pid': '259/3245', 'text': 'Sina Amirrajab'}, {'@pid': '164/7548', 'text': 'Samaneh Abbasi-Sureshjani'}, {'@pid': '170/7661', 'text': 'Yasmina Al Khalil'}, {'@pid': '10/6677', 'text': 'Cristian Lorenz'}, {'@pid': '63/4505', 'text': 'J¨¹rgen Weese'}, {'@pid': '70/5175', 'text': 'Josien P. W. Pluim'}, {'@pid': '66/4471', 'text': 'Marcel Breeuwer'}]","['Conditionalimagesynthesis', 'CardiacMagneticResonanceimaging', 'XCATanatomicalphantom']","Generative adversarial networks (GANs) have provided promising data enrichment solutions by synthesizing high-fidelity images. However, generating large sets of labeled images with new anatomical variations remains unexplored. We propose a novel method for synthesizing cardiac magnetic resonance (CMR) images on a population of virtual subjects with a large anatomical variation, introduced using the 4D eXtended Cardiac and Torso (XCAT) computerized human phantom. We investigate two conditional image synthesis approaches grounded on a semantically-consistent mask-guided image generation technique: 4-class and 8-class XCAT-GANs. The 4-class technique relies on only the annotations of the heart; while the 8-class technique employs a predicted multi-tissue label map of the heart-surrounding organs and provides better guidance for our conditional image synthesis. For both techniques, we train our conditional XCAT-GAN with real images paired with corresponding labels and subsequently at the inference time, we substitute the labels with the XCAT derived ones. Therefore, the trained network accurately transfers the tissue-specific textures to the new label maps. By creating 33 virtual subjects of synthetic CMR images at the end-diastolic and end-systolic phases, we evaluate the usefulness of such data in the downstream cardiac cavity segmentation task under different augmentation strategies. Results demonstrate that even with only 20% of real images (40 volumes) seen during training, segmentation performance is retained with the addition of synthetic CMR images. Moreover, the improvement in utilizing synthetic images for augmenting the real data is evident through the reduction of Hausdorff distance up?to 28% and an increase in the Dice score up?to 5%, indicating a higher similarity to the ground truth in all dimensions.",2020/9/29,None,10.1007/978-3-030-59719-1_13
Database Annotation with Few Examples - An Atlas-Based Framework Using Diffeomorphic Registration of 3D Trees.,"[{'@pid': '209/9740', 'text': 'Pierre-Louis Antonsanti'}, {'@pid': '134/9814', 'text': 'Thomas Benseghir'}, {'@pid': '39/9298', 'text': 'Vincent Jugnon'}, {'@pid': '29/2956', 'text': 'Joan Glaun¨¨s'}]",[],"Automatic annotation of anatomical structures can help simplify workflow during interventions in numerous clinical applications but usually involves a large amount of annotated data. The complexity of the labeling task, together with the lack of representative data, slows down the development of robust solutions. In this paper, we propose a solution requiring very few annotated cases to label 3D pelvic arterial trees of patients with benign prostatic hyperplasia. We take advantage of Large Deformation Diffeomorphic Metric Mapping (LDDMM) to perform registration based on meaningful deformations from which we build an atlas. Branch pairing is then computed from the atlas to new cases using optimal transport to ensure one-to-one correspondence during the labeling process. To tackle topological variations in the tree, which usually degrades the performance of atlas-based techniques, we propose a simple bottom-up label assignment adapted to the pelvic anatomy. The proposed method achieves 97.6% labeling precision with only 5 cases for training, while in comparison learning-based methods only reach 82.2% on such small training sets.",2020/9/29,None,10.1007/978-3-030-59716-0_16
Robust Fusion of Probability Maps.,"[{'@pid': '250/3572', 'text': 'Beno?t Audelan'}, {'@pid': '275/6923', 'text': 'Dimitri Hamzaoui'}, {'@pid': '275/6755', 'text': 'Sarah Montagne'}, {'@pid': '60/6871', 'text': 'Rapha?le Renard-Penna'}, {'@pid': 'd/HerveDelingette', 'text': 'Herv¨¦ Delingette'}]","['Imagesegmentation', 'Datafusion', 'Consensus', 'Mixture']","The fusion of probability maps is required when trying to analyse a collection of image labels or probability maps produced by several segmentation algorithms or human raters. The challenge is to weight properly the combination of maps in order to reflect the agreement among raters, the presence of outliers and the spatial uncertainty in the consensus. In this paper, we address several shortcomings of prior work in continuous label fusion. We introduce a novel approach to jointly estimate a reliable consensus map and assess the production of outliers and the confidence in each rater. Our probabilistic model is based on Student¡¯s?t-distributions allowing local estimates of raters¡¯ performances. The introduction of bias and spatial priors leads to proper rater bias estimates and a control over the smoothness of the consensus map. Image intensity information is incorporated by geodesic distance transform for binary masks. Finally, we propose an approach to cluster raters based on variational boosting thus producing possibly several alternative consensus maps. Our approach was successfully tested on the MICCAI 2016 MS lesions dataset, on MR prostate delineations and on deep learning based segmentation predictions of lung nodules from the LIDC dataset.
",2020/9/29,None,10.1007/978-3-030-59719-1_26
An Interactive Mixed Reality Platform for Bedside Surgical Procedures.,"[{'@pid': '22/4495', 'text': 'Ehsan Azimi'}, {'@pid': '133/3223', 'text': 'Zhiyuan Niu'}, {'@pid': '179/4880', 'text': 'Maia Stiber'}, {'@pid': '275/6753', 'text': 'Nicholas Greene'}, {'@pid': '265/2691', 'text': 'Ruby Liu'}, {'@pid': '227/6850', 'text': 'Camilo Molina'}, {'@pid': '76/9894', 'text': 'Judy Huang'}, {'@pid': '56/3778-1', 'text': 'Chien-Ming Huang 0001'}, {'@pid': '14/4502', 'text': 'Peter Kazanzides'}]","['Surgicalnavigation', 'Neurosurgery', 'Augmentedreality']","In many bedside procedures, surgeons must rely on their spatiotemporal reasoning to estimate the position of an internal target by manually measuring external anatomical landmarks. One particular example that is performed frequently in neurosurgery is ventriculostomy, where the surgeon inserts a catheter into the patient¡¯s skull to divert the cerebrospinal fluid and alleviate the intracranial pressure. However, about one-third of the insertions miss the target.",2020/9/29,None,10.1007/978-3-030-59716-0_7
Manifold Ordinal-Mixup for Ordered Classes in TW3-Based Bone Age Assessment.,"[{'@pid': '275/6757', 'text': 'Byeonguk Bae'}, {'@pid': '41/2370', 'text': 'Jaewon Lee'}, {'@pid': '232/3405', 'text': 'Seo Taek Kong'}, {'@pid': '275/7042', 'text': 'Jinkyeong Sung'}, {'@pid': '96/3380', 'text': 'Kyu-Hwan Jung'}]","['Boneageassessment', 'TW3', 'Ordinallearning', 'Manifoldmixup', 'Deeplearning']","Bone age assessment (BAA) is vital to detecting abnormal growth in children and can be used to investigate its cause. Automating assessments could benefit radiologists by reducing reader variability and reading time. Recently, deep learning (DL) algorithms have been devised to automate BAA using hand X-ray images mostly based on GP-based methods. In contrast to GP-based methods where radiologists compare the whole hand¡¯s X-ray image with standard images in the GP-atlas, TW3 methods operate by analyzing major bones in the hand image to estimate the subject¡¯s bone age. It is thus more attractive to automate TW3 methods for their lower reader variability and higher accuracy; however, the inaccessibility of bone maturity stages inhibited wide-spread application of DL in automating TW3 systems. In this work, we propose an unprecedented DL-based TW3 system by training deep neural networks (DNNs) to extract region of interest (RoI) patches in hand images for all 13 major bones and estimate the bone¡¯s maturity stage which in turn can be used to estimate the bone age. For this purpose, we designed a novel loss function which considers ordinal relations among classes corresponding to maturity stages, and show that DNNs trained using our loss not only attains lower mean absolute error, but also learns a path-connected latent space illuminating the inherent ordinal relations among classes. Our experiments show that DNNs trained using the proposed loss outperform other DL algorithms, known to excel in other tasks, in estimating maturity stage and bone age.",2020/9/29,None,10.1007/978-3-030-59725-2_64
Multi-field of View Aggregation and Context Encoding for Single-Stage Nucleus Recognition.,"[{'@pid': '05/6070', 'text': 'Tian Bai'}, {'@pid': '177/2270', 'text': 'Jiayu Xu'}, {'@pid': '03/10932', 'text': 'Fuyong Xing'}]","['Nucleusrecognition', 'Nucleusclassification', 'Contextencoding', 'Ki67', 'Microscopyimageanalysis', 'Pathologyimageanalysis']","Automated nucleus/cell recognition is a very challenging task, especially for differentiating tumor nuclei from non-tumor nuclei in Ki67 immunohistochemistry (IHC) stained images. Convolutional neural networks and their variants have been recently introduced to identify different types of nuclei and have achieved state-of-the-art performance. However, previous nucleus recognition approaches do not explicitly encode contextual information in the images, which can be very helpful for network representation learning. In this paper, we propose a novel multi-field-of-view context encoding method for single-stage nuclei identification in Ki67 IHC stained images. Specifically, we learn a deep structured regression model that takes multi-field of views of images as input and conducts feature aggregation on the fly for representation learning; then, we design a context encoding module to explicitly explore the multi-field-of-view contextual information and enhance the model¡¯s representation power. In order to further improve nucleus recognition, we also introduce a novel deep regression loss that can emphasize specific channels of the prediction map with category-aware channel suppression. The proposed method can be learned in an end-to-end, pixel-to-pixel manner for single-stage nucleus recognition. We evaluate our method on a large-scale pancreatic neuroendocrine tumor image dataset, and the experiments demonstrate the superior performance of our method in nucleus recognition.",2020/9/29,None,10.1007/978-3-030-59722-1_37
Joint Total Variation ESTATICS for Robust Multi-parameter Mapping.,"[{'@pid': '171/8476', 'text': 'Ya?l Balbastre'}, {'@pid': '163/7704', 'text': 'Mikael Brudfors'}, {'@pid': '150/3434', 'text': 'Michela Azzarito'}, {'@pid': '90/11223', 'text': 'Christian Lambert'}, {'@pid': '165/6550', 'text': 'Martina F. Callaghan'}, {'@pid': '20/3439', 'text': 'John Ashburner'}]",[],"Quantitative magnetic resonance imaging (qMRI) derives tissue-specific parameters ¨C such as the apparent transverse relaxation rate \(R_2^\star \), the longitudinal relaxation rate \(R_1\) and the magnetisation transfer saturation ¨C that can be compared across sites and scanners and carry important information about the underlying microstructure. The multi-parameter mapping (MPM) protocol takes advantage of multi-echo acquisitions with variable flip angles to extract these parameters in a clinically acceptable scan time. In this context, ESTATICS performs a joint loglinear fit of multiple echo series to extract \(R_2^\star \) and multiple extrapolated intercepts, thereby improving robustness to motion and decreasing the variance of the estimators. In this paper, we extend this model in two ways: (1) by introducing a joint total variation (JTV) prior on the intercepts and decay, and (2) by deriving a nonlinear maximum a posteriori estimate. We evaluated the proposed algorithm by predicting left-out echoes in a rich single-subject dataset. In this validation, we outperformed other state-of-the-art methods and additionally showed that the proposed approach greatly reduces the variance of the estimated maps, without introducing bias.",2020/9/29,None,10.1007/978-3-030-59713-9_6
Joint Data Imputation and Mechanistic Modelling for Simulating Heart-Brain Interactions in Incomplete Datasets.,"[{'@pid': '241/8705', 'text': 'Jaume Banus'}, {'@pid': '92/5704', 'text': 'Maxime Sermesant'}, {'@pid': '20/3217', 'text': 'Oscar Camara'}, {'@pid': '90/9763', 'text': 'Marco Lorenzi'}]","['GaussianProcess', 'Variationalinference', 'Lumpedmodel', 'Missingfeatures', 'Biomechanicalsimulation']","The use of mechanistic models in clinical studies is limited by the lack of multi-modal patients data representing different anatomical and physiological processes. For example, neuroimaging datasets do not provide a sufficient representation of heart features for the modeling of cardiovascular factors in brain disorders. To tackle this problem we introduce a probabilistic framework for joint cardiac data imputation and personalisation of cardiovascular mechanistic models, with application to brain studies with incomplete heart data. Our approach is based on a variational framework for the joint inference of an imputation model of cardiac information from the available features, along with a Gaussian Process emulator that can faithfully reproduce personalised cardiovascular dynamics. Experimental results on UK Biobank show that our model allows accurate imputation of missing cardiac features in datasets containing minimal heart information, e.g. systolic and diastolic blood pressures only, while jointly estimating the emulated parameters of the lumped model. This allows a novel exploration of the heart-brain joint relationship through simulation of realistic cardiac dynamics corresponding to different conditions of brain anatomy.",2020/9/29,None,10.1007/978-3-030-59725-2_46
SteGANomaly - Inhibiting CycleGAN Steganography for Unsupervised Anomaly Detection in Brain MRI.,"[{'@pid': '73/5647', 'text': 'Christoph Baur'}, {'@pid': '191/5638', 'text': 'Robert Graf'}, {'@pid': '165/7294', 'text': 'Benedikt Wiestler'}, {'@pid': '165/7751', 'text': 'Shadi Albarqouni'}, {'@pid': 'n/NassirNavab', 'text': 'Nassir Navab'}]",[],"Recently, it has been shown that CycleGANs are masters of steganography. They cannot only learn reliable mappings between two distributions without calling for paired training data, but can effectively hide information unseen during training in mapping results from which input data can be recovered almost perfectly. When preventing this during training, CycleGANs actually map samples much closer to the training distribution. Here, we propose to leverage this effect in the context of trending unsupervised anomaly detection, which primarily relies on modeling healthy anatomy with generative models. Here, we embed anomaly detection into a CycleGAN-based style-transfer framework, which is trained to translate healthy brain MR images to a simulated distribution with lower entropy and vice versa. By filtering high frequency, low amplitude signals from lower entropy samples during training, the resulting model suppresses anomalies in reconstructions of the input data at test time. Similar to Autoencoder and GAN-based anomaly detection methods, this allows us to delineate pathologies directly from residuals between input and reconstruction. Various ablative studies and comparisons to state-of-the-art methods highlight the potential of our method.",2020/9/29,None,10.1007/978-3-030-59713-9_69
A Deep Pattern Recognition Approach for Inferring Respiratory Volume Fluctuations from fMRI Data.,"[{'@pid': '239/1803', 'text': 'Roza G. Bayrak'}, {'@pid': '275/7952', 'text': 'Jorge A. Salas'}, {'@pid': '69/10047', 'text': 'Yuankai Huo'}, {'@pid': '43/8258', 'text': 'Catie Chang'}]","['Respirationdata', 'fMRI', 'Patternrecognition', 'Physiologicalartifactremoval']","Functional magnetic resonance imaging (fMRI) is one of the most widely used non-invasive techniques for investigating human brain activity. Yet, in addition to local neural activity, fMRI signals can be substantially influenced by non-local physiological effects stemming from processes such as slow changes in respiratory volume (RV) over time. While external monitoring of respiration is currently relied upon for quantifying RV and reducing its effects during fMRI scans, these measurements are not always available or of sufficient quality. Here, we propose an end-to-end procedure for modeling fMRI effects linked with RV, in the common scenario of missing respiration data. We compare the performance of multiple deep learning models in reconstructing missing RV data based on fMRI spatiotemporal patterns. Finally, we demonstrate how the inference of missing RV data may improve the quality of resting-state fMRI analysis by directly accounting for signal variations associated with slow changes in the depth of breathing over time.",2020/9/29,None,10.1007/978-3-030-59728-3_42
Self-supervised Discovery of Anatomical Shape Landmarks.,"[{'@pid': '181/6850', 'text': 'Riddhish Bhalodia'}, {'@pid': '14/4520', 'text': 'Ladislav Kavan'}, {'@pid': 'w/RossTWhitaker', 'text': 'Ross T. Whitaker'}]","['Self-supervisedlearning', 'Shapeanalysis', 'Landmarklocalization']","Statistical shape analysis is a very useful tool in a wide range of medical and biological applications. However, it typically relies on the ability to produce a relatively small number of features that can capture the relevant variability in a population. State-of-the-art methods for obtaining such anatomical features rely on either extensive preprocessing or segmentation and/or significant tuning and post-processing. These shortcomings limit the widespread use of shape statistics. We propose that effective shape representations should provide sufficient information to align/register images. Using this assumption we propose a self-supervised, neural network approach for automatically positioning and detecting landmarks in images that can be used for subsequent analysis. The network discovers the landmarks corresponding to anatomical shape features that promote good image registration in the context of a particular class of transformations. In addition, we also propose a regularization for the proposed network which allows for a uniform distribution of these discovered landmarks. In this paper, we present a complete framework, which only takes a set of input images and produces landmarks that are immediately usable for statistical shape analysis. We evaluate the performance on a phantom dataset as well as 2D and 3D images.",2020/9/29,None,10.1007/978-3-030-59719-1_61
Vascular Surface Segmentation for Intracranial Aneurysm Isolation and Quantification.,"[{'@pid': '239/1835', 'text': 'Ziga Bizjak'}, {'@pid': '54/6647', 'text': 'Bostjan Likar'}, {'@pid': '28/6983', 'text': 'Franjo Pernus'}, {'@pid': '06/363', 'text': 'Ziga Spiclin'}]","['Intracranialaneurysm', 'Angiographicimages', '3D-DSA', 'CTA', 'MRA', 'Pointcloud', 'Multi-stagedeeplearning', 'Cross-modality-validation']","Predicting rupture risk and deciding on optimal treatment plan for intracranial aneurysms (IAs) is possible by quantification of their size and shape. For this purpose the IA has to be isolated from 3D angiogram. State-of-the-art methods perform IA isolation by encoding neurosurgeon¡¯s intuition about former non-dilated vessel anatomy through principled approaches like fitting a cutting plane to vasculature surface, using Gaussian curvature and vessel centerline distance constraints, by deformable contours or graph cuts guided by the curvature or restricted by Voronoi surface decomposition and similar. However, the large variability of IAs and their parent vasculature configurations often leads to failure or non-intuitive isolation. Manual corrections are thus required, but suffer from poor reproducibility. In this paper, we aim to increase the accuracy, robustness and reproducibility of IA isolation through two stage deep learning based segmentation of vascular surface. The surface was represented by local patches in form of point clouds, which were fed into first stage multilayer neural network (MNN) to obtain descriptors invariant to point ordering, rotation and scale. Binary classifier as second stage MNN was used to isolate surface belonging to the IA. Method validation was based on 57 3D-DSA, 28 CTA and 5 MRA images, where cross-modality-validation showed high segmentation sensitivity of 0.985, a substantial improvement over 0.830 obtained for the state-of-the-art method on the same datasets. Visual analysis of IA isolation and its high accuracy and reliability consistent across CTA, MRA and 3D-DSA scans confirmed the clinical applicability of proposed method.",2020/9/29,None,10.1007/978-3-030-59725-2_13
Automated Acquisition Planning for Magnetic Resonance Spectroscopy in Brain Cancer.,"[{'@pid': '229/2575', 'text': 'Patrick J. Bolan'}, {'@pid': '275/7301', 'text': 'Francesca Branzoli'}, {'@pid': '275/7939', 'text': 'Anna Luisa Di Stefano'}, {'@pid': '275/8044', 'text': 'Lucia Nichelli'}, {'@pid': '91/6318', 'text': 'Romain Valabr¨¨gue'}, {'@pid': '275/7398', 'text': 'Sara L. Saunders'}, {'@pid': '02/4471', 'text': 'Mehmet Ak?akaya'}, {'@pid': '272/5450', 'text': 'Marc Sanson'}, {'@pid': '39/6796', 'text': 'St¨¦phane Leh¨¦ricy'}, {'@pid': '151/9809', 'text': 'Malgorzata Marjanska'}]","['Braincancer', 'Medicalimagesegmentation', 'Imageguidedintervention']","In vivo magnetic resonance spectroscopy (MRS) can provide clinically valuable metabolic information from brain tumors that can be used for prognosis and monitoring response to treatment. Unfortunately, this technique has not been widely adopted in clinical practice or even clinical trials due to the difficulty in acquiring and analyzing the data. In this work we propose a computational approach to solve one of the most critical technical challenges: the problem of quickly and accurately positioning an MRS volume of interest (a cuboid voxel) inside a tumor using MR images for guidance. The proposed automated method comprises a convolutional neural network to segment the lesion, followed by a discrete optimization to position an MRS voxel optimally within the lesion. In a retrospective comparison, the novel automated method is shown to provide improved lesion coverage compared to manual voxel placement.",2020/9/29,None,10.1007/978-3-030-59728-3_71
Learning Joint Shape and Appearance Representations with Metamorphic Auto-Encoders.,"[{'@pid': '205/5419', 'text': 'Alexandre B?ne'}, {'@pid': '275/6954', 'text': 'Paul Vernhet'}, {'@pid': '90/4579', 'text': 'Olivier Colliot'}, {'@pid': '75/6435', 'text': 'Stanley Durrleman'}]","['Numericalbrainatlas', 'Shapeanalysis', 'Metamorphosis']","Transformation-based methods for shape analysis offer a consistent framework to model the geometrical content of images. Most often relying on diffeomorphic transforms, they lack however the ability to properly handle texture and differing topological content. Conversely, modern deep learning methods offer a very efficient way to analyze image textures. Building on the theory of metamorphoses, which models images as combined intensity-domain and spatial-domain transforms of a prototype, we introduce the ¡°metamorphic¡± auto-encoding architecture. This class of neural networks is interpreted as a Bayesian generative and hierarchical model, allowing the joint estimation of the network parameters, a representative prototype of the training images, as well as the relative importance between the geometrical and texture contents.",2020/9/29,None,10.1007/978-3-030-59710-8_20
Diagnostic Assessment of Deep Learning Algorithms for Detection and Segmentation of Lesion in Mammographic Images.,"[{'@pid': '275/6761', 'text': 'Thomas Boot'}, {'@pid': '97/7717', 'text': 'Humayun Irshad'}]","['Mammography', 'Computeraideddiagnosis', 'Massdetection', 'Masssegmentation']","Computer-aided detection or diagnosing support methods aims to improve breast cancer screening programs by helping radiologists to evaluate digital mammography (DM) exams. This system relates to the use of deep learning for automated detection and segmentation of soft tissue lesions at the early stage. This paper presents a novel deep learning approach, based on a two stage object detector combining an enhanced Faster R-CNN with the Libra R-CNN structure for the Object Detection segment. A segmentation network is placed on top of previous structure in order to provide accurate extraction and localization of masses various features, i.e: margin, shape. The segmentation head is based on a Recurrent Residual Convolutional Neural Network and can lead to an additional feature classification for specific instance properties. A database of digital mammograms was collected from one vendor, Hologic, of which 1,200 images contained masses. The performance for our automated detection system was assessed with the sensitivity of the model which reached a micro average recall: 0.892, micro average precision: 0.734, micro average F1 score: 0.805. Macro average recall: 0.896, macro average precision: 0.819, macro average F1 score: 0.843. The segmentation performance for the same test set was evaluated to a mean IOU of 0.859.",2020/9/29,None,10.1007/978-3-030-59719-1_6
SALAD - Self-supervised Aggregation Learning for Anomaly Detection on X-Rays.,"[{'@pid': '59/10419', 'text': 'Behzad Bozorgtabar'}, {'@pid': '50/6718', 'text': 'Dwarikanath Mahapatra'}, {'@pid': '244/8160', 'text': 'Guillaume Vray'}, {'@pid': 't/JeanPhilippeThiran', 'text': 'Jean-Philippe Thiran'}]","['Anomalydetection', 'X-rays', 'Self-supervisedlearning', 'Deepsimilaritymetric']","Deep anomaly detection models using a supervised mode of learning usually work under a closed set assumption and suffer from overfitting to previously seen rare anomalies at training, which hinders their applicability in a real scenario. In addition, obtaining annotations for X-rays is very time consuming and requires extensive training of radiologists. Hence, training anomaly detection in a fully unsupervised or self-supervised fashion would be advantageous, allowing a significant reduction of time spent on the report by radiologists. In this paper, we present SALAD, an end-to-end deep self-supervised methodology for anomaly detection on X-Ray images. The proposed method is based on an optimization strategy in which a deep neural network is encouraged to represent prototypical local patterns of the normal data in the embedding space. During training, we record the prototypical patterns of normal training samples via a memory bank. Our anomaly score is then derived by measuring similarity to a weighted combination of normal prototypical patterns within a memory bank without using any anomalous patterns. We present extensive experiments on the challenging NIH Chest X-rays and MURA dataset, which indicate that our algorithm improves state-of-the-art methods by a wide margin.",2020/9/29,None,10.1007/978-3-030-59710-8_46
A Graph-Based Method for Optimal Active Electrode Selection in Cochlear Implants.,"[{'@pid': '275/6715', 'text': 'Erin Bratu'}, {'@pid': '213/3882', 'text': 'Robert T. Dwyer'}, {'@pid': '62/9220', 'text': 'Jack H. Noble'}]","['Cochlearimplants', 'Graphsearch', 'Imageguidedcochlearimplantprogramming']","The cochlear implant (CI) is a neural prosthetic that is the standard-of-care treatment for severe-to-profound hearing loss. CIs consist of an electrode array inserted into the cochlea that electrically stimulates auditory nerve fibers to induce the sensation of hearing. Competing stimuli occur when multiple electrodes stimulate the same neural pathways. This is known to negatively impact hearing outcomes. Previous research has shown that image-processing techniques can be used to analyze the CI position in CT scans to estimate the degree of competition between electrodes based on the CI user¡¯s unique anatomy and electrode placement. The resulting data permits an algorithm or expert to select a subset of electrodes to keep active to alleviate competition. Expert selection of electrodes using this data has been shown in clinical studies to lead to significantly improved hearing outcomes for CI users. Currently, we aim to translate these techniques to a system designed for worldwide clinical use, which mandates that the selection of active electrodes be automated by robust algorithms. Previously proposed techniques produce optimal plans with only 48% success rate. In this work, we propose a new graph-based approach. We design a graph with nodes that represent electrodes and edge weights that encode competition between electrode pairs. We then find an optimal path through this graph to determine the active electrode set. Our method produces results judged by an expert to be optimal in over 95% of cases. This technique could facilitate widespread clinical translation of image-guided cochlear implant programming methods.",2020/9/29,None,10.1007/978-3-030-59716-0_4
Can a Hand-Held Navigation Device Reduce Cognitive Load? A User-Centered Approach Evaluated by 18 Surgeons.,"[{'@pid': '275/7072', 'text': 'Caroline Brendle'}, {'@pid': '275/6799', 'text': 'Laura Sch¨¹tz'}, {'@pid': '64/11269', 'text': 'Javier Esteban'}, {'@pid': '152/2000', 'text': 'Sandro M. Krieg'}, {'@pid': '30/7703', 'text': 'Ulrich Eck'}, {'@pid': 'n/NassirNavab', 'text': 'Nassir Navab'}]","['Hand-heldnavigationdevice', 'Surgicalnavigation', 'Spinalfusion', 'Pediclescrewplacement', 'Augmentedreality', 'Cognitiveload', 'Surgicalvisualization']","During spinal fusion surgery, the orientation of the pedicle screw in the right angle plays a crucial role for the outcome of the operation. Local separation of navigation information and the surgical situs, in combination with intricate visualizations, can limit the benefits of surgical navigation systems. The present study addresses these problems by proposing a hand-held navigation device (HND) for pedicle screw placement. The in-situ visualization of graphically reduced interfaces, and the simple integration of the device into the surgical work flow, allow the surgeon to position the tool while keeping sight of the anatomical target. 18 surgeons participated in a study comparing the HND to the state-of-the-art visualization on an external screen. Our approach revealed significant improvements in mental demand and overall cognitive load, measured using NASA-TLX (\(p &lt; 0.05\)). Moreover, surgical time (One-Way ANOVA \(p &lt; 0.001\)) and system usability (Kruskal-Wallis test \(p &lt; 0.05\)) were significantly improved.",2020/9/29,None,10.1007/978-3-030-59716-0_38
Flexible Bayesian Modelling for Nonlinear Image Registration.,"[{'@pid': '163/7704', 'text': 'Mikael Brudfors'}, {'@pid': '171/8476', 'text': 'Ya?l Balbastre'}, {'@pid': '23/1613', 'text': 'Guillaume Flandin'}, {'@pid': '40/11373', 'text': 'Parashkev Nachev'}, {'@pid': '20/3439', 'text': 'John Ashburner'}]",[],"We describe a diffeomorphic registration algorithm that allows groups of images to be accurately aligned to a common space, which we intend to incorporate into the SPM software. The idea is to perform inference in a probabilistic graphical model that accounts for variability in both shape and appearance. The resulting framework is general and entirely unsupervised. The model is evaluated at inter-subject registration of 3D human brain scans. Here, the main modeling assumption is that individual anatomies can be generated by deforming a latent ¡®average¡¯ brain. The method is agnostic to imaging modality and can be applied with no prior processing. We evaluate the algorithm using freely available, manually labelled datasets. In this validation we achieve state-of-the-art results, within reasonable runtimes, against previous state-of-the-art widely used, inter-subject registration algorithms. On the unprocessed dataset, the increase in overlap score is over 17%. These results demonstrate the benefits of using informative computational anatomy frameworks for nonlinear registration.",2020/9/29,None,10.1007/978-3-030-59716-0_25
Decision Support for Intoxication Prediction Using Graph Convolutional Networks.,"[{'@pid': '237/9814', 'text': 'Hendrik Burwinkel'}, {'@pid': '150/2920', 'text': 'Matthias Keicher'}, {'@pid': '264/4685', 'text': 'David Bani-Harouni'}, {'@pid': '264/4719', 'text': 'Tobias Zellner'}, {'@pid': '264/4797', 'text': 'Florian Eyer'}, {'@pid': 'n/NassirNavab', 'text': 'Nassir Navab'}, {'@pid': '27/1564', 'text': 'Seyed-Ahmad Ahmadi'}]","['Graphconvolutionalnetworks', 'Representationlearning', 'Diseaseclassification']","Every day, poison control centers (PCC) are called for immediate classification and treatment recommendations of acute intoxication cases. Due to their time-sensitive nature, a doctor is required to propose a correct diagnosis and intervention within a minimal time frame. Usually the toxin is known and recommendations can be made accordingly. However, in challenging cases only symptoms are mentioned and doctors have to rely on clinical experience. Medical experts and our analyses of regional intoxication records provide evidence that this is challenging, since occurring symptoms may not always match textbook descriptions due to regional distinctions or institutional workflow. Computer-aided diagnosis (CADx) can provide decision support, but approaches so far do not consider additional patient data like age or gender, despite their potential value for the diagnosis. In this work, we propose a new machine learning based CADx method which fuses patient symptoms and meta data using graph convolutional networks. We further propose a novel symptom matching method that allows the effective incorporation of prior knowledge into the network and evidently stabilizes the prediction. We validate our method against 10 medical doctors with different experience diagnosing intoxications for 10 different toxins from the PCC in Munich and show our method¡¯s superiority for poison prediction.",2020/9/29,None,10.1007/978-3-030-59713-9_61
Domain-Specific Loss Design for Unsupervised Physical Training - A New Approach to Modeling Medical ML Solutions.,"[{'@pid': '237/9814', 'text': 'Hendrik Burwinkel'}, {'@pid': '84/4496', 'text': 'Holger Matz'}, {'@pid': '264/9361', 'text': 'Stefan Saur'}, {'@pid': '264/9872', 'text': 'Christoph Hauger'}, {'@pid': '264/9849', 'text': 'Ayse Mine Evren'}, {'@pid': '264/9481', 'text': 'Nino Hirnschall'}, {'@pid': '08/11250', 'text': 'Oliver Findl'}, {'@pid': 'n/NassirNavab', 'text': 'Nassir Navab'}, {'@pid': '27/1564', 'text': 'Seyed-Ahmad Ahmadi'}]","['Physicallearning', 'Transferlearning', 'IOLcalculation']","Today, cataract surgery is the most frequently performed ophthalmic surgery in the world. The cataract, a developing opacity of the human eye lens, constitutes the world¡¯s most frequent cause for blindness. During surgery, the lens is removed and replaced by an artificial intraocular lens (IOL). To prevent patients from needing strong visual aids after surgery, a precise prediction of the optical properties of the inserted IOL is crucial. There has been lots of activity towards developing methods to predict these properties from biometric eye data obtained by OCT devices, recently also by employing machine learning. They consider either only biometric data or physical models, but rarely both, and often neglect the IOL geometry. In this work, we propose OpticNet, a novel optical refraction network, loss function, and training scheme which is unsupervised, domain-specific, and physically motivated. We derive a precise light propagation eye model using single-ray raytracing and formulate a differentiable loss function that back-propagates physical gradients into the network. Further, we propose a new transfer learning procedure, which allows unsupervised training on the physical model and fine-tuning of the network on a cohort of real IOL patient cases. We show that our network is not only superior to systems trained with standard procedures but also that our method outperforms the current state of the art in IOL calculation when compared on two biometric data sets.",2020/9/29,None,10.1007/978-3-030-59713-9_52
Deep Volumetric Universal Lesion Detection Using Light-Weight Pseudo 3D Convolution and Surface Point Regression.,"[{'@pid': '185/6840', 'text': 'Jinzheng Cai'}, {'@pid': '28/7692-6', 'text': 'Ke Yan 0006'}, {'@pid': '248/7623', 'text': 'Chi-Tung Cheng'}, {'@pid': '67/4008-6', 'text': 'Jing Xiao 0006'}, {'@pid': '248/7506', 'text': 'Chien-Hung Liao'}, {'@pid': '78/6574-1', 'text': 'Le Lu 0001'}, {'@pid': '60/11273', 'text': 'Adam P. Harrison'}]","['Volumetricuniversallesiondetection', 'Light-weightpseudo3Dconvolution', 'Surfacepointregression']","Identifying, measuring and reporting lesions accurately and comprehensively from patient CT scans are important yet time-consuming procedures for physicians. Computer-aided lesion/significant-findings detection techniques are at the core of medical imaging, which remain very challenging due to the tremendously large variability of lesion appearance, location and size distributions in 3D imaging. In this work, we propose a novel deep anchor-free one-stage volumetric lesion detector (VLD) framework that incorporates (1) pseudo 3D convolution operators to recycle the architectural configurations and pre-trained weights from the off-the-shelf 2D networks, especially ones with large capacities to cope with data variance, and (2) a new surface point regression method to effectively regress the 3D lesion spatial extents by pinpointing their representative key points on lesion surfaces. Experimental validations are first conducted on the public large-scale NIH DeepLesion dataset where our proposed method delivers new state-of-the-art quantitative performance. We also test VLD on our in-house dataset for liver tumor detection. VLD generalizes well in both large-scale and small-sized tumor datasets in CT imaging.",2020/9/29,None,10.1007/978-3-030-59719-1_1
Multi-vertebrae Segmentation from Arbitrary Spine MR Images Under Global View.,"[{'@pid': '125/4975', 'text': 'Heyou Chang'}, {'@pid': '158/6541', 'text': 'Shen Zhao'}, {'@pid': '31/6916', 'text': 'Hao Zheng'}, {'@pid': '48/4792-8', 'text': 'Yang Chen 0008'}, {'@pid': '49/595-1', 'text': 'Shuo Li 0001'}]","['Multi-vertebraesegmentation', 'Globalinformation', 'Graphconvolutionalnetwork', 'Attentionnetwork']","Multi-vertebrae segmentation plays an important role in spine diseases diagnosis and treatment planning. Global spatial dependencies between vertebrae are essential prior information for automatic multi-vertebrae segmentation. However, due to the lack of global information, previous methods have to localize specific vertebrae regions first, then segment and recognize the vertebrae in the region, resulting in a reduction in feature reuse and increase in computation. In this paper, we propose to leverage both global spatial and label information for multi-vertebrae segmentation from arbitrary MR images in one go. Specifically, a spatial graph convolutional network (GCN) is designed to first automatically learn an adjacency matrix and construct a graph on local feature maps, then adopt stacked GCN to capture the global spatial relationships between vertebrae. A label attention network is built to predict the appearance probabilities of all vertebrae using attention mechanism to reduce the ambiguity caused by variant FOV or similar appearances of adjacent vertebrae. The proposed method is trained in an end-to-end manner and evaluated on a challenging dataset of 292 MRI scans with various fields of view, image characteristics and vertebra deformations. The experimental results show that our method achieves high performance (\(89.28\pm 5.21\) of IDR and \(85.37\pm 4.09\%\) of mIoU) from arbitrary input images.",2020/9/29,None,10.1007/978-3-030-59725-2_68
Joint Modeling of Chest Radiographs and Radiology Reports for Pulmonary Edema Assessment.,"[{'@pid': '220/0874', 'text': 'Geeticka Chauhan'}, {'@pid': '40/8436', 'text': 'Ruizhi Liao'}, {'@pid': 'w/WilliamMWellsIII', 'text': 'William Wells'}, {'@pid': '97/8154', 'text': 'Jacob Andreas'}, {'@pid': '10/5630', 'text': 'Xin Wang'}, {'@pid': '234/7666', 'text': 'Seth J. Berkowitz'}, {'@pid': '62/11507', 'text': 'Steven Horng'}, {'@pid': '11/6043', 'text': 'Peter Szolovits'}, {'@pid': 'g/PolinaGolland', 'text': 'Polina Golland'}]",[],"We propose and demonstrate a novel machine learning algorithm that assesses pulmonary edema severity from chest radiographs. While large publicly available datasets of chest radiographs and free-text radiology reports exist, only limited numerical edema severity labels can be extracted from radiology reports. This is a significant challenge in learning such models for image classification. To take advantage of the rich information present in the radiology reports, we develop a neural network model that is trained on both images and free-text to assess pulmonary edema severity from chest radiographs at inference time. Our experimental results suggest that the joint image-text representation learning improves the performance of pulmonary edema assessment compared to a supervised model trained on images only. We also show the use of the text for explaining the image classification by the joint model. To the best of our knowledge, our approach is the first to leverage free-text radiology reports for improving the image model performance in this application. Our code is available at: https://github.com/RayRuizhiLiao/joint_chestxray.",2020/9/29,None,10.1007/978-3-030-59713-9_51
Semantic Hierarchy Guided Registration Networks for Intra-subject Pulmonary CT Image Alignment.,"[{'@pid': '231/2083', 'text': 'Liyun Chen'}, {'@pid': '178/4484', 'text': 'Xiaohuan Cao'}, {'@pid': '09/3666', 'text': 'Lei Chen'}, {'@pid': '118/9834', 'text': 'Yaozong Gao'}, {'@pid': '14/4383', 'text': 'Dinggang Shen'}, {'@pid': '75/5723-1', 'text': 'Qian Wang 0001'}, {'@pid': '52/4733', 'text': 'Zhong Xue'}]","['Medicalimageregistration', 'Convolutionneuralnetwork', 'LungCTfollow-up']","CT scanning has been widely used for diagnosis, staging and follow-up studies of pulmonary nodules, where image registration plays an essential role in follow-up assessment of CT images. However, it is challenging to align subtle structures in the lung CTs often with large deformation. Unsupervised learning-based registration methods, optimized according to the image similarity metrics, become popular in recent years due to their efficiency and robustness. In this work, we consider segmented tissues, i.e., airways, lobules, and pulmonary vessel structures, in a hierarchical way and propose a multi-stage registration workflow to predict deformation fields. The proposed workflow consists of two registration networks. The first network is the label alignment network, used to align the given segmentations. The second network is the vessel alignment network, used to further predict deformation fields to register vessels in lungs. By combining these two networks, we can register lung CT images not only in the semantic level but also in the texture level. In experiments, we evaluated the proposed algorithm on lung CT images for clinical follow-ups. The results indicate that our method has better performance especially in aligning critical structures such as airways and vessel branches in the lung, compared to the existing methods.",2020/9/29,None,10.1007/978-3-030-59716-0_18
MRI Image Reconstruction via Learning Optimization Using Neural ODEs.,"[{'@pid': '95/1710', 'text': 'Eric Z. Chen'}, {'@pid': '51/4242', 'text': 'Terrence Chen'}, {'@pid': '85/10238', 'text': 'Shanhui Sun'}]","['NeuralODE', 'MRIimagereconstruction', 'Deeplearning']",We propose to formulate MRI image reconstruction as an optimization problem and model the optimization trajectory as a dynamic process using ordinary differential equations (ODEs). We model the dynamics in ODE with a neural network and solve the desired ODE with the off-the-shelf (fixed) solver to obtain reconstructed images. We extend this model and incorporate the knowledge of off-the-shelf ODE solvers into the network design (learned solvers). We investigate several models based on three ODE solvers and compare models with fixed solvers and learned solvers. Our models achieve better reconstruction results and are more parameter efficient than other popular methods such as UNet and cascaded CNN. We introduce a new way of tackling the MRI reconstruction problem by modeling the continuous optimization dynamics using neural ODEs.,2020/9/29,None,10.1007/978-3-030-59713-9_9
Compressive MR Fingerprinting Reconstruction with Neural Proximal Gradient Iterations.,"[{'@pid': '92/1489-4', 'text': 'Dongdong Chen 0004'}, {'@pid': 'd/MikeEDavis', 'text': 'Mike E. Davies'}, {'@pid': '18/7824', 'text': 'Mohammad Golbabaee'}]","['Magneticresonancefingerprinting', 'CompressedSensing', 'Deeplearning', 'Learnedproximalgradientdescent']","Consistency of the predictions with respect to the physical forward model is pivotal for reliably solving inverse problems. This consistency is mostly un-controlled in the current end-to-end deep learning methodologies proposed for the Magnetic Resonance Fingerprinting (MRF) problem. To address this, we propose PGD-Net, a learned proximal gradient descent framework that directly incorporates the forward acquisition and Bloch dynamic models within a recurrent learning mechanism. The PGD-Net adopts a compact neural proximal model for de-aliasing and quantitative inference, that can be flexibly trained on scarce MRF training datasets. Our numerical experiments show that the PGD-Net can achieve a superior quantitative inference accuracy, much smaller storage requirement, and a comparable runtime to the recent deep learning MRF baselines, while being much faster than the dictionary matching schemes. Code has been released at https://github.com/edongdongchen/PGD-Net.",2020/9/29,None,10.1007/978-3-030-59713-9_2
Automated Intracranial Artery Labeling Using a Graph Neural Network and Hierarchical Refinement.,"[{'@pid': 'c/LiChen20', 'text': 'Li Chen 0020'}, {'@pid': '31/998', 'text': 'Thomas S. Hatsukami'}, {'@pid': '78/4381', 'text': 'Jenq-Neng Hwang'}, {'@pid': '00/4572', 'text': 'Chun Yuan'}]","['Arterylabeling', 'Graphneuralnetwork', 'Hierarchicalrefinement', 'Intracranialartery']","Automatically labeling intracranial arteries (ICA) with their anatomical names is beneficial for feature extraction and detailed analysis of intracranial vascular structures. There are significant variations in the ICA due to natural and pathological causes, making it challenging for automated labeling. However, the existing public dataset for evaluation of anatomical labeling is limited. We construct a comprehensive dataset with 729 Magnetic Resonance Angiography scans and propose a Graph Neural Network (GNN) method to label arteries by classifying types of nodes and edges in an attributed relational graph. In addition, a hierarchical refinement framework is developed for further improving the GNN outputs to incorporate structural and relational knowledge about the ICA. Our method achieved a node labeling accuracy of 97.5%, and 63.8% of scans were correctly labeled for all Circle of Willis nodes, on a testing set of 105 scans with both healthy and diseased subjects. This is a significant improvement over available state-of-the-art methods. Automatic artery labeling is promising to minimize manual effort in characterizing the complicated ICA networks and provides valuable information for the identification of geometric risk factors of vascular disease. Our code and dataset are available at https://github.com/clatfd/GNN-ART-LABEL.",2020/9/29,None,10.1007/978-3-030-59725-2_8
Estimating Tissue Microstructure with Undersampled Diffusion Data via Graph Convolutional Neural Networks.,"[{'@pid': '76/4764-1', 'text': 'Geng Chen 0001'}, {'@pid': '199/9888', 'text': 'Yoonmi Hong'}, {'@pid': '65/9905', 'text': 'Yongqin Zhang'}, {'@pid': '12/7618', 'text': 'Jaeil Kim'}, {'@pid': '244/1829', 'text': 'Khoi Minh Huynh'}, {'@pid': '38/9113', 'text': 'Jiquan Ma'}, {'@pid': '98/3393', 'text': 'Weili Lin'}, {'@pid': '14/4383', 'text': 'Dinggang Shen'}, {'@pid': '93/5188', 'text': 'Pew-Thian Yap'}]","['DiffusionMRI', 'GraphCNN', 'Microstructureimaging']","Advanced diffusion models for tissue microstructure are widely employed to study brain disorders. However, these models usually require diffusion MRI (DMRI) data with densely sampled q-space, which is prohibitive in clinical settings. This problem can be resolved by using deep learning techniques, which learn the mapping between sparsely sampled q-space data and the high-quality diffusion microstructural indices estimated from densely sampled data. However, most existing methods simply view the input DMRI data as a vector without considering data structure in the q-space. In this paper, we propose to overcome this limitation by representing DMRI data using graphs and utilizing graph convolutional neural networks to estimate tissue microstructure. Our method makes full use of the q-space angular neighboring information to improve estimation accuracy. Experimental results based on data from the Baby Connectome Project demonstrate that our method outperforms state-of-the-art methods both qualitatively and quantitatively.",2020/9/29,None,10.1007/978-3-030-59728-3_28
Deep Class-Specific Affinity-Guided Convolutional Network for Multimodal Unpaired Image Segmentation.,"[{'@pid': '212/6645', 'text': 'Jingkun Chen'}, {'@pid': '15/9687', 'text': 'Wenqi Li'}, {'@pid': '39/5544-4', 'text': 'Hongwei Li 0004'}, {'@pid': '90/6415-1', 'text': 'Jianguo Zhang 0001'}]","['Segmentation', 'Class-specificaffinity', 'Featuretransfer']","Multi-modal medical image segmentation plays an essential role in clinical diagnosis. It remains challenging as the input modalities are often not well-aligned spatially. Existing learning-based methods mainly consider sharing trainable layers across modalities and minimizing visual feature discrepancies. While the problem is often formulated as joint supervised feature learning, multiple-scale features and class-specific representation have not yet been explored. In this paper, we propose an affinity-guided fully convolutional network for multimodal image segmentation. To learn effective representations, we design class-specific affinity matrices to encode the knowledge of hierarchical feature reasoning, together with the shared convolutional layers to ensure the cross-modality generalization. Our affinity matrix does not depend on spatial alignments of the visual features and thus allows us to train with unpaired, multimodal inputs. We extensively evaluated our method on two public multimodal benchmark datasets and outperform state-of-the-art methods.
",2020/9/29,None,10.1007/978-3-030-59719-1_19
An Enhanced Coarse-to-Fine Framework for the Segmentation of Clinical Target Volume.,"[{'@pid': '29/11448', 'text': 'Huai Chen'}, {'@pid': '94/1638', 'text': 'Dahong Qian'}, {'@pid': '72/585', 'text': 'Weiping Liu'}, {'@pid': '66/3387', 'text': 'Hui Li'}, {'@pid': '76/6001', 'text': 'Lisheng Wang'}]","['Clinicaltargetvolumesegmentation', 'Multi-modalitymedicalimages', 'Coarse-to-fine', 'Iterativerefinement']","In radiation therapy, obtaining accurate boundary of the clinical target volume (CTV) is the vital step to decrease the risk of treatment failures. However, it is a time-consuming and laborious task to obtain the delineation by hand. Therefore, an automatic algorithm is urgently needed to realize accurate segmentation. In this paper, we propose an enhanced coarse-to-fine frameworkto automatically fuse the information of CT, T1 and T2 images to get the target region. This framework includes a coarse-segmentation stage to identify the region of interest (ROI) of targets and a fine-segmentation stage to iteratively refine the segmentation. In the coarse-segmentation stage, the F-loss is proposed to keep the high recall rate of the ROI. In the fine segmentation, the ROI of target will be first cropped according to the ROI obtained by coarse-segmentation and be fed into a 3D-Unet to get the initial results. Then, the prediction and medium features will be set as additional information for the next one network to refine the results. When evaluated on the validation dataset of challenge of Anatomical Brain Barriers to Cancer Spread (ABCs), our method won the \(3^{th}\) place in the public leaderboard.",2021/3/13,None,10.1007/978-3-030-71827-5_4
LRTHR-Net - A Low-Resolution-to-High-Resolution Framework to Iteratively Refine the Segmentation of Thyroid Nodule in Ultrasound Images.,"[{'@pid': '29/11448', 'text': 'Huai Chen'}, {'@pid': '211/4015', 'text': 'Shaoli Song'}, {'@pid': '67/7529', 'text': 'Xiuying Wang'}, {'@pid': '242/6299', 'text': 'Renzhen Wang'}, {'@pid': '22/5614', 'text': 'Deyu Meng'}, {'@pid': '76/6001', 'text': 'Lisheng Wang'}]","['Thyroidnodulesegmentation', 'Thyroidultrasound', 'TN-SCUI2020', 'Low-resolution-to-high-resolution', 'Iterativerefinement']","The thyroid nodule is quickly increasing worldwide and the thyroid ultrasound is the key tool for the diagnosis of it. For the subtle difference between malignant and benign nodules, segmenting lesions is the crucial preliminary step for diagnosis. In this paper, we propose a low-resolution-to-high-resolution segmentation framework for TN-SCUI2020 challenge to alleviate the workload of clinicians and improve the efficiency of diagnosis. Specifically speaking, in order to integrate multi-scale information, several low-resolution segmenting results are obtained firstly and combined with a high-resolution image to refine them and obtain high-resolution results. Secondly, iterative-transfer is proposed to effectively initialize network based on previous trained one on small-scale images. Finally, ensemble refinement is introduced to utilize multiple models to refine the segmentation again. Experimental results showed the effectiveness of the proposed framework. And we won the 2nd place in the segmentation task of TN-SCUI2020.",2021/3/13,None,10.1007/978-3-030-71827-5_15
Doctor Imitator - A Graph-Based Bone Age Assessment Framework Using Hand Radiographs.,"[{'@pid': '249/3929', 'text': 'Jintai Chen'}, {'@pid': '250/5778', 'text': 'Bohan Yu'}, {'@pid': '266/7206', 'text': 'Biwen Lei'}, {'@pid': '249/3914', 'text': 'Ruiwei Feng'}, {'@pid': 'c/DannyZChen', 'text': 'Danny Z. Chen'}, {'@pid': '96/2744-1', 'text': 'Jian Wu 0001'}]","['Graph-basedconvolution', 'Boneage', 'Interpretability']","Bone age assessment is challenging in clinical practice due to the complicated bone age assessment process. Current automatic bone age assessment methods were designed with rare consideration of the diagnostic logistics and thus may yield certain uninterpretable hidden states and outputs. Consequently, doctors can find it hard to cooperate with such models harmoniously because it is difficult to check the correctness of the model predictions. In this work, we propose a new graph-based deep learning framework for bone age assessment with hand radiographs, called Doctor Imitator (DI). The architecture of DI is designed to learn the diagnostic logistics of doctors using the scoring methods (e.g., the Tanner-Whitehouse method) for bone age assessment. Specifically, the convolutions of DI capture the local features of the anatomical regions of interest (ROIs) on hand radiographs and predict the ROI scores by our proposed Anatomy-based Group Convolution, summing up for bone age prediction. Besides, we develop a novel Dual Graph-based Attention module to compute patient-specific attention for ROI features and context attention for ROI scores. As far as we know, DI is the first automatic bone age assessment framework following the scoring methods without fully supervised hand radiographs. Experiments on hand radiographs with only bone age supervision verify that DI can achieve excellent performance with sparse parameters and provide more interpretability.",2020/9/29,None,10.1007/978-3-030-59725-2_74
TR-GAN - Topology Ranking GAN with Triplet Loss for Retinal Artery/Vein Classification.,"[{'@pid': '135/7011', 'text': 'Wenting Chen'}, {'@pid': '25/6555', 'text': 'Shuang Yu'}, {'@pid': '39/2439', 'text': 'Junde Wu'}, {'@pid': '86/7113-2', 'text': 'Kai Ma 0002'}, {'@pid': '205/3556', 'text': 'Cheng Bian'}, {'@pid': '271/4639', 'text': 'Chunyan Chu'}, {'@pid': '88/5607', 'text': 'Linlin Shen'}, {'@pid': '44/6510', 'text': 'Yefeng Zheng'}]","['Retinalimaging', 'Artery/VeinClassification', 'Generativeadversarialnetwork', 'Topologyranking']","Retinal artery/vein (A/V) classification lays the foundation for the quantitative analysis of retinal vessels, which is associated with potential risks of various cardiovascular and cerebral diseases. The topological connection relationship, which has been proved effective in improving the A/V classification performance for the conventional graph based method, has not been exploited by the deep learning based method. In this paper, we propose a Topology Ranking Generative Adversarial Network (TR-GAN) to improve the topology connectivity of the segmented arteries and veins, and further to boost the A/V classification performance. A topology ranking discriminator based on ordinal regression is proposed to rank the topological connectivity level of the ground-truth, the generated A/V mask and the intentionally shuffled mask. The ranking loss is further back-propagated to the generator to generate better connected A/V masks. In addition, a topology preserving module with triplet loss is also proposed to extract the high-level topological features and further to narrow the feature distance between the predicted A/V mask and the ground-truth. The proposed framework effectively increases the topological connectivity of the predicted A/V masks and achieves state-of-the-art A/V classification performance on the publicly available AV-DRIVE dataset.",2020/9/29,None,10.1007/978-3-030-59722-1_59
Acceleration of High-Resolution 3D MR Fingerprinting via a Graph Convolutional Network.,"[{'@pid': '12/3274', 'text': 'Feng Cheng'}, {'@pid': '67/6351', 'text': 'Yong Chen'}, {'@pid': '48/10747', 'text': 'Xiaopeng Zong'}, {'@pid': '98/3393', 'text': 'Weili Lin'}, {'@pid': '14/4383', 'text': 'Dinggang Shen'}, {'@pid': '93/5188', 'text': 'Pew-Thian Yap'}]","['3DMRfingerprinting', 'K-spaceinterpolation', 'Graphconvolution', 'GRAPPA']","Magnetic resonance fingerprinting (MRF) is a novel imaging framework for fast and simultaneous quantification of multiple tissue properties. Recently, 3D MRF methods have been developed, but the acquisition speed needs to be improved before they can be adopted for clinical use. The purpose of this study is to develop a novel deep learning approach to accelerate 3D MRF acquisition along the slice-encoding direction in k-space. We introduce a graph-based convolutional neural network that caters to non-Cartesian spiral trajectories commonly used for MRF acquisition. We improve tissue quantification accuracy compared with the state of the art. Our method enables fast 3D MRF with high spatial resolution, allowing whole-brain coverage within 5?min, making MRF more feasible in clinical settings.",2020/9/29,None,10.1007/978-3-030-59713-9_16
Self-weighted Multi-task Learning for Subjective Cognitive Decline Diagnosis.,"[{'@pid': '250/8311', 'text': 'Nina Cheng'}, {'@pid': '16/4982', 'text': 'Alejandro Frangi'}, {'@pid': '21/1302', 'text': 'Zhiguo Zhang 0001'}, {'@pid': '275/7277', 'text': 'Denao Deng'}, {'@pid': '16/5337', 'text': 'Lihua Zhao'}, {'@pid': '25/3611', 'text': 'Tianfu Wang'}, {'@pid': '49/944', 'text': 'Yichen Wei'}, {'@pid': '183/1551', 'text': 'Bihan Yu'}, {'@pid': '247/2784', 'text': 'Wei Mai'}, {'@pid': '247/2661', 'text': 'Gaoxiong Duan'}, {'@pid': '247/2694', 'text': 'Xiucheng Nong'}, {'@pid': '50/3011', 'text': 'Chong Li'}, {'@pid': '192/3853', 'text': 'Jiahui Su'}, {'@pid': '49/9638', 'text': 'Baiying Lei'}]","['Subjectivecognitivedecline', 'Featureselection', 'Multi-tasklearning']","Subjective cognitive decline (SCD) is an early stage of mild cognitive impairment (MCI) and may represent the first symptom manifestation of Alzheimer¡¯s disease (AD). Early diagnosis of MCI is important because early identification and intervention can delay or even reverse the progression of this disease. This paper proposes an automatic diagnostic framework for SCD and MCI. Specifically, we design a new multi-task learning model to integrate neuroimaging functional and structural connectivity in a predictive framework. We construct a functional brain network by sparse low-rank brain network estimation methods, and a structural brain network is constructed using fiber bundle tracking. Subsequently, we use multi-task learning methods to select features for integrated functional and structural connections, the importance of each task and the balance between both modalities are automatically learned. By integrating both functional and structural information, the most discriminative features of the disease are obtained for diagnosis. The experiments on the dataset show that our proposed method achieves good performance and is superior to the traditional algorithms. In addition, the proposed method can identify the most discriminative brain regions and connections. These results follow current clinical findings and add new findings for disease detection and future medical analysis.",2020/9/29,None,10.1007/978-3-030-59728-3_11
3D Convolutional Sequence to Sequence Model for Vertebral Compression Fractures Identification in CT.,"[{'@pid': '239/1938', 'text': 'David Chettrit'}, {'@pid': '275/6953', 'text': 'Tomer Meir'}, {'@pid': '275/6971', 'text': 'Hila Lebel'}, {'@pid': '275/6741', 'text': 'Mila Orlovsky'}, {'@pid': '275/6898', 'text': 'Ronen Gordon'}, {'@pid': '74/2915', 'text': 'Ayelet Akselrod-Ballin'}, {'@pid': '73/11011', 'text': 'Amir Bar'}]",[],"An osteoporosis-related fracture occurs every three seconds worldwide, affecting one in three women and one in five men aged over 50. The early detection of at-risk patients facilitates effective and well-evidenced preventative interventions, reducing the incidence of major osteoporotic fractures. In this study we present an automatic system for identification of vertebral compression fractures on Computed Tomography images, which are often an undiagnosed precursor to major osteoporosis-related fractures. The system integrates a compact 3D representation of the spine, utilizing a Convolutional Neural Network (CNN) for spinal cord detection and a novel end-to-end sequence to sequence 3D architecture. We evaluate several model variants that exploit different representation and classification approaches, and present a framework combining an ensemble of models that achieves state of the art results, validated on a large data set, with a patient-level fracture identification of 0.955 Area Under the Curve (AUC). The system proposed has the potential to support osteoporosis clinical management, improve treatment pathways and to change the course of one of the most burdensome diseases of our generation
.",2020/9/29,None,10.1007/978-3-030-59725-2_72
The Case of Missed Cancers - Applying AI as a Radiologist&apos;s Safety Net.,"[{'@pid': '175/1805', 'text': 'Michal Chorev'}, {'@pid': '205/6038', 'text': 'Yoel Shoshan'}, {'@pid': '74/2915', 'text': 'Ayelet Akselrod-Ballin'}, {'@pid': '42/5384', 'text': 'Adam Spiro'}, {'@pid': '275/6950', 'text': 'Shaked Naor'}, {'@pid': '73/914', 'text': 'Alon Hazan'}, {'@pid': '275/6816', 'text': 'Vesna Barros'}, {'@pid': '275/7049', 'text': 'Iuliana Weinstein'}, {'@pid': '275/6718', 'text': 'Esma Herzel'}, {'@pid': '24/7312', 'text': 'Varda Shalev'}, {'@pid': '275/6771', 'text': 'Michal Guindy'}, {'@pid': '19/6540', 'text': 'Michal Rosen-Zvi'}]","['Computer-aideddiagnosis', 'Deeplearning', 'Breastimaging']","We investigate the potential contribution of an AI system as a safety net application for radiologists in breast cancer screening. As a safety net, the AI alerts on cases suspected to be malignant which the radiologist did not recommend for a recall. We analyzed held-out data of 2,638 exams enriched with 90 missed cancers. In screening mammography settings, we show that a system alerting on 11 out of every 1,000 cases, could detect up to 10.7% of the radiologists¡¯ missed cancers. Thus, significantly increasing radiologist¡¯s sensitivity to 80.3%, while only slightly decreasing their specificity to 95.3%. Importantly, the safety net demonstrated a significant contribution to their performance even when radiologists utilized both mammography and ultrasound images. In those settings, it would have alerted 8.5 times per 1,000 cases, and detected 11.7% of the radiologists¡¯ missed cancers. In an analysis of the missed cancers by an expert, we found that most of the cancers detected by the AI were visible post-hoc. Finally, we performed a reader study with five radiologists over 120 exams, 10 of which were originally missed cancers. The AI safety net was able to assist 3 out of the 5 radiologists in detecting missed cancers without raising any false alerts.",2020/9/29,None,10.1007/978-3-030-59725-2_22
Pay More Attention to Discontinuity for Medical Image Segmentation.,"[{'@pid': '200/9070', 'text': 'Jiajia Chu'}, {'@pid': '35/5165', 'text': 'Yajie Chen'}, {'@pid': '69/5011', 'text': 'Wei Zhou'}, {'@pid': '271/0256', 'text': 'Heshui Shi'}, {'@pid': '96/5464', 'text': 'Yukun Cao'}, {'@pid': '88/8375', 'text': 'Dandan Tu'}, {'@pid': '198/8344', 'text': 'Ri-Chu Jin'}, {'@pid': '86/11269', 'text': 'Yongchao Xu'}]","['Discontinuity', 'Medicalimagesegmentation', 'Edgedetection']","Medical image segmentation is one of the most important tasks for computer aided diagnosis in medical image analysis. Thanks to deep learning, great progress has been made recently. Yet, most existing segmentation methods still struggle at discontinuity positions (including region boundary and discontinuity within regions), especially when generalized to unseen datasets. In particular, discontinuity within regions and being close to the real region contours may cause wrong boundary delineation. In this paper, different from existing methods that focus only on alleviating the discontinuity issue on region boundary, we propose to pay more attention to all discontinuity including the discontinuity within regions. Specifically, we leverage a simple edge detector to locate all the discontinuity and apply additional supervision on these areas. Extensive experiments on cardiac, prostate, and liver segmentation tasks demonstrate that such a simple approach effectively mitigates the inaccurate segmentation due to discontinuity and achieves noticeable improvements over some state-of-the-art methods.",2020/9/29,None,10.1007/978-3-030-59719-1_17
StyPath - Style-Transfer Data Augmentation for Robust Histology Image Classification.,"[{'@pid': '263/4153', 'text': 'Pietro Antonio Cicalese'}, {'@pid': '207/7586', 'text': 'Aryan Mobiny'}, {'@pid': '263/4078', 'text': 'Pengyu Yuan'}, {'@pid': '05/6114', 'text': 'Jan Becker'}, {'@pid': '45/9313', 'text': 'Chandra Mohan'}, {'@pid': '59/9550', 'text': 'Hien Van Nguyen'}]","['Pathology', 'Style-transfer', 'CNNclassifier', 'Dataaugmentation', 'Inter-observeragreement']","The classification of Antibody Mediated Rejection (AMR) in kidney transplant remains challenging even for experienced nephropathologists; this is partly because histological tissue stain analysis is often characterized by low inter-observer agreement and poor reproducibility. One of the implicated causes for inter-observer disagreement is the variability of tissue stain quality between (and within) pathology labs, coupled with the gradual fading of archival sections. Variations in stain colors and intensities can make tissue evaluation difficult for pathologists, ultimately affecting their ability to describe relevant morphological features. Being able to accurately predict the AMR status based on kidney histology images is crucial for improving patient treatment and care. We propose a novel pipeline to build robust deep neural networks for AMR classification based on StyPath, a histological data augmentation technique that leverages a light weight style-transfer algorithm as a means to reduce sample-specific bias. Each image was generated in \(1.84 \pm 0.03\)?s using a single GTX TITAN V gpu and pytorch, making it faster than other popular histological data augmentation techniques. We evaluated our model using a Monte Carlo (MC) estimate of Bayesian performance and generate an epistemic measure of uncertainty to compare both the baseline and StyPath augmented models. We also generated Grad-CAM representations of the results which were assessed by an experienced nephropathologist; we used this qualitative analysis to elucidate on the assumptions being made by each model. Our results imply that our style-transfer augmentation technique improves histological classification performance (reducing error from 14.8% to 11.5%) and generalization ability.",2020/9/29,None,10.1007/978-3-030-59722-1_34
Latent-Graph Learning for Disease Prediction.,"[{'@pid': '122/8728', 'text': 'Luca Cosmo'}, {'@pid': '187/4426', 'text': 'Anees Kazi'}, {'@pid': '27/1564', 'text': 'Seyed-Ahmad Ahmadi'}, {'@pid': 'n/NassirNavab', 'text': 'Nassir Navab'}, {'@pid': '07/2668', 'text': 'Michael M. Bronstein'}]","['Graphconvolution', 'Diseaseprediction', 'Graphlearning']","Recently, Graph Convolutional Networks (GCNs) have proven to be a powerful machine learning tool for Computer Aided Diagnosis (CADx) and disease prediction. A key component in these models is to build a population graph, where the graph adjacency matrix represents pair-wise patient similarities. Until now, the similarity metrics have been defined manually, usually based on meta-features like demographics or clinical scores. The definition of the metric, however, needs careful tuning, as GCNs are very sensitive to the graph structure. In this paper, we demonstrate for the first time in the CADx domain that it is possible to learn a single, optimal graph towards the GCN¡¯s downstream task of disease classification. To this end, we propose a novel, end-to-end trainable graph learning architecture for dynamic and localized graph pruning. Unlike commonly employed spectral GCN approaches, our GCN is spatial and inductive, and can thus infer previously unseen patients as well. We demonstrate significant classification improvements with our learned graph on two CADx problems in medicine. We further explain and visualize this result using an artificial dataset, underlining the importance of graph learning for more accurate and robust inference with GCNs in medical applications.
",2020/9/29,None,10.1007/978-3-030-59713-9_62
Collaborative Learning of Cross-channel Clinical Attention for Radiotherapy-Related Esophageal Fistula Prediction from CT.,"[{'@pid': '17/3482', 'text': 'Hui Cui'}, {'@pid': '275/6804', 'text': 'Yiyue Xu'}, {'@pid': '118/4591', 'text': 'Wanlong Li'}, {'@pid': '94/6774', 'text': 'Linlin Wang'}, {'@pid': '33/2980', 'text': 'Henry B. L. Duh'}]","['Esophagealfistulaprediction', 'CT', 'Crosschannelattention']","Early prognosis of the radiotherapy-related esophageal fistula is of great significance in making personalized stratification and optimal treatment plans for esophageal cancer (EC) patients. The effective fusion of diagnostic consideration guided multi-level radiographic visual descriptors is a challenging task. We propose an end-to-end clinical knowledge enhanced multi-level cross-channel feature extraction and aggregation model. Firstly, clinical attention is represented by contextual CT, segmented tumor and anatomical surroundings from nine views of planes. Then for each view, a Cross-Channel-Atten Network is proposed with CNN blocks for multi-level feature extraction, cross-channel convolution module for multi-domain clinical knowledge embedding at the same feature level, and attentional mechanism for the final adaptive fusion of multi-level cross-domain radiographic features. The experimental results and ablation study on 558 EC patients showed that our model outperformed the other methods in comparison with or without multi-view, multi-domain knowledge, and multi-level attentional features. Visual analysis of attention maps shows that the network learns to focus on tumor and organs of interests, including esophagus, trachea, and mediastinal connective tissues.",2020/9/29,None,10.1007/978-3-030-59710-8_21
A Deep-Generative Hybrid Model to Integrate Multimodal and Dynamic Connectivity for Predicting Spectrum-Level Deficits in Autism.,"[{'@pid': '226/3325', 'text': 'Niharika Shimona D&apos;Souza'}, {'@pid': '152/1352', 'text': 'Mary Beth Nebel'}, {'@pid': '267/1348', 'text': 'Deana Crocetti'}, {'@pid': '116/2873', 'text': 'Nicholas F. Wymbs'}, {'@pid': '15/4759', 'text': 'Joshua Robinson'}, {'@pid': '53/1411', 'text': 'Stewart Mostofsky'}, {'@pid': '79/7823', 'text': 'Archana Venkataraman'}]",[],"We propose an integrated deep-generative framework, that jointly models complementary information from resting-state functional MRI (rs-fMRI) connectivity and diffusion tensor imaging (DTI) tractography to extract predictive biomarkers of a disease. The generative part of our framework is a structurally-regularized Dynamic Dictionary Learning (sr-DDL) model that decomposes the dynamic rs-fMRI correlation matrices into a collection of shared basis networks and time varying patient-specific loadings. This matrix factorization is guided by the DTI tractography matrices to learn anatomically informed connectivity profiles. The deep part of our framework is an LSTM-ANN block, which models the temporal evolution of the patient sr-DDL loadings to predict multidimensional clinical severity. Our coupled optimization procedure collectively estimates the basis networks, the patient-specific dynamic loadings, and the neural network weights. We validate our framework on a multi-score prediction task in 57 patients diagnosed with Autism Spectrum Disorder (ASD). Our hybrid model outperforms state-of-the-art baselines in a five-fold cross validated setting and extracts interpretable multimodal neural signatures of brain dysfunction in ASD.
",2020/9/29,None,10.1007/978-3-030-59728-3_43
Suggestive Annotation of Brain Tumour Images with Gradient-Guided Sampling.,"[{'@pid': '171/8003', 'text': 'Chengliang Dai'}, {'@pid': '63/1591', 'text': 'Shuo Wang'}, {'@pid': '198/0698', 'text': 'Yuanhan Mo'}, {'@pid': '275/7059', 'text': 'Kaichen Zhou'}, {'@pid': '63/1426', 'text': 'Elsa D. Angelini'}, {'@pid': 'g/YikeGuo', 'text': 'Yike Guo'}, {'@pid': '20/5519', 'text': 'Wenjia Bai'}]","['Suggestiveannotation', 'Braintumoursegmentation', 'MR']","Machine learning has been widely adopted for medical image analysis in recent years given its promising performance in image segmentation and classification tasks. As a data-driven science, the success of machine learning, in particular supervised learning, largely depends on the availability of manually annotated datasets. For medical imaging applications, such annotated datasets are not easy to acquire. It takes a substantial amount of time and resource to curate an annotated medical image set. In this paper, we propose an efficient annotation framework for brain tumour images that is able to suggest informative sample images for human experts to annotate. Our experiments show that training a segmentation model with only 19% suggestively annotated patient scans from BraTS 2019 dataset can achieve a comparable performance to training a model on the full dataset for whole tumour segmentation task. It demonstrates a promising way to save manual annotation cost and improve data efficiency in medical imaging applications.
",2020/9/29,None,10.1007/978-3-030-59719-1_16
Ranking-Based Survival Prediction on Histopathological Whole-Slide Images.,"[{'@pid': '242/4517', 'text': 'Donglin Di'}, {'@pid': '55/8742', 'text': 'Shengrui Li'}, {'@pid': '29/4190-18', 'text': 'Jun Zhang 0018'}, {'@pid': '33/3099-2', 'text': 'Yue Gao 0002'}]","['Survivalprediction', 'WSI', 'Ranking', 'Hypergraph']","Survival prediction for patients based on gigapixel histopathological whole-slide images (WSIs) has attracted increasing attention in recent years. Previous studies mainly focus on the framework of predicting the survival hazard scores based on one individual WSI for each patient directly. These prediction methods ignore the relative survival differences among patients, i.e., the ranking information, which is important for a regression task. Under such circumstances, we propose a ranking-based survival prediction method on WSIs ¨C RankSurv, which takes the ranking information into consideration during the learning process. First, a hypergraph representation is introduced to conduct hazard prediction on each WSI respectively, which is able to learn the high-order correlation among different patches in the WSI. Then, a ranking-based prediction process is conducted using pairwise survival data. Experiments are conducted on three public carcinoma datasets (i.e., LUSC, GBM, and NLST). Quantitative results show that the proposed method significantly outperforms state-of-the-art methods on all three datasets, which demonstrates the effectiveness of the proposed ranking-based survival prediction framework.",2020/9/29,None,10.1007/978-3-030-59722-1_41
Feature-Enhanced Graph Networks for Genetic Mutational Prediction Using Histopathological Images in Colon Cancer.,"[{'@pid': '275/6848', 'text': 'Kexin Ding'}, {'@pid': '48/6001', 'text': 'Qiao Liu'}, {'@pid': '99/4565', 'text': 'Edward Lee'}, {'@pid': '62/8453', 'text': 'Mu Zhou'}, {'@pid': '33/5455', 'text': 'Aidong Lu'}, {'@pid': '53/3894', 'text': 'Shaoting Zhang'}]","['Histopathologicalimageanalysis', 'Graphconvolutionalnetworks', 'Genemutationprediction']","Mining histopathological and genetic data provides a unique avenue to deepen our understanding of cancer biology. However, extensive cancer heterogeneity across image- and molecular-scales poses technical challenges for feature extraction and outcome prediction. In this study, we propose a feature-enhanced graph network (FENet) for genetic mutation prediction using histopathological images in colon cancer. Unlike conventional approaches analyzing patch-based feature alone without considering their spatial connectivity, we seek to link and explore non-isomorphic topological structures in histopathological images. Our FENet incorporates feature enhancement in convolutional graph neural networks to aggregate discriminative features for capturing gene mutation status. Specifically, our approach could identify both local patch feature information and global topological structure in histopathological images simultaneously. Furthermore, we introduced an ensemble strategy by constructing multiple subgraphs to boost the prediction performance. Extensive experiments on the TCGA-COAD and TCGA-READ cohort including both histopathological images and three key genes¡¯ mutation profiles (APC, KRAS, and TP53) demonstrated the superiority of FENet for key mutational outcome prediction in colon cancer.",2020/9/29,None,10.1007/978-3-030-59713-9_29
Retinal Nerve Fiber Layer Defect Detection with Position Guidance.,"[{'@pid': '51/3939', 'text': 'Fei Ding'}, {'@pid': '36/4658-1', 'text': 'Gang Yang 0001'}, {'@pid': '83/2408', 'text': 'Dayong Ding'}, {'@pid': '254/1786', 'text': 'Gangwei Cheng'}]","['RNFLD', 'Fundusimage', 'Cnn', 'Position', 'Dependency']","The retinal nerve fiber layer defect (RNFLD) provides early diagnostic evidence for many irreversible disabling or blinding diseases. This paper aims for automated RNFLD detection based on fundus images. Different from previous works that only consider the local contexts, we are the first to propose to detect RNFLD with position guidance, which senses both the physiological position and global dependencies with ease. Our solution consists of a position-consistent data preprocessing, a Position Guided Network, and a weakly supervised learning strategy. In the position-consistent data preprocessing, the optic disc region is evenly divided into several sectors according to the distribution regularity of RNFL. To detect RNFLD in sectors, the proposed Position Guided Network highlights the significant region with a position-aware attention module and captures the global dependencies with a bidirectional GRU module. The dataset about RNFLD suffers from noise labels, which is verified in our created dataset containing 4,335 fundus images. Thus the weakly supervised learning strategy, which jointly optimizes network parameters and label distributions, is proposed to reduce the impact of noise labels. Tested on a clinical dataset of 750 images, our solution achieves outstanding performance, attaining the F1 score of 81.00% that outperforms the baseline by 13.71%.",2020/9/29,None,10.1007/978-3-030-59722-1_72
High-Order Attention Networks for Medical Image Segmentation.,"[{'@pid': '51/3939', 'text': 'Fei Ding'}, {'@pid': '36/4658-1', 'text': 'Gang Yang 0001'}, {'@pid': '20/3894', 'text': 'Jun Wu'}, {'@pid': '83/2408', 'text': 'Dayong Ding'}, {'@pid': '275/7061', 'text': 'Jie Xv'}, {'@pid': '254/1786', 'text': 'Gangwei Cheng'}, {'@pid': '58/5856', 'text': 'Xirong Li'}]","['Segmentation', 'Receptivefield', 'High-ordergraph']","Segmentation is a fundamental task in medical image analysis. Current state-of-the-art Convolutional Neural Networks on medical image segmentation capture local context information using fixed-shape receptive fields and feature detectors with position-invariant weights, which limits the robustness to the variance of input, such as medical objects of variant sizes, shapes, and domains. In order to capture global context information, we propose High-order Attention (HA), a novel attention module with adaptive receptive fields and dynamic weights. HA allows each pixel to has its own global attention map that models its relationship to all other pixels. In particular, HA constructs the attention map through graph transduction and thus captures high relevant context information at high-order. Consequently, feature maps at each position are selectively aggregated as a weighted sum of feature maps at all positions. We further embed the proposed HA module into an efficient encoder-decoder structure for medical image segmentation, namely High-order Attention Network (HANet). Extensive experiments are conducted on four benchmark sets for three tasks, i.e., REFUGE and Drishti-GS1 for optic disc/cup segmentation, DRIVE for blood vessel segmentation, and LUNA for lung segmentation. The results justify the effectiveness of the new attention module for medical image segmentation.",2020/9/29,None,10.1007/978-3-030-59710-8_25
Holistic Analysis of Abdominal CT for Predicting the Grade of Dysplasia of Pancreatic Lesions.,"[{'@pid': '182/1793', 'text': 'Konstantin Dmitriev'}, {'@pid': 'k/ArieEKaufman', 'text': 'Arie E. Kaufman'}]",[],"Diagnosis of various pancreatic lesions in CT images is a challenging task owing to a significant overlap in their imaging appearance. An accurate diagnosis of pancreatic lesions and the assessment of their malignant progression, or the grade of dysplasia, is crucial for optimal patient management. Typically, the grade of dysplasia is confirmed histologically via biopsy, yet certain radiological findings, including extrapancreatic, can serve as diagnostic clues of the disease progression. This work introduces a novel method of transforming intermediate activations for processing intact imaging data of varying sizes with convnets with linear layers. Our method allows to efficiently leverage the 3D information of the entire abdominal CT scan to acquire a holistic picture of all radiological findings for an improved and more precise classification of pancreatic lesions. Our model outperforms current state-of-the-art methods in classifying four most common lesion types (by 2.92%), while additionally diagnosing the grade of dysplasia. We conduct a set of experiments to illustrate the effects of a holistic CT analysis and the auxiliary diagnostic data on the accuracy of the final diagnosis.",2020/9/29,None,10.1007/978-3-030-59713-9_28
Discovering Functional Brain Networks with 3D Residual Autoencoder (ResAE).,"[{'@pid': '181/6910', 'text': 'Qinglin Dong'}, {'@pid': '226/4225', 'text': 'Ning Qiang'}, {'@pid': '96/8526', 'text': 'Jinglei Lv'}, {'@pid': '40/1491-1', 'text': 'Xiang Li 0001'}, {'@pid': '96/5013', 'text': 'Tianming Liu'}, {'@pid': '70/2532', 'text': 'Quanzheng Li'}]","['Deeplearning', 'fMRI', 'Brainnetworks', '3Dspatiotemporalmodel']","Functional MRI has attracted increasing attention in cognitive neuroscience and clinical mental health research. Towards understanding how brain give rises to mental phenomena, deep learning has been applied to functional MRI (fMRI) dataset to discover the physiological basis of cognitive process. Considering the unsupervised nature of fMRI due to the complex intrinsic brain activities, an encoder-decoder structure is promising to model hidden structure of latent signal sources. Inspired by the success of deep residual learning, we propose a 68-layer 3D residual autoencoder (3D ResAE) to model deep representations of fMRI in this paper. The proposed model is evaluated on the fMRI data under 3 cognitive tasks in Human Connectome Project (HCP). The experimental results have shown that the temporal representations learned by the encoder matches the task design and the spatial representations can be interpreted to be meaningful functional brain networks (FBNs), which not only include tasks based FBNs, but also intrinsic FBNs. The proposed model also outperforms a 3-layer autoencoder, showing the key factor for the performance improvement is depth. Our work demonstrates the feasibility and success of adopting 2D advanced deep residual networks in computer vision into 3D fMRI volume modeling.",2020/9/29,None,10.1007/978-3-030-59728-3_49
Spatiotemporal Attention Autoencoder (STAAE) for ADHD Classification.,"[{'@pid': '181/6910', 'text': 'Qinglin Dong'}, {'@pid': '226/4225', 'text': 'Ning Qiang'}, {'@pid': '96/8526', 'text': 'Jinglei Lv'}, {'@pid': '40/1491-1', 'text': 'Xiang Li 0001'}, {'@pid': '96/5013', 'text': 'Tianming Liu'}, {'@pid': '70/2532', 'text': 'Quanzheng Li'}]","['Deeplearning', 'rfMRI', 'Attentionmechanism', 'Functionalnetworks', 'ADHD']","It has been of great interest in the neuroimaging community to model spatiotemporal brain function and disorders based on resting state functional magnetic resonance imaging (rfMRI). A variety of spatiotemporal methods have been proposed for rfMRI so far, including deep learning models such as convolution networks (CNN) and recurrent networks (RNN). However, the dominant models fail to capture the long-distance dependency (LDD) due to their sequential nature, which becomes critical at longer sequence lengths due to memory limit. Inspired by human brain¡¯s extraordinary ability of long-term memory and attention, the attention mechanism is designed for machine translation to draw global dependencies and achieved state-of-the-art. In this paper, we propose a spatiotemporal attention autoencoder (STAAE) to discover global features that address LDDs in rfMRI. STAAE encodes the information throughout the rfMRI sequence and reveals resting state networks (RSNs) that characterize spatial and temporal properties of the data. Considering that the rfMRI is measured without external tasks, an unsupervised classification framework is developed based on the connectome generated with STAAE. This framework has been evaluated on 281 children with ADHD and 266 normal control children from 4 sites of ADHD200 datasets. The proposed STAAE reveals the global functional interaction in the brain and achieves a state-of-the-art classification accuracy from 59.5% to 77.2% on multiple sites. It is evident that the proposed attention-based model provides a novel approach towards better understanding of human brain.",2020/9/29,None,10.1007/978-3-030-59728-3_50
TexNet - Texture Loss Based Network for Gastric Antrum Segmentation in Ultrasound.,"[{'@pid': '250/3877', 'text': 'Guohao Dong'}, {'@pid': '250/3873', 'text': 'Yaoxian Zou'}, {'@pid': '275/6800', 'text': 'Jiaming Jiao'}, {'@pid': '30/8131', 'text': 'Yuxi Liu'}, {'@pid': '07/6773', 'text': 'Shuo Liu'}, {'@pid': '83/9222', 'text': 'Tianzhu Liang'}, {'@pid': '191/6684-4', 'text': 'Chaoyue Liu 0004'}, {'@pid': '12/1713', 'text': 'Zhijie Chen'}, {'@pid': '99/549', 'text': 'Lei Zhu'}, {'@pid': '02/450', 'text': 'Dong Ni'}, {'@pid': '149/7468', 'text': 'Muqing Lin'}]","['Gastricantrum', 'Ultrasound', 'Point-of-care', 'Segmentation', 'Textureloss', 'Channelattention']","Gastric Antrum (GA) cross-sectional area measurement using ultrasound imaging is an important point-of-care (POC) application in intense care unit (ICU) and anesthesia. GA in ultrasound images often show substantial differences in both shape and texture among subjects, leading to a challenging task of automated segmentation. To the best of our knowledge, no work has been published for this task. Meanwhile, dice similarity coefficient (DSC) based loss function has been widely used by CNN-based segmentation methods. Simply calculating mask overlap, DSC is often biased towards shape and lack of generalization ability for cases with diversified and complicated texture patterns. In this paper, we present a robust segmentation method (TexNet) by introducing a new loss function based on multiscale information of local boundary texture. The new texture loss provides a complementary measure of texture-wise accuracy in contour area which can reduce overfitting issues caused by using DSC loss alone. Experiments have been performed on 8487 images from 121 patients. Results show that TexNet outperforms state of the art methods with higher accuracy and better consistency. Besides GA, the proposed method could potentially be an ideal solution to segment other organs with large variation in both shape and texture among subjects.",2020/9/29,None,10.1007/978-3-030-59719-1_14
DeU-Net - Deformable U-Net for 3D Cardiac MRI Video Segmentation.,"[{'@pid': '262/5811', 'text': 'Shunjie Dong'}, {'@pid': '206/2101', 'text': 'Jinlong Zhao'}, {'@pid': '05/2983', 'text': 'Maojun Zhang'}, {'@pid': '270/2934', 'text': 'Zhengxue Shi'}, {'@pid': '266/3884', 'text': 'Jianing Deng'}, {'@pid': '94/5536', 'text': 'Yiyu Shi'}, {'@pid': '31/1572', 'text': 'Mei Tian'}, {'@pid': '05/853', 'text': 'Cheng Zhuo'}]",[],"Automatic segmentation of cardiac magnetic resonance imaging (MRI) facilitates efficient and accurate volume measurement in clinical applications. However, due to anisotropic resolution and ambiguous border (e.g., right ventricular endocardium), existing methods suffer from the degradation of accuracy and robustness in 3D cardiac MRI video segmentation. In this paper, we propose a novel Deformable U-Net (DeU-Net) to fully exploit spatio-temporal information from 3D cardiac MRI video, including a Temporal Deformable Aggregation Module (TDAM) and a Deformable Global Position Attention (DGPA) network. First, the TDAM takes a cardiac MRI video clip as input with temporal information extracted by an offset prediction network. Then we fuse extracted temporal information via a temporal aggregation deformable convolution to produce fused feature maps. Furthermore, to aggregate meaningful features, we devise the DGPA network by employing deformable attention U-Net, which can encode a wider range of multi-dimensional contextual information into global and local features. Experimental results show that our DeU-Net achieves the state-of-the-art performance on commonly used evaluation metrics, especially for cardiac marginal information (ASSD and HD).",2020/9/29,None,10.1007/978-3-030-59719-1_10
Scribble-Based Domain Adaptation via Co-segmentation.,"[{'@pid': '242/9013', 'text': 'Reuben Dorent'}, {'@pid': '244/2150', 'text': 'Samuel Joutard'}, {'@pid': '242/9221', 'text': 'Jonathan Shapey'}, {'@pid': '35/7612', 'text': 'Sotirios Bisdas'}, {'@pid': '202/2381', 'text': 'Neil Kitchen'}, {'@pid': '84/5893', 'text': 'Robert Bradford'}, {'@pid': '275/4117', 'text': 'Shakeel R. Saeed'}, {'@pid': '04/7426', 'text': 'Marc Modat'}, {'@pid': '40/2838', 'text': 'S¨¦bastien Ourselin'}, {'@pid': '99/4387', 'text': 'Tom Vercauteren'}]","['Domainadaptation', 'Weaksupervision', 'Regularisedloss']","Although deep convolutional networks have reached state-of-the-art performance in many medical image segmentation tasks, they have typically demonstrated poor generalisation capability. To be able to generalise from one domain (e.g. one imaging modality) to another, domain adaptation has to be performed. While supervised methods may lead to good performance, they require to fully annotate additional data which may not be an option in practice. In contrast, unsupervised methods don¡¯t need additional annotations but are usually unstable and hard to train. In this work, we propose a novel weakly-supervised method. Instead of requiring detailed but time-consuming annotations, scribbles on the target domain are used to perform domain adaptation. This paper introduces a new formulation of domain adaptation based on structured learning and co-segmentation. Our method is easy to train, thanks to the introduction of a regularised loss. The framework is validated on Vestibular Schwannoma segmentation (T1 to T2 scans). Our proposed method outperforms unsupervised approaches and achieves comparable performance to a fully-supervised approach.",2020/9/29,None,10.1007/978-3-030-59710-8_47
"Leveraging Tools from Autonomous Navigation for Rapid, Robust Neuron Connectivity.","[{'@pid': '142/4116', 'text': 'Nathan Drenkow'}, {'@pid': '197/1243', 'text': 'Justin Joyce'}, {'@pid': '205/3312', 'text': 'Jordan Matelsky'}, {'@pid': '275/6948', 'text': 'Jennifer Heiko'}, {'@pid': '275/6885', 'text': 'Reem Larabi'}, {'@pid': '275/6815', 'text': 'Brock Wester'}, {'@pid': '131/6569', 'text': 'Dean Kleissas'}, {'@pid': '129/9138', 'text': 'William R. Gray Roncal'}]","['Connectomics', 'Neuroscience', 'Computervision']","As biological imaging datasets continue to grow in size, extracting information from large image volumes presents a computationally intensive challenge. State-of-the-art algorithms are almost entirely dominated by the use of convolutional neural network approaches that may be difficult to run at scale given schedule, cost, and resource limitations. We demonstrate a novel solution for high-resolution electron microscopy brain image volumes that permits the identification of individual neurons and synapses. Instead of conventional approaches where voxels are labelled according to the neuron or neuron segment to which they belong, we instead focus on extracting the underlying brain graph represented by synaptic connections between individual neurons, while also identifying key features like skeleton similarity and path length. This graph represents a critical step and scaffold for understanding the structure of neuronal circuitry. Our approach, which we call Agents, recasts the segmentation problem to one of path finding between keypoints (i.e., connectivity) in an information sharing framework using virtual agents. We create a family of sensors which follow local decision-making rules that perform computationally cheap operations on potential fields to perform tasks such as avoiding cell membranes and finding synapses. These enable a swarm of virtual agents to efficiently and robustly traverse three-dimensional datasets, create a sparse segmentation of pathways, and capture connectivity information. We achieve results that meet or exceed state-of-the-art performance at a substantially lower computational cost. Agents offers a categorically different approach to connectome estimation that can augment how we extract connectivity information at scale. Our method is generalizable and may be extended to biomedical imaging problems such as tracing the bronchial trees in lungs or road networks in natural images.",2020/9/29,None,10.1007/978-3-030-59722-1_11
Deep Learning Automatic Fetal Structures Segmentation in MRI Scans with Few Annotated Datasets.,"[{'@pid': '275/6983', 'text': 'Gal Dudovitch'}, {'@pid': '275/6772', 'text': 'Daphna Link-Sourani'}, {'@pid': '92/8525', 'text': 'Liat Ben-Sira'}, {'@pid': '275/7011', 'text': 'Elka Miller'}, {'@pid': '62/8525', 'text': 'Dafna Ben-Bashat'}, {'@pid': '66/2699', 'text': 'Leo Joskowicz'}]","['Deeplearning', 'FetalMRI', 'Segmentation', 'Uncertaintyestimation']","We present a new method for end-to-end automatic volumetric segmentation of fetal structures in MRI scans with deep learning networks trained with very few annotated scans. It consists of three main stages: 1) two-step automatic structure segmentation with custom 3D U-Nets; 2) segmentation error estimation, and; 3) segmentation error correction. The automatic structure segmentation stage first computes a region of interest (ROI) on a downscaled scan and then computes a final segmentation on the cropped ROI. The segmentation error estimation stage uses prediction-time augmentations of the input scan to compute multiple segmentations and estimate the segmentation uncertainty for individual slices and for the entire scan. The segmentation error correction stage then uses these estimations to locate the most error-prone slices and to correct the segmentations in those slices based on validated adjacent slices. Experimental results of our methods on fetal body (63 cases, 9 for training, 55 for testing) and fetal brain MRI scans (35 cases, 6 for training, 29 for testing) yield a mean Dice coefficient of 0.96 for both, and a mean Average Symmetric Surface Distance of 0.74?mm and 0.19?mm, respectively, below the observer delineation variability.",2020/9/29,None,10.1007/978-3-030-59725-2_35
Microtubule Tracking in Electron Microscopy Volumes.,"[{'@pid': '222/2939', 'text': 'Nils Eckstein'}, {'@pid': '181/6901', 'text': 'Julia M. Buhmann'}, {'@pid': '28/4203-1', 'text': 'Matthew Cook 0001'}, {'@pid': '60/9514', 'text': 'Jan Funke'}]",[],"We present a method for microtubule tracking in electron microscopy volumes. Our method first identifies a sparse set of voxels that likely belong to microtubules. Similar to prior work, we then enumerate potential edges between these voxels, which we represent in a candidate graph. Tracks of microtubules are found by selecting nodes and edges in the candidate graph by solving a constrained optimization problem incorporating biological priors on microtubule structure. For this, we present a novel integer linear programming formulation, which results in speed-ups of three orders of magnitude and an increase of 53% in accuracy compared to prior art (evaluated on three \(1.2\times 4\times 4\,\upmu \)m volumes of Drosophila neural tissue). We also propose a scheme to solve the optimization problem in a block-wise fashion, which allows distributed tracking and is necessary to process very large electron microscopy volumes. Finally, we release a benchmark dataset for microtubule tracking, here used for training, testing and validation, consisting of eight \(30 \times 1000 \times 1000\) voxel blocks (\(1.2\times 4\times 4\,\upmu \)m) of densely annotated microtubules in the CREMI data set (https://github.com/nilsec/micron).",2020/9/29,None,10.1007/978-3-030-59722-1_10
Fisher-Rao Regularized Transport Analysis of the Glymphatic System and Waste Drainage.,"[{'@pid': '211/2221', 'text': 'Rena Elkin'}, {'@pid': '182/1587', 'text': 'Saad Nadeem'}, {'@pid': '68/8527', 'text': 'Hedok Lee'}, {'@pid': '116/1975', 'text': 'Helene Benveniste'}, {'@pid': 't/AllenTannenbaum', 'text': 'Allen R. Tannenbaum'}]","['Optimalmasstransport', 'Glymphaticsystem', 'Fisher-Raoregularization']","In this work, a unified representation of all the time-varying dynamics is accomplished with a Lagrangian framework for analyzing Fisher-Rao regularized dynamical optimal mass transport (OMT) derived flows. While formally equivalent to the Eulerian based Schr?dinger bridge OMT regularization scheme, the Fisher-Rao approach allows a simple and interpretable methodology for studying the flows of interest in the present work. The advantage of the proposed Lagrangian technique is that the time-varying particle trajectories and attributes are displayed in a single visualization. This provides a natural capability to identify and distinguish flows under different conditions. The Lagrangian analysis applied to the glymphatic system (brain waste removal pathway associated with Alzheimer¡¯s Disease) successfully captures known flows and distinguishes between flow patterns under two different anesthetics, providing deeper insights into altered states of waste drainage.",2020/9/29,None,10.1007/978-3-030-59728-3_56
AGAN - An Anatomy Corrector Conditional Generative Adversarial Network.,"[{'@pid': '00/7654', 'text': 'Melih Engin'}, {'@pid': '275/6779', 'text': 'Robin Lange'}, {'@pid': '275/6882', 'text': 'Andras Nemes'}, {'@pid': '144/1030', 'text': 'Sadaf Monajemi'}, {'@pid': '275/6940', 'text': 'Milad Mohammadzadeh'}, {'@pid': '275/6872', 'text': 'Chin Kong Goh'}, {'@pid': '275/6911', 'text': 'Tian Ming Tu'}, {'@pid': '275/6914', 'text': 'Benjamin Y. Q. Tan'}, {'@pid': '275/6968', 'text': 'Prakash Paliwal'}, {'@pid': '275/6745', 'text': 'Leonard L. L. Yeo'}, {'@pid': '153/4203', 'text': 'Vijay K. Sharma'}]","['Deeplearning', 'Convolutionalneuralnetworks', 'Generativeadversarialnetworks', 'Segmentation', 'Shapepriors', 'Ultrasound']","The accurate segmentation of medical images has important consequences in clinical applications. Noisy and artefact-heavy images can result in erroneous image segmentation and often require expert understanding of the target anatomy by clinicians to interpret and compensate for missing and obfuscated data. This is especially true in ultrasound imaging where shadowing and speckle artefacts are common. We propose a novel approach to handle such artefacts using a conditional Generative Adversarial Network called Anatomical GAN (AGAN) that can correct anatomically-invalid pixel-wise segmentation and impose shape priors in carotid artery ultrasound images by learning the underlying structure of the arteries. These anatomically accurate outputs can then be used in the clinical work flow by clinicians or be further processed by other automated methods for assistance in clinical decision making. AGAN can be chained with any pixel-wise segmentation method and is generalisable for both anatomy and artefacts. Experimental results on a longitudinal ultrasound carotid artery dataset show that AGAN can correct anatomically-invalid segmentation masks obtained with different pixel-wise segmentation methods when other state-of-the-art methods fail.
",2020/9/29,None,10.1007/978-3-030-59713-9_68
A Deep Bayesian Video Analysis Framework - Towards a More Robust Estimation of Ejection Fraction.,"[{'@pid': '236/3815', 'text': 'Mohammad Mahdi Kazemi Esfeh'}, {'@pid': '200/9452', 'text': 'Christina Luong'}, {'@pid': '182/3954', 'text': 'Delaram Behnami'}, {'@pid': '192/9084', 'text': 'Teresa Tsang'}, {'@pid': '30/909', 'text': 'Purang Abolmaesumi'}]","['Bayesiandeeplearning', 'Ejectionfraction', 'Uncertainty', 'Echocardiography', 'Ultrasoundimaging']","Ejection Fraction (EF) is a widely-used and critical index of cardiac health. EF measures the efficacy of the cyclic contraction of the ventricles and the outward pumpage of blood through the arteries. Timely and robust evaluation of EF is essential, as reduced EF indicates dysfunction in blood delivery during the ventricular systole, and is associated with a number of cardiac and non-cardiac risk factors and mortality-related outcomes. Automated reliable EF estimation in echocardiography (echo) has proven challenging due to low and variable image quality, and limited amounts of data for training data-driven algorithms which delays the integration of the technologies in the clinical workflow. In this paper, we introduce a Bayesian learning framework for automated EF assessment in echo videos. Our key contribution is to automatically estimate the epistemic uncertainty, i.e.?the model uncertainty, in EF estimation. We anticipate that such information about uncertainty can be incorporated in clinical decision making. We use a ResNet18-based (2?+?1)D as the baseline architecture for video analysis and provide its side-by-side comparison of our probabilistic approach using public data from 10,031 echo exams. Our results clearly indicate the superior performance of the Bayesian model in the clinically critical lower EF population.",2020/9/29,None,10.1007/978-3-030-59713-9_56
Microscopic Fine-Grained Instance Classification Through Deep Attention.,"[{'@pid': '173/0240', 'text': 'Mengran Fan'}, {'@pid': '147/9917', 'text': 'Tapabrata Chakraborti'}, {'@pid': '117/6621', 'text': 'Eric I-Chao Chang'}, {'@pid': '03/4702-1', 'text': 'Yan Xu 0001'}, {'@pid': '11/377', 'text': 'Jens Rittscher'}]","['Medicalimageclassification', 'Deepattentionmechanism']","Fine-grained classification of microscopic image data with limited samples is an open problem in computer vision and biomedical imaging. Deep learning based vision systems mostly deal with high number of low-resolution images, whereas subtle details in biomedical images require higher resolution. To bridge this gap, we propose a simple yet effective deep network that performs two tasks simultaneously in an end-to-end manner. First, it utilises a gated attention module that can focus on multiple key instances at high resolution without extra annotations or region proposals. Second, the global structural features and local instance features are fused for final image level classification. The result is a robust but lightweight end-to-end trainable deep network that yields state-of-the-art results in two separate fine-grained multi-instance biomedical image classification tasks: a benchmark breast cancer histology dataset and our new fungi species mycology dataset. In addition, we demonstrate the interpretability of the proposed model by visualising the concordance of the learned features with clinically relevant features.",2020/9/29,None,10.1007/978-3-030-59722-1_47
Convolutional Bayesian Models for Anatomical Landmarking on Multi-dimensional Shapes.,"[{'@pid': '75/1950', 'text': 'Yonghui Fan'}, {'@pid': '88/128-1', 'text': 'Yalin Wang 0001'}]","['Anatomicallandmarking', 'Gaussianprocesskernels']","The anatomical landmarking on statistical shape models is widely used in structural and morphometric analyses. The current study focuses on leveraging geometric features to realize an automatic and reliable landmarking. The existing implementations usually rely on classical geometric features and data-driven learning methods. However, such designs often have limitations to specific shape types. Additionally, calculating the features as a standalone step increases the computational cost. In this paper, we propose a convolutional Bayesian model for anatomical landmarking on multi-dimensional shapes. The main idea is to embed the convolutional filtering in a stationary kernel so that the geometric features are efficiently captured and implicitly encoded into the prior knowledge of a Gaussian process. In this way, the posterior inference is geometrically meaningful without entangling with extra features. By using a Gaussian process regression framework and the active learning strategy, our method is flexible and efficient in extracting arbitrary numbers of landmarks. We demonstrate extensive applications on various publicly available datasets, including one brain imaging cohort and three skeletal anatomy datasets. Both the visual and numerical evaluations verify the effectiveness of our method in extracting significant landmarks.",2020/9/29,None,10.1007/978-3-030-59719-1_76
Positive-Aware Lesion Detection Network with Cross-scale Feature Pyramid for OCT Images.,"[{'@pid': '266/7101', 'text': 'Dongyi Fan'}, {'@pid': '223/0849', 'text': 'Chengfen Zhang'}, {'@pid': '98/6982', 'text': 'Bin Lv'}, {'@pid': '266/7103', 'text': 'Lilong Wang'}, {'@pid': '237/6058', 'text': 'Guanzheng Wang'}, {'@pid': '181/2695', 'text': 'Min Wang'}, {'@pid': '08/4873', 'text': 'Chuanfeng Lv'}, {'@pid': '73/5346', 'text': 'Guotong Xie'}]","['OpticalCoherenceTomography', 'Lesiondetection', 'Cross-scalefeaturepyramid', 'Positive-aware']","Optical coherence tomography (OCT) is an important imaging technique in ophthalmology, and accurate detection of retinal lesions plays an important role in computer-aided diagnosis. However, the particularities of retinal lesions, such as their complex appearance and large variation of scale, limit the successful application of conventional deep learning-based object detection networks for OCT lesion detection. In this study, we propose a positive-aware lesion detection network with cross-scale feature pyramid for OCT images. A cross-scale boost module with non-local network is firstly applied to enhance the ability of feature representation for OCT lesions with varying scales. To avoid lesion omission and misdetection, some positive-aware network designs are then added into a two-stage detection network, including global level positive estimation and local level positive mining. Finally, we establish a large OCT dataset with multiple retinal lesions, and perform sufficient comparative experiments on it. The results demonstrate that our proposed network achieves 92.36 mean average precision (mAP) for OCT lesion detection, which is superior to other existing detection approaches. ",2020/9/29,None,10.1007/978-3-030-59722-1_66
"Rethinking PET Image Reconstruction - Ultra-Low-Dose, Sinogram and Deep Learning.","[{'@pid': '275/7831', 'text': 'Qiupeng Feng'}, {'@pid': '48/4950', 'text': 'Huafeng Liu'}]","['Positronemissiontomography', 'Ultra-low-dose', 'Reconstruction', 'Deeplearning']","Although Positron emission tomography (PET) has a wide range of clinical applications, radiation exposure to patients in PET continues to draw concerns. To reduce the radiation risk, efforts have been made to obtain high resolution images from low-resolution images. However, previous studies mainly focused on denoising PET images in image space, which ignored the influence of sinogram quality and constraints in reconstruction process. This paper proposed a directly reconstruction framework from ultra-low-dose sinogram based on deep learning. Two coupled networks are introduced to sequentially denoise low-dose sinogram and reconstruct the activity map. Evaluation on in vivo PET dataset indicates that the proposed method can achieve better performance than other state-of-the-art methods and reconstruct satisfactory PET images with only 0.2% dose of standard one.",2020/9/29,None,10.1007/978-3-030-59728-3_76
Dynamic Multi-object Gaussian Process Models.,"[{'@pid': '208/3766', 'text': 'Jean-Rassaire Fouefack'}, {'@pid': '208/3849', 'text': 'Bhushan Borotikar'}, {'@pid': '29/2722', 'text': 'Tania S. Douglas'}, {'@pid': '14/175', 'text': 'Val¨¦rie Burdin'}, {'@pid': '93/8374', 'text': 'Tinashe E. M. Mutsvangwa'}]","['Combined3Dshapeandposeanalysis', 'Generativemodels', 'StatisticalEuclideanmotionrepresentation', 'Gaussianprocess']","Statistical shape models (SSMs) are state-of-the-art medical image analysis tools for extracting and explaining shape across a set of biological structures. A combined analysis of shape and pose variation would provide additional utility in medical image analysis tasks such as automated multi-organ segmentation and completion of partial data. However, a principled and robust way to combine shape and pose features has been illusive due to three main issues: 1) non-homogeneity of the data (data with linear and non-linear natural variation across features), 2) non-optimal representation of the 3D Euclidean motion (rigid transformation representations that are not proportional to the kinetic energy that moves an object from one position to the other), and 3) artificial discretization of the models. Here, we propose a new dynamic multi-object statistical modelling framework for the analysis of human joints in a continuous domain. Specifically, we propose to normalise shape and dynamic spatial features in the same linearized statistical space, permitting the use of linear statistics; and we adopt an optimal 3D Euclidean motion representation for more accurate rigid transformation comparisons. The method affords an efficient generative dynamic multi-object modelling platform for biological joints. We validate the method using controlled synthetic data. The shape-pose prediction results suggest that the novel concept may have utility for a range of medical image analysis applications including management of human joint disorders.",2020/9/29,None,10.1007/978-3-030-59719-1_73
Using Elastix to Register Inhale/Exhale Intrasubject Thorax CT - A Unsupervised Baseline to the Task 2 of the Learn2Reg Challenge.,"[{'@pid': '274/3628', 'text': 'Constance Fourcade'}, {'@pid': '182/1577', 'text': 'Mathieu Rubeaux'}, {'@pid': '55/6754', 'text': 'Diana Mateus'}]","['Registration', 'Inhale/exhalelungCT']","As part of MICCAI 2020, the Learn2Reg registration challenge was proposed as a benchmark to allow registration algorithms comparison. The task 2 of this challenge consists in intrasubject 3D HRCT inhale/exhale thorax images registration. In this context, we propose a classical iterative-based registration approach based on Elastix toolbox, optimizing normalized cross-correlation metric regularized by a bending energy penalty term. This conventional registration approach, as opposed to novel deep learning techniques, reached visually interesting results, with a target registration error of 6.55 ¡À 2.69?mm and a Log-Jacobian standard deviation of 0.07 ¡À 0.03. The code is publicly available at: https://github.com/fconstance/Learn2Reg_Task2_SimpleElastix.",2021/3/13,None,10.1007/978-3-030-71827-5_13
Domain Adaptive Relational Reasoning for 3D Multi-organ Segmentation.,"[{'@pid': '255/5996', 'text': 'Shuhao Fu'}, {'@pid': '121/5575', 'text': 'Yongyi Lu'}, {'@pid': '59/2227-33', 'text': 'Yan Wang 0033'}, {'@pid': '192/1413', 'text': 'Yuyin Zhou'}, {'@pid': '71/3692-2', 'text': 'Wei Shen 0002'}, {'@pid': '07/689', 'text': 'Elliot K. Fishman'}, {'@pid': 'y/AlanLYuille', 'text': 'Alan L. Yuille'}]","['Unsuperviseddomainadaptation', 'Relationalreasoning', 'Multi-organsegmentation']","In this paper, we present a novel unsupervised domain adaptation (UDA) method, named Domain Adaptive Relational Reasoning (DARR), to generalize 3D multi-organ segmentation models to medical data collected from different scanners and/or protocols (domains). Our method is inspired by the fact that the spatial relationship between internal structures in medical images is relatively fixed, e.g., a spleen is always located at the tail of a pancreas, which serves as a latent variable to transfer the knowledge shared across multiple domains. We formulate the spatial relationship by solving a jigsaw puzzle task, i.e., recovering a CT scan from its shuffled patches, and jointly train it with the organ segmentation task. To guarantee the transferability of the learned spatial relationship to multiple domains, we additionally introduce two schemes: 1) Employing a super-resolution network also jointly trained with the segmentation model to standardize medical images from different domain to a certain spatial resolution; 2) Adapting the spatial relationship for a test image by test-time jigsaw puzzle training. Experimental results show that our method improves the performance by \(29.60\%\) DSC on target datasets on average without using any data from the target domain during training.
",2020/9/29,None,10.1007/978-3-030-59710-8_64
Cost-Sensitive Regularization for Diabetic Retinopathy Grading from Eye Fundus Images.,"[{'@pid': '160/2676', 'text': 'Adrian Galdran'}, {'@pid': '165/8035', 'text': 'Jose Dolz'}, {'@pid': '274/2720', 'text': 'Hadi Chakor'}, {'@pid': '92/1233', 'text': 'Herv¨¦ Lombaert'}, {'@pid': '68/4478', 'text': 'Ismail Ben Ayed'}]","['Diabeticretinopathygrading', 'Cost-sensitiveclassifiers']","Assessing the degree of disease severity in biomedical images is a task similar to standard classification but constrained by an underlying structure in the label space. Such a structure reflects the monotonic relationship between different disease grades. In this paper, we propose a straightforward approach to enforce this constraint for the task of predicting Diabetic Retinopathy (DR) severity from eye fundus images based on the well-known notion of Cost-Sensitive classification. We expand standard classification losses with an extra term that acts as a regularizer, imposing greater penalties on predicted grades when they are farther away from the true grade associated to a particular image. Furthermore, we show how to adapt our method to the modelling of label noise in each of the sub-problems associated to DR grading, an approach we refer to as Atomic Sub-Task modeling. This yields models that can implicitly take into account the inherent noise present in DR grade annotations. Our experimental analysis on several public datasets reveals that, when a standard Convolutional Neural Network is trained using this simple strategy, improvements of 3¨C5% of quadratic-weighted kappa scores can be achieved at a negligible computational cost. Code to reproduce our results is released at github.com/agaldran/cost_sensitive_loss_classification.",2020/9/29,None,10.1007/978-3-030-59722-1_64
Two-Stream Deep Feature Modelling for Automated Video Endoscopy Data Analysis.,"[{'@pid': '199/2226', 'text': 'Harshala Gammulle'}, {'@pid': '66/2758', 'text': 'Simon Denman'}, {'@pid': '80/166', 'text': 'Sridha Sridharan'}, {'@pid': '03/5529', 'text': 'Clinton Fookes'}]","['Endoscopyimageanalysis', 'Deepnetworks', 'Relationalnetworks.']","Automating the analysis of imagery of the Gastrointestinal (GI) tract captured during endoscopy procedures has substantial potential benefits for patients, as it can provide diagnostic support to medical practitioners and reduce mistakes via human error. To further the development of such methods, we propose a two-stream model for endoscopic image analysis. Our model fuses two streams of deep feature inputs by mapping their inherent relations through a novel relational network model, to better model symptoms and classify the image. In contrast to handcrafted feature-based models, our proposed network is able to learn features automatically and outperforms existing state-of-the-art methods on two public datasets: KVASIR and Nerthus. Our extensive evaluations illustrate the importance of having two streams of inputs instead of a single stream and also demonstrates the merits of the proposed relational network architecture to combine those streams.",2020/9/29,None,10.1007/978-3-030-59716-0_71
Predicting Potential Propensity of Adolescents to Drugs via New Semi-supervised Deep Ordinal Regression Model.,"[{'@pid': '275/6936', 'text': 'Alireza Ganjdanesh'}, {'@pid': '50/10883', 'text': 'Kamran Ghasedi'}, {'@pid': '33/7424', 'text': 'Liang Zhan'}, {'@pid': 'c/WeidongCai', 'text': 'Weidong Cai 0001'}, {'@pid': '03/281', 'text': 'Heng Huang'}]","['Adolescent', 'Marijuana', 'Deeplearning', 'Ordinalregression', 'Semi-supervisedlearning', 'DiffusionMRI', 'Meandiffusivity']","Addiction to drugs between young people is one of the most severe problems in the real world, and it imposes a huge financial and emotional burden on their families and societies. Therefore, predicting potential inclination to drugs at earlier ages can prevent lots of detriments. In this paper, we propose a new semi-supervised deep ordinal regression model to predict the possible propensity of adolescents to marijuana using the diffusion MRI-derived mean diffusivity (MD) from 148 Regions of Interest (ROIs). The traditional deep ordinal regression models cannot be directly applied to our biomedical problem which only has a small number of labeled data, not enough to train the deep learning models. Thus, we design a semi-supervised learning mechanism for deep ordinal regression, such that both labeled and unlabeled data can be used to enhance the model training. In our experiments, we use the ABCD dataset, which contains MRI images of the adolescents under study and their answers in the Likert scale to a questionnaire containing questions about Marijuana. Experimental results on the ABCD dataset validate the superior performance of our new method. Our study provides an inexpensive way to predict the drug tendency using brain MRI data.",2020/9/29,None,10.1007/978-3-030-59710-8_62
Poincar¨¦ Embedding Reveals Edge-Based Functional Networks of the Brain.,"[{'@pid': '196/0809', 'text': 'Siyuan Gao'}, {'@pid': '125/3214', 'text': 'Gal Mishne'}, {'@pid': '69/3436', 'text': 'Dustin Scheinost'}]","['Brainnetworks', 'Poincar¨¦embedding', 'Clustering']","Many approaches have been applied to fMRI data in order to understand the network organization of the brain. While the majority of these works defines networks as a collection of regions (i.e., nodes), there is ample evidence that defining networks as a collection of connections between regions (i.e., edges) offers numerous advantages, including a natural way of grouping regions into multiple networks. Here, we proposed a framework for creating edge-based networks from resting-state functional connectivity data. This framework relies on a novel embedding approach¡ªbased on the Poincar¨¦ embedding¡ªto handle the large number of edges found in fMRI data (e.g., \(O(N^2)\)). We applied this framework to resting-state fMRI data from the Human Connectome Project and compared the resultant networks to networks derived from clustering nodes and from previously proposed methods for clustering edges. While previous methods for clustering edges failed to discover a valuable network representation of the human brain, the edge-based networks derived from clustering the Poincar¨¦ embedding showed clear and interpretable functional networks. Overall, our framework provides a novel tool for characterizing the functional network organization of the brain.
",2020/9/29,None,10.1007/978-3-030-59728-3_44
Spatial Semantic-Preserving Latent Space Learning for Accelerated DWI Diagnostic Report Generation.,"[{'@pid': '250/2462', 'text': 'Aydan Gasimova'}, {'@pid': '236/5740', 'text': 'Gavin Seegoolam'}, {'@pid': '01/5394-18', 'text': 'Liang Chen 0018'}, {'@pid': '129/4749', 'text': 'Paul Bentley'}, {'@pid': '69/2478', 'text': 'Daniel Rueckert'}]",[],"In light of recent works exploring automated pathological diagnosis, studies have also shown that medical text reports can be generated with varying levels of efficacy. Brain diffusion-weighted MRI (DWI) has been used for the diagnosis of ischaemia in which brain death can follow in immediate hours. It is therefore of the utmost importance to obtain ischaemic brain diagnosis as soon as possible in a clinical setting. Previous studies have shown that MRI acquisition can be accelerated using variable-density Cartesian undersampling methods. In this study, we propose an accelerated DWI acquisition pipeline for the purpose of generating text reports containing diagnostic information. We demonstrate that we can learn a semantic-preserving latent space for minor as well as extremely undersampled MR images capable of achieving promising results on a diagnostic report generation task.",2020/9/29,None,10.1007/978-3-030-59728-3_33
"A Bi-directional, Multi-modality Framework for Segmentation of Brain Structures.","[{'@pid': '294/9098', 'text': 'Skylar S. Gay'}, {'@pid': '295/0307', 'text': 'Cenji Yu'}, {'@pid': '295/0039', 'text': 'Dong Joo Rhee'}, {'@pid': '294/9485', 'text': 'Carlos Sjogreen'}, {'@pid': '294/9332', 'text': 'Raymond P. Mumme'}, {'@pid': '173/4150', 'text': 'Callistus M. Nguyen'}, {'@pid': '294/9412', 'text': 'Tucker J. Netherton'}, {'@pid': '38/4194', 'text': 'Carlos E. C¨¢rdenas S.'}, {'@pid': '159/9542', 'text': 'Laurence E. Court'}]","['MICCAI2020', 'ABCs', 'Deeplearning', 'Segmentation']","Careful delineation of normal-tissue organs-at-risk is essential for brain tumor radiotherapy. However, this process is time-consuming and subject to variability. In this work, we propose a multi-modality framework that automatically segments eleven structures. Large structures used for defining the clinical target volume (CTV), such as the cerebellum, are directly segmented from T1-weighted and T2-weighted MR images. Smaller structures used in radiotherapy plan optimization are more difficult to segment, thus, a region of interest is first identified and cropped by a classification model, and then these structures are segmented from the new volume. This bi-directional framework allows for rapid model segmentation and good performance on a standardized challenge dataset when evaluated with volumetric and surface metrics.",2021/3/13,None,10.1007/978-3-030-71827-5_6
Spatial Component Analysis to Mitigate Multiple Testing in Voxel-Based Analysis.,"[{'@pid': '89/3649', 'text': 'Samuel Gerber'}, {'@pid': '88/3304', 'text': 'Marc Niethammer'}]",[],"Voxel-based analysis provides a simple, easy to interpret approach to discover regions correlated with a variable of interest such as for example a pathology indicator. Voxel-based analysis methods perform a statistical test at each voxel and are prone to false positives due to multiple testing, or when corrected for multiple testing may miss regions of interest. Component based approaches, such as principal or independent component analysis provide an approach to mitigate multiple testing, by testing for correlations to projections of the data to the components. We propose a spatially regularized component analysis approach to find components for image data sets that are spatially localized and smooth. We show that the proposed approach leads to components that are easier to interpret and can improve predictive performance when used with linear regression models. We develop an efficient optimization approach using the Grassmannian projection kernel and a randomized SVD. The proposed optimization is capable to deal with data sets too large to fit into memory. We demonstrate the approach with an application to study Alzheimer¡¯s disease using over 1200 images from the OASIS-3 data set.",2020/9/29,None,10.1007/978-3-030-59728-3_65
A New Metric for Characterizing Dynamic Redundancy of Dense Brain Chronnectome and Its Application to Early Detection of Alzheimer&apos;s Disease.,"[{'@pid': '130/9066', 'text': 'Maryam Ghanbari'}, {'@pid': '250/3657', 'text': 'Li-Ming Hsu'}, {'@pid': '23/759', 'text': 'Zhen Zhou'}, {'@pid': '275/7914', 'text': 'Amir Ghanbari'}, {'@pid': '261/2442', 'text': 'Zhanhao Mo'}, {'@pid': '93/5188', 'text': 'Pew-Thian Yap'}, {'@pid': '26/4189-2', 'text': 'Han Zhang 0002'}, {'@pid': '14/4383', 'text': 'Dinggang Shen'}]","['Graphtheory', 'Complexbrainnetworks', 'Diseasediagnosis']","Graph theory has been used extensively to investigate information exchange efficiency among brain regions represented as graph nodes. In this work, we propose a new metric to measure how the brain network is robust or resilient to any attack on its nodes and edges. The metric measures redundancy in the sense that it calculates the minimum number of independent, not necessarily shortest, paths between every pair of nodes. We adopt this metric for characterizing (i) the redundancy of time-varying brain networks, i.e., chronnectomes, computed along the progression of Alzheimer¡¯s disease (AD), including early mild cognitive impairment (EMCI), and (ii) changes in progressive MCI compared to stable MCI by calculating the probabilities of having at least 2 (or 3) independent paths between every pair of brain regions in a short period of time. Finally, we design a learning-based early AD detection framework, coined ¡°REdundancy Analysis of Dynamic functional connectivity for Disease Diagnosis (READ\(^3\))¡±, and show its superiority over other AD early detection methods. With the ability to measure dynamic resilience and robustness of brain networks, the metric is complementary to the commonly used ¡°cost-efficiency¡± in brain network analysis.",2020/9/29,None,10.1007/978-3-030-59728-3_1
Distractor-Aware Neuron Intrinsic Learning for Generic 2D Medical Image Classifications.,"[{'@pid': '191/6550', 'text': 'Lijun Gong'}, {'@pid': '86/7113-2', 'text': 'Kai Ma 0002'}, {'@pid': '44/6510', 'text': 'Yefeng Zheng'}]","['NeuronIntrinsicLearning', 'Distractor-awareness', 'MedicalImageClassification']","Medical image analysis benefits Computer Aided Diagnosis (CADx). A fundamental analyzing approach is the classification of medical images, which serves for skin lesion diagnosis, diabetic retinopathy grading, and cancer classification on histological images. When learning these discriminative classifiers, we observe that the convolutional neural networks (CNNs) are vulnerable to distractor interference. This is due to the similar sample appearances from different categories (i.e., small inter-class distance). Existing attempts select distractors from input images by empirically estimating their potential effects to the classifier. The essences of how these distractors affect CNN classification are not known. In this paper, we explore distractors from the CNN feature space via proposing a neuron intrinsic learning method. We formulate a novel distractor-aware loss that encourages large distance between the original image and its distractor in the feature space. The novel loss is combined with the original classification loss to update network parameters by back-propagation. Neuron intrinsic learning first explores distractors crucial to the deep classifier and then uses them to robustify CNN inherently. Extensive experiments on medical image benchmark datasets indicate that the proposed method performs favorably against the state-of-the-art approaches.",2020/9/29,None,10.1007/978-3-030-59713-9_57
Towards Robust Bone Age Assessment - Rethinking Label Noise and Ambiguity.,"[{'@pid': '39/5700-2', 'text': 'Ping Gong 0002'}, {'@pid': '257/3403', 'text': 'Zihao Yin'}, {'@pid': '71/3387-1', 'text': 'Yizhou Wang 0001'}, {'@pid': '90/6896', 'text': 'Yizhou Yu'}]","['Boneageassessment', 'Noisylabel', 'Graphconvolutionalnetwork']","The effects of label noise and ambiguity are widespread, especially for subjective tasks such as bone age assessment (BAA). However, most existing BAA algorithms ignore these issues. We propose a robust framework for BAA supporting Tanner &amp; Whitehouse 3 (TW3) method, which is clinically more objective and reproducible than Greulich &amp; Pyle (GP) method, but has received less attention from the research community. Since the publicly available RSNA BAA dataset was annotated using GP method, we contribute additional TW3 annotations. We formulate TW3 BAA as an ordinal regression problem, and address both label noise and ambiguity with a two stage deep learning framework. The first stage focuses on correcting erroneous labels with ambiguity tolerated, while the latter stage introduces a module called Residual Context Graph (RCG) to conquer label ambiguity. Inspired by the way human experts handle ambiguity, we combine fine-grained local features with a graph based context. Experiments show the proposed framework outperforms previously reported TW3-based BAA systems by large margins. TW3 annotations of bone maturity levels for a portion of the RSNA BAA dataset will be made publicly available.",2020/9/29,None,10.1007/978-3-030-59725-2_60
SIMBA - Specific Identity Markers for Bone Age Assessment.,"[{'@pid': '200/5697', 'text': 'Cristina Gonz¨¢lez'}, {'@pid': '169/7108', 'text': 'Mar¨ªa Escobar'}, {'@pid': '212/9527', 'text': 'Laura Alexandra Daza'}, {'@pid': '212/9575', 'text': 'Felipe Torres'}, {'@pid': '212/9542', 'text': 'Gustavo Triana'}, {'@pid': '41/639', 'text': 'Pablo Arbel¨¢ez'}]","['Boneageassessment', 'Computer-aideddiagnosis', 'Identitymarkers', 'Relativeboneage']","Bone Age Assessment (BAA) is a task performed by radiologists to diagnose abnormal growth in a child. In manual approaches, radiologists take into account different identity markers when calculating bone age, i.e., chronological age and gender. However, the current automated Bone Age Assessment methods do not completely exploit the information present in the patient¡¯s metadata. With this lack of available methods as motivation, we present SIMBA: Specific Identity Markers for Bone Age Assessment. SIMBA is a novel approach for the task of BAA based on the use of identity markers. For this purpose, we build upon the state-of-the-art model, fusing the information present in the identity markers with the visual features created from the original hand radiograph. We then use this robust representation to estimate the patient¡¯s relative bone age: the difference between chronological age and bone age. We validate SIMBA on the Radiological Hand Pose Estimation dataset and find that it outperforms previous state-of-the-art methods. SIMBA sets a trend of a new wave of Computer-aided Diagnosis methods that incorporate all of the data that is available regarding a patient. To promote further research in this area and ensure reproducibility we will provide the source code as well as the pre-trained models of SIMBA.",2020/9/29,None,10.1007/978-3-030-59725-2_73
Assisted Probe Positioning for Ultrasound Guided Radiotherapy Using Image Sequence Classification.,"[{'@pid': '274/3160', 'text': 'Alex Grimwood'}, {'@pid': '275/7000', 'text': 'Helen McNair'}, {'@pid': '45/5086', 'text': 'Yipeng Hu'}, {'@pid': '165/2331', 'text': 'Ester Bonmati'}, {'@pid': '63/4928', 'text': 'Dean C. Barratt'}, {'@pid': '133/3547', 'text': 'Emma J. Harris'}]","['Ultrasound-guidedradiotherapy', 'Imageclassification', 'Prostateradiotherapy']","Effective transperineal ultrasound image guidance in prostate external beam radiotherapy requires consistent alignment between probe and prostate at each session during patient set-up. Probe placement and ultrasound image interpretation are manual tasks contingent upon operator skill, leading to interoperator uncertainties that degrade radiotherapy precision. We demonstrate a method for ensuring accurate probe placement through joint classification of images and probe position data. Using a multi-input multi-task algorithm, spatial coordinate data from an optically tracked ultrasound probe is combined with an image classifier using a recurrent neural network to generate two sets of predictions in real-time. The first set identifies relevant prostate anatomy visible in the field of view using the classes: outside prostate, prostate periphery, prostate centre. The second set recommends a probe angular adjustment to achieve alignment between the probe and prostate centre with the classes: move left, move right, stop. The algorithm was trained and tested on 9,743 clinical images from 61 treatment sessions across 32 patients. We evaluated classification accuracy against class labels derived from three experienced observers at 2/3 and 3/3 agreement thresholds. For images with unanimous consensus between observers, anatomical classification accuracy was 97.2% and probe adjustment accuracy was 94.9%. The algorithm identified optimal probe alignment within a mean (standard deviation) range of 3.7¡ã (1.2¡ã) from angle labels with full observer consensus, comparable to the 2.8¡ã (2.6¡ã) mean interobserver range. We propose such an algorithm could assist radiotherapy practitioners with limited experience of ultrasound image interpretation by providing effective real-time feedback during patient set-up.",2020/9/29,None,10.1007/978-3-030-59716-0_52
Pair-Wise and Group-Wise Deformation Consistency in Deep Registration Network.,"[{'@pid': '254/8211', 'text': 'Dongdong Gu'}, {'@pid': '178/4484', 'text': 'Xiaohuan Cao'}, {'@pid': '90/3486', 'text': 'Shanshan Ma'}, {'@pid': '09/3666', 'text': 'Lei Chen'}, {'@pid': '81/5599', 'text': 'Guocai Liu'}, {'@pid': '14/4383', 'text': 'Dinggang Shen'}, {'@pid': '52/4733', 'text': 'Zhong Xue'}]","['Medicalimageregistration', 'Deeplearning', 'Inverseconsistency', 'Cycleconsistency']","Ideally the deformation field from one image to another should be invertible and smooth to register images bidirectionally and preserve topology of anatomical structures. In traditional registration methods, differential geometry constraints could guarantee such topological consistency but are computationally intensive and time consuming. Recent studies showed that image registration using deep neural networks is as accurate as and also much faster than traditional methods. Current popular unsupervised learning-based algorithms aim to directly estimate spatial transformations by optimizing similarity between images under registration; however, the estimated deformation fields are often in one direction and do not possess inverse-consistency if swapping the order of two input images. Notice that the consistent registration can reduce systematic bias caused by the order of input images, increase robustness, and improve reliability of subsequent data analysis. Accordingly, in this paper, we propose a new training strategy by introducing both pair-wise and group-wise deformation consistency constraints. Specifically, losses enforcing both inverse-consistency for image pairs and cycle-consistency for image groups are proposed for model training, in addition to conventional image similarity and topology constraints. Experiments on 3D brain magnetic resonance (MR) images showed that such a learning algorithm yielded consistent deformations even after switching the order of input images or reordering images within groups. Furthermore, the registration results of longitudinal elderly MR images demonstrated smaller volumetric measurement variability in labeling regions of interest (ROIs).",2020/9/29,None,10.1007/978-3-030-59716-0_17
Learning a Deformable Registration Pyramid.,"[{'@pid': '262/0547', 'text': 'Niklas Gunnarsson'}, {'@pid': '155/3118', 'text': 'Jens Sj?lund'}, {'@pid': '85/4891', 'text': 'Thomas B. Sch?n'}]","['Medicalimageregistration', 'Deeplearning', 'Deformableregistration.']","We introduce an end-to-end unsupervised (or weakly supervised) image registration method that blends conventional medical image registration with contemporary deep learning techniques from computer vision. Our method downsamples both the fixed and the moving images into multiple feature map levels where a displacement field is estimated at each level and then further refined throughout the network. We train and test our model on three different datasets. In comparison with the initial registrations we find an improved performance using our model, yet we expect it would improve further if the model was fine-tuned for each task. The implementation is publicly available (https://github.com/ngunnar/learning-a-deformable-registration-pyramid).
",2021/3/13,None,10.1007/978-3-030-71827-5_10
Prediction of Plantar Shear Stress Distribution by Conditional GAN with Attention Mechanism.,"[{'@pid': '259/6117', 'text': 'Jinghui Guo'}, {'@pid': '188/3274', 'text': 'Ali Ersen'}, {'@pid': '89/4402-27', 'text': 'Yang Gao 0027'}, {'@pid': '74/2577-2', 'text': 'Yu Lin 0002'}, {'@pid': 'k/LatifurKhan', 'text': 'Latifur Khan'}, {'@pid': '275/7068', 'text': 'Metin Yavuz'}]","['ConditionalGAN', 'Positionalattention', 'Plantarshearstress', 'Diabeticfootulcers']","Diabetic foot ulcers (DFUs) are known to have multifactorial etiology. Among the biomechanical factors that lead to plantar ulcers, shear stresses have been either neglected or unmeasured due to challenges in complexity and equipment availability. The purpose of this study is to develop a software that predicts plantar shear stress using plantar pressure and temperature distributions. Thirty-one subjects, 8 of them at risk of developing DFUs were recruited, and plantar thermography, pressure and shear stress distributions were collected. We introduce the conditional generative adversarial networks (cGAN) for shear stress distribution prediction and propose an attention mechanism to improve the model¡¯s accuracy. The networks can learn the mapping from pressure to shear stress distribution. The attention mechanism can merge temperature distribution into GAN without resizing or aligning it manually. We then test on our dataset with 185 groups. The predicted anteroposterior shear stress distributions give \(72.97\%\) accuracy on peak location prediction and 14.12?kPa on global root mean square error. Our initial results are promising in terms of feasibility of our approach in predicting plantar shear stresses and this approach may benefit to address the DFU risks before ulceration.",2020/9/29,None,10.1007/978-3-030-59713-9_74
Lesion Mask-Based Simultaneous Synthesis of Anatomic and Molecular MR Images Using a GAN.,"[{'@pid': '76/7058', 'text': 'Pengfei Guo'}, {'@pid': '201/7129', 'text': 'Puyang Wang'}, {'@pid': '24/569', 'text': 'Jinyuan Zhou'}, {'@pid': '76/6100', 'text': 'Vishal M. Patel'}, {'@pid': '65/4185', 'text': 'Shanshan Jiang'}]","['MRI', 'Multi-modalitysynthesis', 'GAN']","Data-driven automatic approaches have demonstrated their great potential in resolving various clinical diagnostic dilemmas for patients with malignant gliomas in neuro-oncology with the help of conventional and advanced molecular MR images. However, the lack of sufficient annotated MRI data has vastly impeded the development of such automatic methods. Conventional data augmentation approaches, including flipping, scaling, rotation, and distortion are not capable of generating data with diverse image content. In this paper, we propose a method, called synthesis of anatomic and molecular MR images network (SAMR), which can simultaneously synthesize data from arbitrary manipulated lesion information on multiple anatomic and molecular MRI sequences, including T1-weighted (\(T_1\)w), gadolinium enhanced \(T_1\)w (Gd-\(T_1\)w), T2-weighted (\(T_2\)w), fluid-attenuated inversion recovery (FLAIR), and amide proton transfer-weighted (APTw). The proposed framework consists of a stretch-out up-sampling module, a brain atlas encoder, a segmentation consistency module, and multi-scale label-wise discriminators. Extensive experiments on real clinical data demonstrate that the proposed model can perform significantly better than the state-of-the-art synthesis methods.
",2020/9/29,None,10.1007/978-3-030-59713-9_11
Deep Graph Normalizer - A Geometric Deep Learning Approach for Estimating Connectional Brain Templates.,"[{'@pid': '275/7373', 'text': 'Mustafa Burak Gurbuz'}, {'@pid': '124/9354', 'text': 'Islem Rekik'}]","['Connectionalbraintemplates', 'DeepGraphNormalizer', 'Populationmultiviewbrainnetworkintegration']","A connectional brain template (CBT) is a normalized graph-based representation of a population of brain networks ¡ªalso regarded as an ¡®average¡¯ connectome. CBTs are powerful tools for creating representative maps of brain connectivity in typical and atypical populations. Particularly, estimating a well-centered and representative CBT for populations of multi-view brain networks (MVBN) is more challenging since these networks sit on complex manifolds and there is no easy way to fuse different heterogeneous network views. This problem remains unexplored with the exception of a few recent works rooted in the assumption that the relationship between connectomes are mostly linear. However, such an assumption fails to capture complex patterns and non-linear variation across individuals. Besides, existing methods are simply composed of sequential MVBN processing blocks without any feedback mechanism, leading to error accumulation. To address these issues, we propose Deep Graph Normalizer (DGN), the first geometric deep learning (GDL) architecture for normalizing a population of MVBNs by integrating them into a single connectional brain template. Our end-to-end DGN learns how to fuse multi-view brain networks while capturing non-linear patterns across subjects and preserving brain graph topological properties by capitalizing on graph convolutional neural networks. We also introduce a randomized weighted loss function which also acts as a regularizer to minimize the distance between the population of MVBNs and the estimated CBT, thereby enforcing its centeredness. We demonstrate that DGN significantly outperforms existing state-of-the-art methods on estimating CBTs on both small-scale and large-scale connectomic datasets in terms of both representativeness and discriminability (i.e., identifying distinctive connectivities fingerprinting each brain network population). Our DGN code is available at https://github.com/basiralab/DGN.",2020/9/29,None,10.1007/978-3-030-59728-3_16
Semi-supervised Medical Image Classification with Global Latent Mixing.,"[{'@pid': '201/7634', 'text': 'Prashnna Kumar Gyawali'}, {'@pid': '205/3415', 'text': 'Sandesh Ghimire'}, {'@pid': '266/1390', 'text': 'Pradeep Bajracharya'}, {'@pid': '39/7780-7', 'text': 'Zhiyuan Li 0007'}, {'@pid': '02/6162', 'text': 'Linwei Wang'}]","['Semi-supervisedlearning', 'Mixup', 'Chestx-ray', 'Skinimages.']","Computer-aided diagnosis via deep learning relies on large-scale annotated data sets, which can be costly when involving expert knowledge. Semi-supervised learning (SSL) mitigates this challenge by leveraging unlabeled data. One effective SSL approach is to regularize the local smoothness of neural functions via perturbations around single data points. In this work, we argue that regularizing the global smoothness of neural functions by filling the void in between data points can further improve SSL. We present a novel SSL approach that trains the neural network on linear mixing of labeled and unlabeled data, at both the input and latent space in order to regularize different portions of the network. We evaluated the presented model on two distinct medical image data sets for semi-supervised classification of thoracic disease and skin lesion, demonstrating its improved performance over SSL with local perturbations and SSL with global mixing but at the input space only. Our code is available at https://github.com/Prasanna1991/LatentMixing.
",2020/9/29,None,10.1007/978-3-030-59710-8_59
TRAKO - Efficient Transmission of Tractography Data for Visualization.,"[{'@pid': '153/7790', 'text': 'Daniel Haehn'}, {'@pid': '264/0144', 'text': 'Loraine Franke'}, {'@pid': '21/3626-13', 'text': 'Fan Zhang 0013'}, {'@pid': '226/3321', 'text': 'Suheyla Cetin Karayumak'}, {'@pid': '71/2246', 'text': 'Steve Pieper'}, {'@pid': '90/5355', 'text': 'Lauren J. O&apos;Donnell'}, {'@pid': '37/3484', 'text': 'Yogesh Rathi'}]","['Compression', 'Diffusionimaging', 'Tractography']","Fiber tracking produces large tractography datasets that are tens of gigabytes in size consisting of millions of streamlines. Such vast amounts of data require formats that allow for efficient storage, transfer, and visualization. We present TRAKO, a new data format based on the Graphics Layer Transmission Format (glTF) that enables immediate graphical and hardware-accelerated processing. We integrate a state-of-the-art compression technique for vertices, streamlines, and attached scalar and property data. We then compare TRAKO to existing tractography storage methods and provide a detailed evaluation on eight datasets. TRAKO can achieve data reductions of over 28x without loss of statistical significance when used to replicate analysis from previously published studies.",2020/9/29,None,10.1007/978-3-030-59728-3_32
Variable Fraunhofer MEVIS RegLib Comprehensively Applied to Learn2Reg Challenge.,"[{'@pid': '279/6448', 'text': 'Stephanie H?ger'}, {'@pid': '80/4577', 'text': 'Stefan Heldmann'}, {'@pid': '232/2179', 'text': 'Alessa Hering'}, {'@pid': '232/2191', 'text': 'Sven Kuckertz'}, {'@pid': '258/4047', 'text': 'Annkristin Lange'}]","['Imageregistration', 'Registrationchallenge', 'Learn2Reg']","In this paper, we present our contribution to the learn2reg challenge. We applied the Fraunhofer MEVIS registration library RegLib comprehensively to all 4 tasks of the challenge. For tasks 1¨C3, we used a classic iterative registration method with NGF distance measure, second order curvature regularizer, and a multi-level optimization scheme. For task 4, a deep learning approach with a weakly supervised trained U-Net was applied using the same cost function as in the iterative approach.",2021/3/13,None,10.1007/978-3-030-71827-5_9
"Learning Semantics-Enriched Representation via Self-discovery, Self-classification, and Self-restoration.","[{'@pid': '270/2128', 'text': 'Fatemeh Haghighi'}, {'@pid': '150/8462', 'text': 'Mohammad Reza Hosseinzadeh Taher'}, {'@pid': '07/3019', 'text': 'Zongwei Zhou'}, {'@pid': '22/7648', 'text': 'Michael B. Gotway'}, {'@pid': '22/5131', 'text': 'Jianming Liang'}]","['Self-supervisedlearning', 'Transferlearning', '3Dmodelpre-training']","Medical images are naturally associated with rich semantics about the human anatomy, reflected in an abundance of recurring anatomical patterns, offering unique potential to foster deep semantic representation learning and yield semantically more powerful models for different medical applications. But how exactly such strong yet free semantics embedded in medical images can be harnessed for self-supervised learning remains largely unexplored. To this end, we train deep models to learn semantically enriched visual representation by self-discovery, self-classification, and self-restoration of the anatomy underneath medical images, resulting in a semantics-enriched, general-purpose, pre-trained 3D model, named Semantic Genesis. We examine our Semantic Genesis with all the publicly-available pre-trained models, by either self-supervision or fully supervision, on the six distinct target tasks, covering both classification and segmentation in various medical modalities (i.e., CT, MRI, and X-ray). Our extensive experiments demonstrate that Semantic Genesis significantly exceeds all of its 3D counterparts as well as the de facto ImageNet-based transfer learning in 2D. This performance is attributed to our novel self-supervised learning framework, encouraging deep models to learn compelling semantic representation from abundant anatomical patterns resulting from consistent anatomies embedded in medical images. Code and pre-trained Semantic Genesis are available at https://github.com/JLiangLab/SemanticGenesis.
",2020/9/29,None,10.1007/978-3-030-59710-8_14
Deep Doubly Supervised Transfer Network for Diagnosis of Breast Cancer with Imbalanced Ultrasound Imaging Modalities.,"[{'@pid': '207/0977', 'text': 'Xiangmin Han'}, {'@pid': '125/8189', 'text': 'Jun Wang'}, {'@pid': '45/7741', 'text': 'Weijun Zhou'}, {'@pid': '144/0437', 'text': 'Cai Chang'}, {'@pid': '52/2125', 'text': 'Shihui Ying'}, {'@pid': '31/626-4', 'text': 'Jun Shi 0004'}]","['Ultrasoundimaging', 'Breastcancer', 'Deepdoublysupervisedtransferlearning', 'Supportvectormachineplus', 'Maximummeandiscrepancy']","Elastography ultrasound (EUS) provides additional bio-mechanical information about lesion for B-mode ultrasound (BUS) in the diagnosis of breast cancers. However, joint utilization of both BUS and EUS is not popular due to the lack of EUS devices in rural hospitals, which arouses a novel modality imbalance problem in computer-aided diagnosis (CAD) for breast cancers. Current transfer learning (TL) pay little attention to this special issue of clinical modality imbalance, that is, the source domain (EUS modality) has fewer labeled samples than those in the target domain (BUS modality). Moreover, these TL methods cannot fully use the label information to explore the intrinsic relation between two modalities and then guide the promoted knowledge transfer. To this end, we propose a novel doubly supervised TL network (DDSTN) that integrates the Learning Using Privileged Information (LUPI) paradigm and the Maximum Mean Discrepancy (MMD) criterion into a unified deep TL framework. The proposed algorithm can not only make full use of the shared labels to effectively guide knowledge transfer by LUPI paradigm, but also perform additional supervised transfer between unpaired data. We further introduce the MMD criterion to enhance the knowledge transfer. The experimental results on the breast ultrasound dataset indicate that the proposed DDSTN outperforms all the compared state-of-the-art algorithms for the BUS-based CAD.",2020/9/29,None,10.1007/978-3-030-59725-2_14
Local and Global Structure-Aware Entropy Regularized Mean Teacher Model for 3D Left Atrium Segmentation.,"[{'@pid': '183/1291', 'text': 'Wenlong Hang'}, {'@pid': '17/1152', 'text': 'Wei Feng'}, {'@pid': '20/1080', 'text': 'Shuang Liang'}, {'@pid': '165/8092', 'text': 'Lequan Yu'}, {'@pid': '65/3144-1', 'text': 'Qiong Wang 0001'}, {'@pid': '76/4931', 'text': 'Kup-Sze Choi'}, {'@pid': '00/1015-1', 'text': 'Jing Qin 0001'}]","['Self-ensembling', 'Entropyminimization', 'Structuralconsistency', 'Segmentation.']","Emerging self-ensembling methods have achieved promising semi-supervised segmentation performances on medical images through forcing consistent predictions of unannotated data under different perturbations. However, the consistency only penalizes on independent pixel-level predictions, making structure-level information of predictions not exploited in the learning procedure. In view of this, we propose a novel structure-aware entropy regularized mean teacher model to address the above limitation. Specifically, we firstly introduce the entropy minimization principle to the student network, thereby adjusting itself to produce high-confident predictions of unannotated images. Based on this, we design a local structural consistency loss to encourage the consistency of inter-voxel similarities within the same local region of predictions from teacher and student networks. To further capture local structural dependencies, we enforce the global structural consistency by matching the weighted self-information maps between two networks. In this way, our model can minimize the prediction uncertainty of unannotated images, and more importantly that it can capture local and global structural information and their complementarity. We evaluate the proposed method on a publicly available 3D left atrium MR image dataset. Experimental results demonstrate that our method achieves outstanding segmentation performances than the state-of-the-art approaches in scenes with limited annotated images.",2020/9/29,None,10.1007/978-3-030-59710-8_55
Discrete Unsupervised 3D Registration Methods for the Learn2Reg Challenge.,"[{'@pid': '151/9486', 'text': 'Lasse Hansen'}, {'@pid': '95/10171', 'text': 'Mattias P. Heinrich'}]","['Discreteoptimisation', 'Graphicalmodels', 'Constrast-independentfeatures']","The Learn2Reg challenge poses four very different tasks with varying difficulty for image registration algorithms. In this short paper, we describe our choices for two state-of-the-art discrete 3D registration methods that enable fast and accurate estimation of large deformations without expert supervision during training. Both approaches primarily focus on the use of contrast-invariant features with dense displacement evaluation and were ranked among the top three of all challenge contestants, yielding two first places and three second places for the four sub-tasks.",2021/3/13,None,10.1007/978-3-030-71827-5_8
Reconstruction and Quantification of 3D Iris Surface for Angle-Closure Glaucoma Detection in Anterior Segment OCT.,"[{'@pid': '267/1298', 'text': 'Jinkui Hao'}, {'@pid': '63/7767', 'text': 'Huazhu Fu'}, {'@pid': '272/9600', 'text': 'Yanwu Xu'}, {'@pid': '87/3691', 'text': 'Yan Hu'}, {'@pid': '87/3534', 'text': 'Fei Li'}, {'@pid': '226/3953', 'text': 'Xiulan Zhang'}, {'@pid': '23/108-1', 'text': 'Jiang Liu 0001'}, {'@pid': '17/9876', 'text': 'Yitian Zhao'}]","['AS-OCT', '3Dirissurface', 'Angle-closureglaucoma']","Precise characterization and analysis of iris shape from Anterior Segment OCT (AS-OCT) are of great importance in facilitating diagnosis of angle-closure-related diseases. Existing methods focus solely on analyzing structural properties identified from an individual 2D slice, while accurate characterization of morphological changes of iris shape in 3D AS-OCT may be able to reveal in addition the risk of disease progression. In this paper, we propose a novel framework for reconstruction and quantification of 3D iris surface from AS-OCT volume. We consider it to be the first work to detect angle-closure glaucoma by means of 3D representation. An iris segmentation network with wavelet refinement block (WRB) is first proposed to generate the initial shape of the iris from single AS-OCT slice. The 3D iris surface is then reconstructed using a guided optimization method with Poisson-disk sampling. Finally, a set of surface-based features are extracted, which are used in detecting of angle-closure glaucoma. Experimental results demonstrate that our method is highly effective in iris segmentation and surface reconstruction. Moreover, we show that 3D-based representation achieves better performance in angle-closure glaucoma detection than does 2D-based feature.",2020/9/29,None,10.1007/978-3-030-59722-1_68
Open-Appositional-Synechial Anterior Chamber Angle Classification in AS-OCT Sequences.,"[{'@pid': '237/9846', 'text': 'Huaying Hao'}, {'@pid': '63/7767', 'text': 'Huazhu Fu'}, {'@pid': '272/9600', 'text': 'Yanwu Xu'}, {'@pid': '83/2274', 'text': 'Jianlong Yang'}, {'@pid': '87/3534', 'text': 'Fei Li'}, {'@pid': '226/3953', 'text': 'Xiulan Zhang'}, {'@pid': '23/108-1', 'text': 'Jiang Liu 0001'}, {'@pid': '17/9876', 'text': 'Yitian Zhao'}]","['Angle-closureglaucoma', 'Anteriorchamberangles', 'AS-OCT']","Anterior chamber angle (ACA) classification is a key step in the diagnosis of angle-closure glaucoma in Anterior Segment Optical Coherence Tomography (AS-OCT). Existing automated analysis methods focus on a binary classification system (i.e., open angle or angle-closure) in a 2D AS-OCT slice. However, clinical diagnosis requires a more discriminating ACA three-class system (i.e., open, appositional, or synechial angles) for the benefit of clinicians who seek better to understand the progression of the spectrum of angle-closure glaucoma types. To address this, we propose a novel sequence multi-scale aggregation deep network (SMA-Net) for open-appositional-synechial ACA classification based on an AS-OCT sequence. In our method, a Multi-Scale Discriminative Aggregation (MSDA) block is utilized to learn the multi-scale representations at slice level, while a ConvLSTM is introduced to study the temporal dynamics of these representations at sequence level. Finally, a multi-level loss function is used to combine the slice-based and sequence-based losses. The proposed method is evaluated across two AS-OCT datasets. The experimental results show that the proposed method outperforms existing state-of-the-art methods in applicability, effectiveness, and accuracy. We believe this work to be the first attempt to classify ACAs into open, appositional, or synechial types grading using AS-OCT sequences.",2020/9/29,None,10.1007/978-3-030-59722-1_69
Deformation Aware Augmented Reality for Craniotomy Using 3D/2D Non-rigid Registration of Cortical Vessels.,"[{'@pid': '57/8670', 'text': 'Nazim Haouchine'}, {'@pid': '259/6923', 'text': 'Parikshit Juvekar'}, {'@pid': 'w/WilliamMWellsIII', 'text': 'William M. Wells III'}, {'@pid': '90/3993', 'text': 'Stephane Cotin'}, {'@pid': '09/4129', 'text': 'Alexandra J. Golby'}, {'@pid': 'f/SarahFFriskenGibson', 'text': 'Sarah F. Frisken'}]","['3D/2Dnon-rigidregistration', 'Physics-basedmodelling', 'AugmentedReality', 'Image-guidedneurosurgery', 'Shape-from-Template']","Intra-operative brain shift is a well-known phenomenon that describes non-rigid deformation of brain tissues due to gravity and loss of cerebrospinal fluid among other phenomena. This has a negative influence on surgical outcome that is often based on pre-operative planning where the brain shift is not considered. We present a novel brain-shift aware Augmented Reality method to align pre-operative 3D data onto the deformed brain surface viewed through a surgical microscope. We formulate our non-rigid registration as a Shape-from-Template problem. A pre-operative 3D wire-like deformable model is registered onto a single 2D image of the cortical vessels, which is automatically segmented. This 3D/2D registration drives the underlying brain structures, such as tumors, and compensates for the brain shift in sub-cortical regions. We evaluated our approach on simulated and real data composed of 6 patients. It achieved good quantitative and qualitative results making it suitable for neurosurgical guidance.",2020/9/29,None,10.1007/978-3-030-59719-1_71
Parkinson&apos;s Disease Detection from fMRI-Derived Brainstem Regional Functional Connectivity Networks.,"[{'@pid': '159/9578', 'text': 'Nandinee Fariah Haq'}, {'@pid': '189/6643', 'text': 'Jiayue Cai'}, {'@pid': '275/7701', 'text': 'Tianze Yu'}, {'@pid': '15/78', 'text': 'Martin J. McKeown'}, {'@pid': '13/3672-1', 'text': 'Z. Jane Wang 0001'}]","['Parkinson¡¯sdisease', 'Brainstem', 'Functionalsub-regions']","Parkinson¡¯s disease is the second most prevalent neurodegenerative disorder after Alzheimer¡¯s disease. The brainstem, despite its early and crucial involvement in Parkinson¡¯s disease, is largely unexplored in the domain of functional medical imaging. Here we propose a data-driven, connectivity-pattern based framework to extract functional sub-regions within the brainstem and devise a machine learning based tool that can discriminate Parkinson¡¯s disease from healthy participants. We first propose a novel framework to generate a group model of brainstem functional sub-regions by optimizing a community quality function, and generate a brainstem regional network. We then extract graph theoretic features from this brainstem regional network and, after employing an SVM classifier, achieve a sensitivity of disease detection of 94% ¨C comparable to approaches that normally require whole-brain analysis. To the best of our knowledge, this is the first study that employs brainstem functional sub-regions for Parkinson¡¯s disease detection.
",2020/9/29,None,10.1007/978-3-030-59728-3_4
Interacting with Medical Volume Data in Projective Augmented Reality.,"[{'@pid': '239/1763', 'text': 'Florian Heinrich'}, {'@pid': '239/1760', 'text': 'Kai Bornemann'}, {'@pid': '120/6338', 'text': 'Kai Lawonn'}, {'@pid': '57/2217-1', 'text': 'Christian Hansen 0001'}]","['Interactiontechniques', 'Medicalvolumedata', 'Projectiveaugmentedreality']","Medical volume data is usually explored on monoscopic monitors. Displaying this data in three-dimensional space facilitates the development of mental maps and the identification of anatomical structures and their spatial relations. Using augmented reality (AR) may further enhance these effects by spatially aligning the volume data with the patient. However, conventional interaction methods, e.g. mouse and keyboard, may not be applicable in this environment. Appropriate interaction techniques are needed to naturally and intuitively manipulate the image data. To this end, a user study comparing four gestural interaction techniques with respect to both clipping and windowing tasks was conducted. Image data was directly displayed on a phantom using stereoscopic projective AR and direct volume visualization. Participants were able to complete both tasks with all interaction techniques with respectively similar clipping accuracy and windowing efficiency. However, results suggest advantages of gestures based on motion-sensitive devices in terms of reduced task completion time and less subjective workload. This work presents an important first step towards a surgical AR visualization system enabling intuitive exploration of volume data. Yet, more research is required to assess the interaction techniques¡¯ applicability for intraoperative use.",2020/9/29,None,10.1007/978-3-030-59716-0_41
Deep Generative Model for Synthetic-CT Generation with Uncertainty Predictions.,"[{'@pid': '275/6833', 'text': 'Matt Hemsley'}, {'@pid': '275/6930', 'text': 'Brige Chugh'}, {'@pid': '04/3474', 'text': 'Mark Ruschin'}, {'@pid': '06/5141', 'text': 'Young Lee'}, {'@pid': '275/7078', 'text': 'Chia-Lin Tseng'}, {'@pid': '69/10728', 'text': 'Greg J. Stanisz'}, {'@pid': '275/6787', 'text': 'Angus Lau'}]","['Generativeadversarialnetworks', 'MR-onlyradiationtherapy', 'Uncertainty']","MR-only radiation treatment planning is attractive due to the superior soft tissue definition of MRI as compared to CT, and the elimination of the uncertainty introduced by CT-MRI registration. To facilitate MR-only radiation therapy planning, synthetic-CT (sCT) algorithms (for electron density correction) are required for dose calculation. Deep neural networks for sCT generation are useful due to their predictive power, but lack of uncertainty information is a concern for clinical implementation. The feasibility of using a conditional generative adversarial model (cGAN) to generate sCTs with accompanying uncertainty maps was investigated. Dropout-based variational inference was used to account for uncertainty in the trained model. The cGAN loss function was also combined with an additional term such that the network learns which regions of input data are associated with highly variable outputs. On a dataset of 105 brain cancer patients, our results demonstrate that the network generates well-calibrated uncertainty predictions and produces sCTs with equivalent accuracy as previously reported deterministic models.",2020/9/29,None,10.1007/978-3-030-59710-8_81
Region Proposals for Saliency Map Refinement for Weakly-Supervised Disease Localisation and Classification.,"[{'@pid': '208/6833', 'text': 'Renato Hermoza'}, {'@pid': '201/7638', 'text': 'Gabriel Maicas'}, {'@pid': '25/5812', 'text': 'Jacinto C. Nascimento'}, {'@pid': '53/3609', 'text': 'Gustavo Carneiro'}]","['Weaklysupervisedlearning', 'Objectlocalisation', 'Gumbelsoftmax', 'Regionproposal', 'Saliencymaps', 'Attentionmaps', 'ChestXray14']","The deployment of automated systems to diagnose diseases from medical images is challenged by the requirement to localise the diagnosed diseases to justify or explain the classification decision. This requirement is hard to fulfil because most of the training sets available to develop these systems only contain global annotations, making the localisation of diseases a weakly supervised approach. The main methods designed for weakly supervised disease classification and localisation rely on saliency or attention maps that are not specifically trained for localisation, or on region proposals that can not be refined to produce accurate detections. In this paper, we introduce a new model that combines region proposal and saliency detection to overcome both limitations for weakly supervised disease classification and localisation. Using the ChestX-ray14 data set, we show that our proposed model establishes the new state-of-the-art for weakly-supervised disease diagnosis and localisation. We make our code available at https://github.com/renato145/RpSalWeaklyDet.",2020/9/29,None,10.1007/978-3-030-59725-2_52
Patch-Based Abnormality Maps for Improved Deep Learning-Based Classification of Huntington&apos;s Disease.,"[{'@pid': '187/3010', 'text': 'Kilian Hett'}, {'@pid': '150/3004', 'text': 'R¨¦mi Giraud'}, {'@pid': '78/5594', 'text': 'Hans J. Johnson'}, {'@pid': '57/11402', 'text': 'Jane S. Paulsen'}, {'@pid': '124/2640', 'text': 'Jeffrey D. Long'}, {'@pid': '03/315', 'text': 'Ipek Oguz'}]","['Patch-basedmethod', 'Deeplearning', 'Huntington¡¯sdisease']","Deep learning techniques have demonstrated state-of-the-art performances in many medical imaging applications. These methods can efficiently learn specific patterns. An alternative approach to deep learning is patch-based grading methods, which aim to detect local similarities and differences between groups of subjects. This latter approach usually requires less training data compared to deep learning techniques. In this work, we propose two major contributions: first, we combine patch-based and deep learning methods. Second, we propose to extend the patch-based grading method to a new patch-based abnormality metric. Our method enables us to detect localized structural abnormalities in a test image by comparison to a template library consisting of images from a variety of healthy controls. We evaluate our method by comparing classification performance using different sets of features and models. Our experiments show that our novel patch-based abnormality metric increases deep learning performance from 91.3% to 95.8% of accuracy compared to standard deep learning approaches based on the MRI intensity.",2020/9/29,None,10.1007/978-3-030-59728-3_62
